{
    "docs": [
        {
            "location": "/", 
            "text": "REX-Ray\n\n\nOpenly serious about storage\n\n\n\n\nREX-Ray is an open source, storage management solution designed to support\ncontainer runtimes such as Docker and Mesos. REX-Ray enables stateful\napplications, such as databases, to persist and maintain its data after the life\ncycle of the container has ended. Built-in high availability enables\norchestrators such as \nDocker Swarm\n,\n\nKubernetes\n, and \nMesos\nFrameworks\n like\n\nMarathon\n to automatically orchestrate\nstorage tasks between hosts in a cluster.\n\n\nBuilt on top of the \nlibStorage\n\nframework, REX-Ray's simplified architecture consists of a single binary and\nruns as a stateless service on every host using a configuration file to\norchestrate multiple storage platforms.\n\n\n\n\n\n\nnote\n\n\nThe current REX-Ray release omits support for several, previously verified\nstorage platforms. These providers will be reintroduced incrementally,\nbeginning with 0.4.1. If an absent driver prevents the use of REX-Ray,\nplease continue to use 0.3.3 until such time the storage platform is re-\nintroduced as a part of the\n\nlibStorage\n framework.\nInstructions on how to \ninstall\n\nand \nconfigure\n REX-Ray 0.3.3 are\nboth available.\n\n\n\n\nSupported Technologies\n\n\nStorage Provider Support\n\n\nThe following storage providers and platforms are supported by REX-Ray.\n\n\n\n\n\n\n\n\nProvider\n\n\nStorage Platform(s)\n\n\n\n\n\n\n\n\n\n\nAmazon EC2\n\n\nEBS\n, \nEFS\n, \nS3FS\n\n\n\n\n\n\nCeph\n\n\nRBD\n\n\n\n\n\n\nDell EMC\n\n\nScaleIO\n, \nIsilon\n\n\n\n\n\n\nDigitalOcean\n\n\nBlock Storage\n\n\n\n\n\n\nFittedCloud\n\n\nEBS Optimizer\n\n\n\n\n\n\nGoogle\n\n\nGCE Persistent Disk\n\n\n\n\n\n\nMicrosoft\n\n\nAzure Unmanaged Disk\n\n\n\n\n\n\nVirtualBox\n\n\nVirtual Media\n\n\n\n\n\n\n\n\nOperating System Support\n\n\nThe following operating systems (OS) are supported by REX-Ray:\n\n\n\n\n\n\n\n\nOS\n\n\nCommand Line\n\n\nService\n\n\n\n\n\n\n\n\n\n\nUbuntu 12+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nDebian 6+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nRedHat\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nCentOS 6+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nCoreOS\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nTinyLinux (boot2docker)\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nOS X Yosemite+\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nWindows\n\n\nNo\n\n\nNo\n\n\n\n\n\n\n\n\nContainer Runtime Support\n\n\nREX-Ray currently supports the following container platforms:\n\n\n\n\n\n\n\n\nPlatform\n\n\nUse\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nVolume Driver Plugin\n\n\n\n\n\n\nMesos\n\n\nVolume Driver Isolator module\n\n\n\n\n\n\nMesos + Docker\n\n\nVolume Driver Plugin\n\n\n\n\n\n\n\n\nContainer Orchestration Support\n\n\nREX-Ray currently supports the following container orchestrators:\n\n\n\n\n\n\n\n\nOrchestrator\n\n\nContainer Runtime\n\n\n\n\n\n\n\n\n\n\nDocker Swarm\n\n\nDocker\n\n\n\n\n\n\nKubernetes\n\n\nDocker\n\n\n\n\n\n\nMarathon\n\n\nDocker, Mesos Containerizer\n\n\n\n\n\n\n\n\nQuick Start\n\n\nInstall\n\n\nThe following command will download the most recent and stable build of REX-Ray\nand install to \n/usr/bin/rexray\n on Linux systems. REX-Ray will be registered\nas either a SystemD or SystemV service depending upon the OS.\n\n\n$ curl -sSL https://dl.bintray.com/emccode/rexray/install | sh\n\n\n\n\nConfigure\n\n\nREX-Ray requires a configuration file for storing details used to communicate\nwith storage providers. This can include authentication credentials and driver-\nspecific configuration options. Use the \nREX-Ray Configuration\nGenerator\n or refer to the libStorage\nStorage Providers \ndocumentation\n for sample configurations of all supported\nstorage platforms. Additionally, look at\n\ncore properties\n \n\n\nlogging\n for advanced\nconfigurations.\n\n\nCreate a configuration file on the host at \n/etc/rexray/config.yml\n. Here is a\nsimple example for using Oracle VirtualBox:\n\n\nlibstorage:\n  service: virtualbox\n\n\n\n\nRefer to the \nVirtualBox\ndocumentation\n for additional VirtualBox configuration options.\n\n\nStart the VirtualBox SOAP API service using:\n\n\n$ vboxwebsrv -H 0.0.0.0 -v\n\n\n\n\nFrom here, REX-Ray can now be used as a command line tool. View the commands\navailable:\n\n\n$ rexray --help\n\n\n\n\nTo verify the configuration file is working, use REX-Ray to list the volumes:\n\n\n$ rexray volume ls\nID                                    Name             Status    Size\n1b819454-a280-4cff-aff5-141f4e8fd154  libStorage.vmdk  attached  16\n\n\n\n\nIf there is an error, use the \n-l debug\n flag and consult \ndebugging\ninstructions\n.\n\n\nStart as a Service\n\n\nContainer platforms rely on REX-Ray to be running as a service to function\nproperly. For instance, Docker communicates to the REX-Ray Volume Driver via a\nUNIX socket file.\n\n\n$ rexray service start\n\n\n\n\nDemo\n\n\nView the \nVagrant Demo\n as well as visit the \n{code}\nLabs\n for more information on ways to\nsetup REX-Ray and run different types of applications such as Postgres and\nMinecraft.\n\n\nGetting Help\n\n\nHaving issues? No worries, let's figure it out together.\n\n\nDebug\n\n\nThe \n-l debug\n flag can be appended to any command in order to get verbose\noutput. The following command will list all of the volumes visible to REX-Ray\nwith debug logging enabled:\n\n\n$ rexray volume -l debug ls\n\n\n\n\nFor an example of the full output from the above command, please refer to this\n\nGist\n.\n\n\nGitHub and Slack\n\n\nIf a little extra help is needed, please don't hesitate to use \nGitHub\nissues\n or join the active\nconversation on the \n{code} by Dell EMC Community Slack\nTeam\n in the #project-rexray channel", 
            "title": "Home"
        }, 
        {
            "location": "/#rex-ray", 
            "text": "Openly serious about storage   REX-Ray is an open source, storage management solution designed to support\ncontainer runtimes such as Docker and Mesos. REX-Ray enables stateful\napplications, such as databases, to persist and maintain its data after the life\ncycle of the container has ended. Built-in high availability enables\norchestrators such as  Docker Swarm , Kubernetes , and  Mesos\nFrameworks  like Marathon  to automatically orchestrate\nstorage tasks between hosts in a cluster.  Built on top of the  libStorage \nframework, REX-Ray's simplified architecture consists of a single binary and\nruns as a stateless service on every host using a configuration file to\norchestrate multiple storage platforms.    note  The current REX-Ray release omits support for several, previously verified\nstorage platforms. These providers will be reintroduced incrementally,\nbeginning with 0.4.1. If an absent driver prevents the use of REX-Ray,\nplease continue to use 0.3.3 until such time the storage platform is re-\nintroduced as a part of the libStorage  framework.\nInstructions on how to  install \nand  configure  REX-Ray 0.3.3 are\nboth available.", 
            "title": "REX-Ray"
        }, 
        {
            "location": "/#supported-technologies", 
            "text": "", 
            "title": "Supported Technologies"
        }, 
        {
            "location": "/#storage-provider-support", 
            "text": "The following storage providers and platforms are supported by REX-Ray.     Provider  Storage Platform(s)      Amazon EC2  EBS ,  EFS ,  S3FS    Ceph  RBD    Dell EMC  ScaleIO ,  Isilon    DigitalOcean  Block Storage    FittedCloud  EBS Optimizer    Google  GCE Persistent Disk    Microsoft  Azure Unmanaged Disk    VirtualBox  Virtual Media", 
            "title": "Storage Provider Support"
        }, 
        {
            "location": "/#operating-system-support", 
            "text": "The following operating systems (OS) are supported by REX-Ray:     OS  Command Line  Service      Ubuntu 12+  Yes  Yes    Debian 6+  Yes  Yes    RedHat  Yes  Yes    CentOS 6+  Yes  Yes    CoreOS  Yes  Yes    TinyLinux (boot2docker)  Yes  Yes    OS X Yosemite+  Yes  No    Windows  No  No", 
            "title": "Operating System Support"
        }, 
        {
            "location": "/#container-runtime-support", 
            "text": "REX-Ray currently supports the following container platforms:     Platform  Use      Docker  Volume Driver Plugin    Mesos  Volume Driver Isolator module    Mesos + Docker  Volume Driver Plugin", 
            "title": "Container Runtime Support"
        }, 
        {
            "location": "/#container-orchestration-support", 
            "text": "REX-Ray currently supports the following container orchestrators:     Orchestrator  Container Runtime      Docker Swarm  Docker    Kubernetes  Docker    Marathon  Docker, Mesos Containerizer", 
            "title": "Container Orchestration Support"
        }, 
        {
            "location": "/#quick-start", 
            "text": "", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#install", 
            "text": "The following command will download the most recent and stable build of REX-Ray\nand install to  /usr/bin/rexray  on Linux systems. REX-Ray will be registered\nas either a SystemD or SystemV service depending upon the OS.  $ curl -sSL https://dl.bintray.com/emccode/rexray/install | sh", 
            "title": "Install"
        }, 
        {
            "location": "/#configure", 
            "text": "REX-Ray requires a configuration file for storing details used to communicate\nwith storage providers. This can include authentication credentials and driver-\nspecific configuration options. Use the  REX-Ray Configuration\nGenerator  or refer to the libStorage\nStorage Providers  documentation  for sample configurations of all supported\nstorage platforms. Additionally, look at core properties    logging  for advanced\nconfigurations.  Create a configuration file on the host at  /etc/rexray/config.yml . Here is a\nsimple example for using Oracle VirtualBox:  libstorage:\n  service: virtualbox  Refer to the  VirtualBox\ndocumentation  for additional VirtualBox configuration options.  Start the VirtualBox SOAP API service using:  $ vboxwebsrv -H 0.0.0.0 -v  From here, REX-Ray can now be used as a command line tool. View the commands\navailable:  $ rexray --help  To verify the configuration file is working, use REX-Ray to list the volumes:  $ rexray volume ls\nID                                    Name             Status    Size\n1b819454-a280-4cff-aff5-141f4e8fd154  libStorage.vmdk  attached  16  If there is an error, use the  -l debug  flag and consult  debugging\ninstructions .", 
            "title": "Configure"
        }, 
        {
            "location": "/#start-as-a-service", 
            "text": "Container platforms rely on REX-Ray to be running as a service to function\nproperly. For instance, Docker communicates to the REX-Ray Volume Driver via a\nUNIX socket file.  $ rexray service start", 
            "title": "Start as a Service"
        }, 
        {
            "location": "/#demo", 
            "text": "View the  Vagrant Demo  as well as visit the  {code}\nLabs  for more information on ways to\nsetup REX-Ray and run different types of applications such as Postgres and\nMinecraft.", 
            "title": "Demo"
        }, 
        {
            "location": "/#getting-help", 
            "text": "Having issues? No worries, let's figure it out together.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/#debug", 
            "text": "The  -l debug  flag can be appended to any command in order to get verbose\noutput. The following command will list all of the volumes visible to REX-Ray\nwith debug logging enabled:  $ rexray volume -l debug ls  For an example of the full output from the above command, please refer to this Gist .", 
            "title": "Debug"
        }, 
        {
            "location": "/#github-and-slack", 
            "text": "If a little extra help is needed, please don't hesitate to use  GitHub\nissues  or join the active\nconversation on the  {code} by Dell EMC Community Slack\nTeam  in the #project-rexray channel", 
            "title": "GitHub and Slack"
        }, 
        {
            "location": "/user-guide/installation/", 
            "text": "Installation\n\n\nGetting the bits, bit by bit\n\n\n\n\nOverview\n\n\nThere are several different methods available for installing REX-Ray. It\nis written in Go, so there are typically no dependencies that must be installed\nalongside its single binary file. The manual methods can be extremely simple\nthrough tools like \ncurl\n. You also have the opportunity to perform install\nsteps individually. Following the manual installs, \nconfiguration\n\nmust take place.\n\n\nGreat examples of automation tools, such as \nAnsible\n and \nPuppet\n, are also\nprovided. These approaches automate the entire configuration process.\n\n\nManual Installs\n\n\nManual installations are in contrast to batch, automated installations.\n\n\nMake sure that before installing REX-Ray that you have uninstalled any previous\nversions. A \nrexray uninstall\n can assist with this where appropriate.\n\n\nFollowing an installation and configuration, you can use REX-Ray interactively\nthrough commands like \nrexray volume\n. Noticeably different from this is having\nREX-Ray integrate with Container Engines such as Docker. This requires that\nyou run \nrexray start\n or relevant service start command like\n\nsystemctl start rexray\n.\n\n\nInstall via curl\n\n\nThe following command will download the most recent, stable build of REX-Ray\nand install it to \n/usr/bin/rexray\n or \n/opt/bin/rexray\n. On Linux systems\nREX-Ray will also be registered as either a SystemD or SystemV service.\n\n\nThere is an optional flag to choose which version to install. Notice how we\nspecify \nstable\n, see the additional version names below that are also valid.\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- stable\n\n\n\n\nInstall a pre-built binary\n\n\nThere are a handful of necessary manual steps to properly install REX-Ray\nfrom pre-built binaries.\n\n\n\n\n\n\nDownload the proper binary. There are also pre-built binaries available for\nthe various release types.\n\n\n\n\n\n\n\n\nVersion\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUnstable\n\n\nThe most up-to-date, bleeding-edge, and often unstable REX-Ray binaries.\n\n\n\n\n\n\nStaged\n\n\nThe most up-to-date, release candidate REX-Ray binaries.\n\n\n\n\n\n\nStable\n\n\nThe most up-to-date, stable REX-Ray binaries.\n\n\n\n\n\n\n\n\n\n\n\n\nUncompress and move the binary to the proper location. Preferably \n/usr/bin\n\nshould be where REX-Ray is moved, but this path is not required.\n\n\n\n\nInstall as a service with \nrexray install\n. This will register itself\nwith SystemD or SystemV for proper initialization.\n\n\n\n\nBuild and install from source\n\n\nIt is also easy to build REX-Ray from source using Docker:\n\n\n$ git clone https://github.com/codedellemc/rexray \n make -C rexray\n\n\n\n\nFor building REX-Ray without Docker or to review the various build options\nplease see the \nBuild Reference\n.\n\n\nUninstall\n\n\nDepending on how it was installed, REX-Ray can be installed one of a few ways:\n\n\nRPM\n\n\nIf REX-Ray was installed on a system that uses the RPM package management\nsystem, such as Redhat, CentOS, the following command can be used to uninstall\nREX-Ray:\n\n\n$ sudo rpm -e rexray\n\n\n\n\nDEB\n\n\nIf REX-Ray was installed on a system that uses the DEB package management\nsystem, such as Debian, Ubuntu, the following command can be used to uninstall\nREX-Ray:\n\n\n$ sudo dpkg --remove rexray\n\n\n\n\nDefault\n\n\nNo matter how REX-Ray was installed, the following command will always attempt\nto perform an uninstallation using the OS-recommended method:\n\n\n$ sudo rexray uninstall\n\n\n\n\nAutomated Installs\n\n\nBecause REX-Ray is simple to install using the \ncurl\n script, installation\nusing configuration management tools is relatively easy as well. However,\nthere are a few areas that may prove to be tricky, such as writing the\nconfiguration file.\n\n\nThis section provides examples of automated installations using common\nconfiguration management and orchestration tools.\n\n\nAnsible\n\n\nWith Ansible, installing the latest REX-Ray binaries can be accomplished by\nincluding the \nemccode.rexray\n role from Ansible Galaxy.  The role accepts\nall the necessary variables to properly fill out your \nconfig.yml\n file.\n\n\nInstall the role from Galaxy:\n\n\n$ ansible-galaxy install emccode.rexray\n\n\n\n\nExample playbook for installing REX-Ray on GCE Docker hosts:\n\n\n- hosts: gce_docker_hosts\n  roles:\n  - { role: emccode.rexray,\n      rexray_service: true,\n      rexray_storage_drivers: [gce],\n      rexray_gce_keyfile: \n/opt/gce_keyfile\n }\n\n\n\n\nRun the playbook:\n\n\n$ ansible-playbook -i \ninventory\n playbook.yml\n\n\n\n\nAWS CloudFormation\n\n\nWith CloudFormation, the installation of the latest Docker and REX-Ray binaries\ncan be passed to the orchestrator using the 'UserData' property in a\nCloudFormation template. While the payload could also be provided as raw user\ndata via the AWS GUI, it would not sustain scalable automation.\n\n\nProperties\n: {\n  \nUserData\n: {\n    \nFn::Base64\n: {\n      \nFn::Join\n: [\n, [\n        \n#!/bin/bash -xe\\n\n,\n        \napt-get update\\n\n,\n        \napt-get -y install python-setuptools\\n\n,\n        \neasy_install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\\n\n,\n        \nln -s /usr/local/lib/python2.7/dist-packages/aws_cfn_bootstrap-1.4-py2.7.egg/init/ubuntu/cfn-hup /etc/init.d/cfn-hup\\n\n,\n        \nchmod +x /etc/init.d/cfn-hup\\n\n,\n        \nupdate-rc.d cfn-hup defaults\\n \n,\n        \nservice cfn-hup start\\n\n,\n        \n/usr/local/bin/cfn-init --stack \n, {\n          \nRef\n: \nAWS::StackName\n\n        }, \n --resource RexrayInstance \n, \n --configsets InstallAndRun --region \n, {\n          \nRef\n: \nAWS::Region\n\n        }, \n\\n\n,\n\n        \n# Install the latest Docker..\\n\n,\n        \n/usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com/\\n\n,\n        \nchmod +x /tmp/install-docker.sh\\n\n,\n        \n/tmp/install-docker.sh\\n\n,\n\n        \n# add the ubuntu user to the docker group..\\n\n,\n        \n/usr/sbin/usermod -G docker ubuntu\\n\n,\n\n        \n# Install the latest REX-ray\\n\n,\n        \n/usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\\n\n,\n        \nchmod +x /tmp/install-rexray.sh\\n\n,\n        \n/tmp/install-rexray.sh\\n\n,\n        \nchgrp docker /etc/rexray/config.yml\\n\n,\n        \nreboot\\n\n\n      ]]\n    }\n  }\n}\n\n\n\n\nDocker Machine (VirtualBox)\n\n\nSSH can be used to remotely deploy REX-Ray to a Docker Machine. While the\nfollowing example used VirtualBox as the underlying storage platform, the\nprovided \nconfig.yml\n file \ncould\n be modified to use any of the supported\ndrivers.\n\n\n\n\n\n\nSSH into the Docker machine and install REX-Ray.\n\n\n$ docker-machine ssh testing1 \\\n\"curl -sSL https://dl.bintray.com/emccode/rexray/install | sh\"\n\n\n\n\n\n\n\nInstall the udev extras package. This step is only required for versions of\n   boot2docker older than 1.10.\n\n\n$ docker-machine ssh testing1 \\\n\"wget http://tinycorelinux.net/6.x/x86_64/tcz/udev-extra.tcz \\\n\n tce-load -i udev-extra.tcz \n sudo udevadm trigger\"\n\n\n\n\n\n\n\nCreate a basic REX-Ray configuration file inside the Docker machine.\n\n\nNote\n: It is recommended to replace the \nvolumePath\n parameter with the\nlocal path VirtualBox uses to store its virtual media disk files.\n\n\n$ docker-machine ssh testing1 \\\n    \"sudo tee -a /etc/rexray/config.yml \n EOF\n    libstorage:\n      integration:\n        volume:\n          operations:\n            mount:\n              preempt: false\n    virtualbox:\n      volumePath: $HOME/VirtualBox/Volumes\n    \"\n\n\n\n\n\n\n\nFinally, start the REX-Ray service inside the Docker machine.\n\n\n$ docker-machine ssh testing1 \"sudo rexray start\"\n\n\n\n\n\n\n\nOpenStack Heat\n\n\nUsing OpenStack Heat, in the HOT template format (yaml):\n\n\nresources:\n  my_server:\n    type: OS::Nova::Server\n    properties:\n      user_data_format: RAW\n      user_data:\n        str_replace:\n          template: |\n            #!/bin/bash -v\n            /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com\n            chmod +x /tmp/install-docker.sh\n            /tmp/install-docker.sh\n            /usr/sbin/usermod -G docker ubuntu\n            /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\n            chmod +x /tmp/install-rexray.sh\n            /tmp/install-rexray.sh\n            chgrp docker /etc/rexray/config.yml\n          params:\n            dummy: \n\n\n\n\n\nVagrant\n\n\nUsing Vagrant is a great option to deploy pre-configured REX-Ray nodes,\nincluding Docker, using the VirtualBox driver. All volume requests are handled\nusing VirtualBox's Virtual Media.\n\n\nA Vagrant environment and instructions using it are provided\n\nhere\n.", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/installation/#installation", 
            "text": "Getting the bits, bit by bit", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/installation/#overview", 
            "text": "There are several different methods available for installing REX-Ray. It\nis written in Go, so there are typically no dependencies that must be installed\nalongside its single binary file. The manual methods can be extremely simple\nthrough tools like  curl . You also have the opportunity to perform install\nsteps individually. Following the manual installs,  configuration \nmust take place.  Great examples of automation tools, such as  Ansible  and  Puppet , are also\nprovided. These approaches automate the entire configuration process.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/installation/#manual-installs", 
            "text": "Manual installations are in contrast to batch, automated installations.  Make sure that before installing REX-Ray that you have uninstalled any previous\nversions. A  rexray uninstall  can assist with this where appropriate.  Following an installation and configuration, you can use REX-Ray interactively\nthrough commands like  rexray volume . Noticeably different from this is having\nREX-Ray integrate with Container Engines such as Docker. This requires that\nyou run  rexray start  or relevant service start command like systemctl start rexray .", 
            "title": "Manual Installs"
        }, 
        {
            "location": "/user-guide/installation/#install-via-curl", 
            "text": "The following command will download the most recent, stable build of REX-Ray\nand install it to  /usr/bin/rexray  or  /opt/bin/rexray . On Linux systems\nREX-Ray will also be registered as either a SystemD or SystemV service.  There is an optional flag to choose which version to install. Notice how we\nspecify  stable , see the additional version names below that are also valid.  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- stable", 
            "title": "Install via curl"
        }, 
        {
            "location": "/user-guide/installation/#install-a-pre-built-binary", 
            "text": "There are a handful of necessary manual steps to properly install REX-Ray\nfrom pre-built binaries.    Download the proper binary. There are also pre-built binaries available for\nthe various release types.     Version  Description      Unstable  The most up-to-date, bleeding-edge, and often unstable REX-Ray binaries.    Staged  The most up-to-date, release candidate REX-Ray binaries.    Stable  The most up-to-date, stable REX-Ray binaries.       Uncompress and move the binary to the proper location. Preferably  /usr/bin \nshould be where REX-Ray is moved, but this path is not required.   Install as a service with  rexray install . This will register itself\nwith SystemD or SystemV for proper initialization.", 
            "title": "Install a pre-built binary"
        }, 
        {
            "location": "/user-guide/installation/#build-and-install-from-source", 
            "text": "It is also easy to build REX-Ray from source using Docker:  $ git clone https://github.com/codedellemc/rexray   make -C rexray  For building REX-Ray without Docker or to review the various build options\nplease see the  Build Reference .", 
            "title": "Build and install from source"
        }, 
        {
            "location": "/user-guide/installation/#uninstall", 
            "text": "Depending on how it was installed, REX-Ray can be installed one of a few ways:", 
            "title": "Uninstall"
        }, 
        {
            "location": "/user-guide/installation/#rpm", 
            "text": "If REX-Ray was installed on a system that uses the RPM package management\nsystem, such as Redhat, CentOS, the following command can be used to uninstall\nREX-Ray:  $ sudo rpm -e rexray", 
            "title": "RPM"
        }, 
        {
            "location": "/user-guide/installation/#deb", 
            "text": "If REX-Ray was installed on a system that uses the DEB package management\nsystem, such as Debian, Ubuntu, the following command can be used to uninstall\nREX-Ray:  $ sudo dpkg --remove rexray", 
            "title": "DEB"
        }, 
        {
            "location": "/user-guide/installation/#default", 
            "text": "No matter how REX-Ray was installed, the following command will always attempt\nto perform an uninstallation using the OS-recommended method:  $ sudo rexray uninstall", 
            "title": "Default"
        }, 
        {
            "location": "/user-guide/installation/#automated-installs", 
            "text": "Because REX-Ray is simple to install using the  curl  script, installation\nusing configuration management tools is relatively easy as well. However,\nthere are a few areas that may prove to be tricky, such as writing the\nconfiguration file.  This section provides examples of automated installations using common\nconfiguration management and orchestration tools.", 
            "title": "Automated Installs"
        }, 
        {
            "location": "/user-guide/installation/#ansible", 
            "text": "With Ansible, installing the latest REX-Ray binaries can be accomplished by\nincluding the  emccode.rexray  role from Ansible Galaxy.  The role accepts\nall the necessary variables to properly fill out your  config.yml  file.  Install the role from Galaxy:  $ ansible-galaxy install emccode.rexray  Example playbook for installing REX-Ray on GCE Docker hosts:  - hosts: gce_docker_hosts\n  roles:\n  - { role: emccode.rexray,\n      rexray_service: true,\n      rexray_storage_drivers: [gce],\n      rexray_gce_keyfile:  /opt/gce_keyfile  }  Run the playbook:  $ ansible-playbook -i  inventory  playbook.yml", 
            "title": "Ansible"
        }, 
        {
            "location": "/user-guide/installation/#aws-cloudformation", 
            "text": "With CloudFormation, the installation of the latest Docker and REX-Ray binaries\ncan be passed to the orchestrator using the 'UserData' property in a\nCloudFormation template. While the payload could also be provided as raw user\ndata via the AWS GUI, it would not sustain scalable automation.  Properties : {\n   UserData : {\n     Fn::Base64 : {\n       Fn::Join : [ , [\n         #!/bin/bash -xe\\n ,\n         apt-get update\\n ,\n         apt-get -y install python-setuptools\\n ,\n         easy_install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\\n ,\n         ln -s /usr/local/lib/python2.7/dist-packages/aws_cfn_bootstrap-1.4-py2.7.egg/init/ubuntu/cfn-hup /etc/init.d/cfn-hup\\n ,\n         chmod +x /etc/init.d/cfn-hup\\n ,\n         update-rc.d cfn-hup defaults\\n  ,\n         service cfn-hup start\\n ,\n         /usr/local/bin/cfn-init --stack  , {\n           Ref :  AWS::StackName \n        },   --resource RexrayInstance  ,   --configsets InstallAndRun --region  , {\n           Ref :  AWS::Region \n        },  \\n ,\n\n         # Install the latest Docker..\\n ,\n         /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com/\\n ,\n         chmod +x /tmp/install-docker.sh\\n ,\n         /tmp/install-docker.sh\\n ,\n\n         # add the ubuntu user to the docker group..\\n ,\n         /usr/sbin/usermod -G docker ubuntu\\n ,\n\n         # Install the latest REX-ray\\n ,\n         /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\\n ,\n         chmod +x /tmp/install-rexray.sh\\n ,\n         /tmp/install-rexray.sh\\n ,\n         chgrp docker /etc/rexray/config.yml\\n ,\n         reboot\\n \n      ]]\n    }\n  }\n}", 
            "title": "AWS CloudFormation"
        }, 
        {
            "location": "/user-guide/installation/#docker-machine-virtualbox", 
            "text": "SSH can be used to remotely deploy REX-Ray to a Docker Machine. While the\nfollowing example used VirtualBox as the underlying storage platform, the\nprovided  config.yml  file  could  be modified to use any of the supported\ndrivers.    SSH into the Docker machine and install REX-Ray.  $ docker-machine ssh testing1 \\\n\"curl -sSL https://dl.bintray.com/emccode/rexray/install | sh\"    Install the udev extras package. This step is only required for versions of\n   boot2docker older than 1.10.  $ docker-machine ssh testing1 \\\n\"wget http://tinycorelinux.net/6.x/x86_64/tcz/udev-extra.tcz \\  tce-load -i udev-extra.tcz   sudo udevadm trigger\"    Create a basic REX-Ray configuration file inside the Docker machine.  Note : It is recommended to replace the  volumePath  parameter with the\nlocal path VirtualBox uses to store its virtual media disk files.  $ docker-machine ssh testing1 \\\n    \"sudo tee -a /etc/rexray/config.yml   EOF\n    libstorage:\n      integration:\n        volume:\n          operations:\n            mount:\n              preempt: false\n    virtualbox:\n      volumePath: $HOME/VirtualBox/Volumes\n    \"    Finally, start the REX-Ray service inside the Docker machine.  $ docker-machine ssh testing1 \"sudo rexray start\"", 
            "title": "Docker Machine (VirtualBox)"
        }, 
        {
            "location": "/user-guide/installation/#openstack-heat", 
            "text": "Using OpenStack Heat, in the HOT template format (yaml):  resources:\n  my_server:\n    type: OS::Nova::Server\n    properties:\n      user_data_format: RAW\n      user_data:\n        str_replace:\n          template: |\n            #!/bin/bash -v\n            /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com\n            chmod +x /tmp/install-docker.sh\n            /tmp/install-docker.sh\n            /usr/sbin/usermod -G docker ubuntu\n            /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\n            chmod +x /tmp/install-rexray.sh\n            /tmp/install-rexray.sh\n            chgrp docker /etc/rexray/config.yml\n          params:\n            dummy:", 
            "title": "OpenStack Heat"
        }, 
        {
            "location": "/user-guide/installation/#vagrant", 
            "text": "Using Vagrant is a great option to deploy pre-configured REX-Ray nodes,\nincluding Docker, using the VirtualBox driver. All volume requests are handled\nusing VirtualBox's Virtual Media.  A Vagrant environment and instructions using it are provided here .", 
            "title": "Vagrant"
        }, 
        {
            "location": "/user-guide/config/", 
            "text": "Configuring REX-Ray\n\n\nTweak this, turn that, peek behind the curtain...\n\n\n\n\nOverview\n\n\nThis page reviews how to configure REX-Ray to suit any environment, beginning\nwith the most common use cases, exploring recommended guidelines, and\nfinally, delving into the details of more advanced settings.\n\n\nBasic Configuration\n\n\nThis section outlines the two most common configuration scenarios encountered\nby REX-Ray's users:\n\n\n\n\nREX-Ray as a stand-alone CLI tool\n\n\nREX-Ray as a service\n\n\n\n\n\n\nnote\n\n\nPlease remember to replace the placeholders in the following examples\nwith values valid for the systems on which the examples are executed.\n\n\nThe example below specifies the \nvolumePath\n property as\n\n$HOME/VirtualBox/Volumes\n. While the text \n$HOME\n will be replaced with\nthe actual value for that environment variable at runtime, the path may\nstill be invalid. The \nvolumePath\n property should reflect a path on the\nsystem on which the VirtualBox server is running, and that is not always\nthe same system on which the \nlibStorage\n server is running.\n\n\nSo please, make sure to update the \nvolumePath\n property for the VirtualBox\ndriver to a path valid on the system on which the VirtualBox server is\nrunning.\n\n\nThe same goes for VirtualBox property \nendpoint\n as the VirtualBox\nweb service is not always available at \n10.0.2.2:18083\n.\n\n\n\n\nStand-alone CLI Mode\n\n\nIt is possible to use REX-Ray directly from the command line without any\nconfiguration files. The following example uses REX-Ray to list the storage\nvolumes available to a Linux VM hosted by VirtualBox:\n\n\n\n\nnote\n\n\nThe examples below assume that the VirtualBox web server is running on the\nhost OS with authentication disabled and accessible to the guest OS. For\nmore information please refer to the VirtualBox storage driver\n\ndocumentation\n.\n\n\n\n\n$ rexray volume --service virtualbox ls\nID                                    Name             Status    Size\n1b819454-a280-4cff-aff5-141f4e8fd154  libStorage.vmdk  attached  16\n\n\n\n\nIn addition to listing volumes, the REX-Ray CLI can be used to create and\nremove them as well as manage volume snapshots. For an end-to-end example of\nvolume creation, see \nHello REX-Ray\n.\n\n\nEmbedded Server Mode\n\n\nWhen operating as a stand-alone CLI, REX-Ray actually loads an embedded\nlibStorage server for the duration of the CLI process and is accessible by\nonly the process that hosts it. This is known as \nEmbedded Server Mode\n.\n\n\nWhile commonly used when executing one-off commands with REX-Ray as a\nstand-alone CLI tool, Embedded Server Mode can be utilized when configuring\nREX-Ray to advertise a static libStorage server as well. The following\nqualifications must be met for Embedded Server Mode to be activated:\n\n\n\n\n\n\nThe property \nlibstorage.host\n must not be defined via configuration file,\n   environment variable, or CLI flag\n\n\n\n\n\n\nIf the \nlibstorage.host\n property \nis\n defined then the property\n   \nlibstorage.embedded\n can be set to \ntrue\n to explicitly activate\n   Embedded Server Mode.\n\n\n\n\n\n\nIf the \nlibstorage.host\n property is set and \nlibtorage.embedded\n is\n   set to true, Embedded Server Mode will still only activate if the address\n   specified by \nlibstorage.host\n (whether a UNIX socket or TCP port) is\n   not currently in use.\n\n\n\n\n\n\nAuto Service Mode\n\n\nThe Stand-alone CLI Mode \nexample\n also uses the\n\n--service\n flag. This flag's argument sets the \nlibstorage.service\n property,\nwhich has a special meaning inside of REX-Ray -- it serves to enabled\n\nAuto Service Mode\n.\n\n\nServices represent unique libStorage endpoints that are available to libStorage\nclients. Each service is associated with a storage driver. Thus\nAuto Service Mode minimizes configuration for simple environments.\n\n\nThe value of the \nlibstorage.service\n property is used to create a default\nservice configured with a storage driver. This special mode is only activated\nif all of the following conditions are met:\n\n\n\n\nThe \nlibstorage.service\n property is set via:\n\n\nThe CLI flags \n-s|--service\n or \n--libstorageService\n\n\nThe environment variable \nLIBSTORAGE_SERVICE\n\n\nThe configuration file property \nlibstorage.service\n\n\n\n\n\n\nThe \nlibstorage.host\n property is \nnot\n set. This property can be set via:\n\n\nThe CLI flags \n-h|--host\n or \n--libstorageHost\n\n\nThe environment variable \nLIBSTORAGE_HOST\n\n\nThe configuration file property \nlibstorage.host\n\n\n\n\n\n\nThe configuration property \nlibstorage.server.services\n must \nnot\n be set.\n    This property is only configurable via a configuration file.\n\n\n\n\nBecause the above example met the auto service mode conditions, REX-Ray\ncreated a service named \nvirtualbox\n configured to use the \nvirtualbox\n driver.\nThis service runs on the libStorage server embedded inside of REX-Ray and is\naccessible only by the executing CLI process for the duration of said process.\nWhen used in this manner, the service name must also be a valid driver name.\n\n\nService Mode\n\n\nREX-Ray can also run as a persistent service that advertises both\n\nDocker Volume Plug-in\n\nand \nlibStorage\n endpoints.\n\n\nDocker Volume Plug-in\n\n\nThis section refers to the only operational mode that REX-Ray supported in\nversions 0.3.3 and prior. A UNIX socket is created by REX-Ray that serves as a\nDocker Volume Plugin compliant API endpoint. Docker is able to leverage this\nendpoint to deliver on-demand, persistent storage to containers.\n\n\nThe following is a simple example of a configuration file that should be\nlocated at \n/etc/rexray/config.yml\n. This file can be used to configure the\nsame options that were specified in the previous CLI example. Please see the\n\nadvanced section\n for a complete list of\nconfiguration options.\n\n\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nOnce the configuration file is in place, \nrexray service start\n can be used to\nstart the service. Sometimes it is also useful to add \n-l debug\n to enable\nmore verbose logging. Additionally, it's also occasionally beneficial to\nstart the service in the foreground with the \n-f\n flag.\n\n\n$ rexray start\n\nStarting REX-Ray...SUCCESS!\n\n  The REX-Ray daemon is now running at PID 15724. To\n  shutdown the daemon execute the following command:\n\n    sudo /usr/bin/rexray stop\n\n\n\n\nAt this point requests can now be made to the default Docker Volume Plugin\nand Volume Driver advertised by the UNIX socket \nrexray\n at\n\n/run/docker/plugins/rexray.sock\n. More details on configuring the Docker\nVolume Plug-in are available on the \nSchedulers\n page.\n\n\nlibStorage Server and Client\n\n\nIn addition to \nEmbedded Server Mode\n, REX-Ray can also\nexpose the libStorage API statically. This enables REX-Ray to serve a\nlibStorage server and perform only a storage abstraction role.\n\n\nIf the desire is to establish a centralized REX-Ray server that is called\non from remote REX-Ray instances then the following example will be useful.\nThe first configuration is for running REX-Ray purely as a libStorage server.\nThe second defines how one would would use one or more REX-Ray instances in a\nlibStorage client role.\n\n\nThe following examples require multiple systems in order to fulfill these\ndifferent roles. The \nHello REX-Ray\n section on\nthe front page has an end-to-end illustration of this use case that leverages\nVagrant to provide and configure the necessary systems.\n\n\nlibStorage Server\n\n\nThe example below illustrates the necessary settings for configuring REX-Ray\nas a libStorage server:\n\n\nrexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host: tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      virtualbox:\n        driver: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nIn the above sample, the default Docker module is disabled. This means that\nwhile the REX-Ray service would be running, it would not be available to\nDocker on that host.\n\n\nThe \nlibstorage\n section defines the settings that configure the libStorage\nserver:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlibstorage.host\n\n\nInstructs local clients which libStorage endpoint to access\n\n\n\n\n\n\nlibstorage.embedded\n\n\nIndicates the libStorage server should be started even though the \nlibstorage.host\n property is defined\n\n\n\n\n\n\nlibstorage.client.type\n\n\nWhen set to \ncontroller\n this property indicates local clients perform no integration activities\n\n\n\n\n\n\nlibstorage.server.endpoints\n\n\nThe available libStorage server HTTP endpoints\n\n\n\n\n\n\nlibstorage.server.services\n\n\nThe configured libStorage services\n\n\n\n\n\n\n\n\nStart the REX-Ray service with \nrexray service start\n.\n\n\nlibStorage Client\n\n\nOn a separate OS instance running REX-Ray, the follow command can be used to\nlist the instance's available VirtualBox storage volumes:\n\n\n$ rexray volume -h tcp://REXRAY_SERVER:7979 -s virtualbox\n\n\n\n\nAn alternative to the above CLI flags is to add them as persistent settings\nto the \n/etc/rexray/config.yml\n configuration file on this instance:\n\n\nlibstorage:\n  host:    tcp://REXRAY_SERVER:7979\n  service: virtualbox\n\n\n\n\nNow the above command can be simplified further:\n\n\n$ rexray volume\n\n\n\n\nOnce more, the REX-Ray service can be started with \nrexray service start\n and\nthe REX-Ray Docker Volume Plug-in endpoint will utilize the remote libStorage\nserver as its method for communicating with VirtualBox.\n\n\nAgain, a complete end-to-end Vagrant environment for the above example is\navailable at \nHello REX-Ray\n.\n\n\nExample sans Modules\n\n\nLets review the major sections of the configuration file:\n\n\nrexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nSettings occur in three primary areas:\n\n\n\n\nrexray\n\n\nlibstorage\n\n\nvirtualbox\n\n\n\n\nThe \nrexray\n section contains all properties specific to REX-Ray. The\nYAML property path \nrexray.logLevel\n defines the log level for REX-Ray and its\nchild components. All of the \nrexray\n properties are\n\ndocumented\n below.\n\n\nNext, the \nlibstorage\n section defines the service with which REX-Ray will\ncommunicate via the property \nlibstorage.service\n. This property also enables\nthe \nAuto Service Mode\n discussed above since this\nconfiguration example does not define a host or services section. For all\ninformation related to libStorage and its properties, please refer to the\n\nlibStorage documentation\n.\n\n\nFinally, the \nvirtualbox\n section configures the VirtualBox driver selected\nor loaded by REX-Ray, as indicated via the \nlibstorage.service\n property. The\nlibStorage Storage Drivers page has information about the configuration details\nof \neach driver\n,\nincluding \nVirtualBox\n.\n\n\nLogging\n\n\nThe \n-l|--logLevel\n option or \nrexray.logLevel\n configuration key can be set\nto any of the following values to increase or decrease the verbosity of the\ninformation logged to the console or the REX-Ray log file (defaults to\n\n/var/log/rexray/rexray.log\n).\n\n\n\n\npanic\n\n\nfatal\n\n\nerror\n\n\nwarn\n\n\ninfo\n\n\ndebug\n\n\n\n\nTroubleshooting\n\n\nThe command \nrexray env\n can be used to print out the runtime interpretation\nof the environment, including configured properties, in order to help diagnose\nconfiguration issues.\n\n\n$ rexray env | grep DEFAULT | sort -r\nREXRAY_MODULES_DEFAULT-DOCKER_TYPE=docker\nREXRAY_MODULES_DEFAULT-DOCKER_SPEC=/etc/docker/plugins/rexray.spec\nREXRAY_MODULES_DEFAULT-DOCKER_LIBSTORAGE_SERVICE=vfs\nREXRAY_MODULES_DEFAULT-DOCKER_HOST=unix:///run/docker/plugins/rexray.sock\nREXRAY_MODULES_DEFAULT-DOCKER_DISABLED=false\nREXRAY_MODULES_DEFAULT-DOCKER_DESC=The default docker module.\nREXRAY_MODULES_DEFAULT-ADMIN_TYPE=admin\nREXRAY_MODULES_DEFAULT-ADMIN_HOST=unix:///var/run/rexray/server.sock\nREXRAY_MODULES_DEFAULT-ADMIN_DISABLED=false\nREXRAY_MODULES_DEFAULT-ADMIN_DESC=The default admin module.\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_TYPE=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_SIZE=16\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_IOPS=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_FSTYPE=ext4\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_AVAILABILITYZONE=\n\n\n\n\nAdvanced Configuration\n\n\nThe following sections detail every last aspect of how REX-Ray works and can\nbe configured.\n\n\nExample with Modules\n\n\nModules enable a single REX-Ray instance to present multiple personalities or\nvolume endpoints, serving hosts that require access to multiple storage\nplatforms.\n\n\nDefining Modules\n\n\nThe following example demonstrates a basic configuration that presents two\nmodules using the VirtualBox driver: \ndefault-docker\n and \nvb2-module\n.\n\n\nrexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      type: docker\n      desc: The default docker module.\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\n    vb2-module:\n      type: docker\n      desc: The second docker module.\n      host: unix:///run/docker/plugins/vb2.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\nlibstorage:\n  service: virtualbox\n\n\n\n\nWhereas the previous example did not use modules and the example above does,\nthey both begin by defining the root section \nrexray\n. Unlike the previous\nexample, however, the majority of the \nlibstorage\n section and all of the\n\nvirtualbox\n section are no longer at the root. Instead the section\n\nrexray.modules\n is defined. The \nmodules\n key in the \nrexray\n section is where\nall modules are configured. Each key that is a child of \nmodules\n represents the\nname of a module.\n\n\n\n\nnote\n\n\nPlease note that while most of the \nlibstorage\n section has been relocated\nas a child of each module, the \nlibstorage.service\n property is still\ndefined at the root to activate \nAuto Service Mode\n as\na quick-start method of property configuring the embedded libStorage server.\n\n\n\n\nThe above example defines two modules:\n\n\n\n\n\n\ndefault-module\n\n\nThis is a special module, and it's always defined, even if not explicitly\nlisted. In the previous example without modules, the \nlibstorage\n and\n\nvirtualbox\n sections at the root actually informed the configuration of\nthe implicit \ndefault-docker\n module. In this example the explicit\ndeclaration of the \ndefault-docker\n module enables several of its\nproperties to be overridden and given desired values. The Advanced\nConfiguration section has more information on\n\nDefault Modules\n.\n\n\n\n\n\n\nvb2-module\n\n\nThis is a new, custom module configured almost identically to the\n\ndefault-module\n with the exception of a unique host address as defined\nby the module's \nhost\n key.\n\n\n\n\n\n\nNotice that both modules share many of the same properties and values. In fact,\nwhen defining both modules, the top-level \nlibstorage\n and \nvirtualbox\n sections\nwere simply copied into each module as sub-sections. This is perfectly valid\nas any configuration path that begins from the root of the REX-Ray\nconfiguration file can be duplicated beginning as a child of a module\ndefinition. This allows global settings to be overridden just for a specific\nmodules.\n\n\nAs noted, each module shares identical values with the exception of the module's\nname and host. The host is the address used by Docker to communicate with\nREX-Ray. The base name of the socket file specified in the address can be\nused with \ndocker --volume-driver=\n. With the current example the value of the\n\n--volume-driver\n parameter would be either \nvb1\n of \nvb2\n.\n\n\nModules and Inherited Properties\n\n\nThere is also another way to write the previous example while reducing the\nnumber of repeated, identical properties shared by two modules.\n\n\nrexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n    vb2:\n      type: docker\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nThe above example may look strikingly different than the previous one, but it's\nactually the same with just a few tweaks.\n\n\nWhile there are still two modules defined, the second one has been renamed from\n\nvb2-module\n to \nvb2\n. The change is a more succinct way to represent the same\nintent, and it also provides a nice side-effect. If the \nhost\n key is omitted\nfrom a Docker module, the value for the \nhost\n key is automatically generated\nusing the module's name. Therefore since there is no \nhost\n key for the \nvb2\n\nmodule, the value will be \nunix:///run/docker/plugins/vb2.sock\n.\n\n\nAdditionally, \nvirtualbox\n sections from each module definition have been\nremoved and now only a single, global \nvirtualbox\n section is present at the\nroot. When accessing properties, a module will first attempt to access a\nproperty defined in the context of the module, but if that fails the property\nlookup will resolve against globally defined keys as well.\n\n\nFinally, the \nlibstorage\n section has been completely removed from the \nvb2\n\nmodule whereas it still remains in the \ndefault-docker\n section. Volume\ncreation requests without an explicit size value sent to the \ndefault-docker\n\nmodule will result in 1GB volumes whereas the same request sent to the \nvb2\n\nmodule will result in 16GB volumes (since 16GB is the default value for the\n\nlibstorage.integration.volume.operations.create.default.size\n property).\n\n\nDefining Service Endpoints\n\n\nMultiple libStorage services can be defined in order to leverage several\ndifferent combinations of storage provider drivers and their respective\nconfigurations. The following section illustrates how to define two separate\nservices, one using the ScaleIO driver and one using VirtualBox:\n\n\nrexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  server:\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nOnce the services have been defined, it is then up to the modules to specify\nwhich service to use. Notice how the \ndefault-docker\n module specifies\nthe \nvirtualbox\n service as its \nlibstorage.service\n. Any requests to the\nDocker Volume Plug-in endpoint \n/run/docker/plugins/virtualbox.sock\n will\nutilize the libStorage service \nvirtualbox\n on the backend.\n\n\nDefining a libStorage Server\n\n\nThe following example is very similar to the previous one, but in this instance\nthere is a centralized REX-Ray server which services requests from many\nREX-Ray clients.\n\n\nrexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host:     tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nOne of the larger differences between the above example and the previous one is\nthe removal of the module definitions. Docker does not communicate with the\ncentral REX-Ray server directly; instead Docker interacts with the REX-Ray\nservices running on the clients via their Docker Volume Endpoints. The client\nREX-Ray instances then send all storage-related requests to the central REX-Ray\nserver.\n\n\nAdditionally, the above sample configuration introduces a few new properties:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlibstorage.host\n\n\nInstructs local clients which libStorage endpoint to access\n\n\n\n\n\n\nlibstorage.embedded\n\n\nIndicates the libStorage server should be started even though the \nlibstorage.host\n property is defined\n\n\n\n\n\n\nlibstorage.client.type\n\n\nWhen set to \ncontroller\n this property indicates local clients perform no integration activities\n\n\n\n\n\n\nlibstorage.server.endpoints\n\n\nThe available libStorage server HTTP endpoints\n\n\n\n\n\n\n\n\nDefining a libStorage Client\n\n\nThe client configuration is still rather simple. As mentioned in the previous\nsection, the \nrexray.modules\n configuration occurs here. This enables the Docker\nengines running on remote instances to communicate with local REX-Ray exposed\nDocker Volume endpoints that then handle the storage-related requests via the\ncentralized REX-Ray server.\n\n\nrexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  host: tcp://REXRAY_SERVER:7979\n\n\n\n\nlibStorage Configuration\n\n\nREX-Ray embeds both the libStorage client as well as the libStorage server. For\ninformation on configuring the following, please refer to the\n\nlibStorage documentation\n:\n\n\n\n\nVolume options\n\n   such as preemption, disabling operations, etc.\n\n\nFine-tuning \nlogging\n\n\nConfiguring\n\n   OS, integration, and storage drivers\n\n\n\n\nData Directories\n\n\nThe first time REX-Ray is executed it will create several directories if\nthey do not already exist:\n\n\n\n\n/etc/rexray\n\n\n/var/log/rexray\n\n\n/var/run/rexray\n\n\n/var/lib/rexray\n\n\n\n\nThe above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable \nREXRAY_HOME\n.\n\n\nREXRAY_HOME\n can be used to define a custom home directory for REX-Ray.\nThis directory is irrespective of the actual REX-Ray binary. Instead, the\ndirectory specified in \nREXRAY_HOME\n is the root directory where the REX-Ray\nbinary expects all of the program's data directories to be located.\n\n\nFor example, the following command sets a custom value for \nREXRAY_HOME\n and\nthen gets a volume list:\n\n\nenv REXRAY_HOME=/tmp/rexray rexray volume\n\n\n\n\nThe above command would produce a list of volumes and create the following\ndirectories in the process:\n\n\n\n\n/tmp/rexray/etc/rexray\n\n\n/tmp/rexray/var/log/rexray\n\n\n/tmp/rexray/var/run/rexray\n\n\n/tmp/rexray/var/lib/rexray\n\n\n\n\nThe entire configuration section will refer to the global configuration file as\na file located inside of \n/etc/rexray\n, but it should be noted that if\n\nREXRAY_HOME\n is set the location of the global configuration file can be\nchanged.\n\n\nConfiguration Methods\n\n\nThere are three ways to configure REX-Ray:\n\n\n\n\nCommand line options\n\n\nEnvironment variables\n\n\nConfiguration files\n\n\n\n\nThe order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.\n\n\nConfiguration Files\n\n\nThere are two REX-Ray configuration files - global and user:\n\n\n\n\n/etc/rexray/config.yml\n\n\n$HOME/.rexray/config.yml\n\n\n\n\nPlease note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts REX-Ray. And\nif REX-Ray is being started as a service, then \nsudo\n is likely being used,\nwhich means that \n$HOME/.rexray/config.yml\n won't point to \nyour\n home\ndirectory, but rather \n/root/.rexray/config.yml\n.\n\n\nThe next section has an example configuration with the default configuration.\n\n\nConfiguration Properties\n\n\nThe section \nConfiguration Methods\n mentions there are\nthree ways to configure REX-Ray: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.\n\n\nHere is a sample REX-Ray configuration:\n\n\nrexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes\n\n\n\n\nThe properties \nrexray.logLevel\n, \nlibstorage.service\n, and\n\nvirtualbox.volumePath\n are strings. These values can also be set via\nenvironment variables or command line interface (CLI) flags, but to do so\nrequires knowing the names of the environment variables or CLI flags to use.\nLuckily those are very easy to figure out just by knowing the property names.\n\n\nAll properties that might appear in the REX-Ray configuration file\nfall under some type of heading. For example, take the configuration above:\n\n\nThe rule for environment variables is as follows:\n\n\n\n\nEach nested level becomes a part of the environment variable name followed\n    by an underscore \n_\n except for the terminating part.\n\n\nThe entire environment variable name is uppercase.\n\n\n\n\nNested properties follow these rules for CLI flags:\n\n\n\n\nThe root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.\n\n\nThe remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.\n\n\nAll levels are then concatenated together.\n\n\nSee the verbose help for exact global flags using \nrexray --help -v\n\n    as they may be chopped to minimize verbosity.\n\n\n\n\nThe following table illustrates the transformations:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nEnvironment Variable\n\n\nCLI Flag\n\n\n\n\n\n\n\n\n\n\nrexray.logLevel\n\n\nREXRAY_LOGLEVEL\n\n\n--logLevel\n\n\n\n\n\n\nlibstorage.service\n\n\nLIBSTORAGE_SERVICE\n\n\n--libstorageService\n\n\n\n\n\n\nvirtualbox.volumePath\n\n\nVIRTUALBOX_VOLUMEPATH\n\n\n--virtualboxVolumePath\n\n\n\n\n\n\n\n\nLogging Configuration\n\n\nThe REX-Ray log file is, by default, stored at \n/var/log/rexray/rexray.log\n.\n\n\nLog Levels\n\n\nThe REX-Ray log level determines the level of verbosity emitted by the\ninternal logger. The default level is \nwarn\n, but there are three other levels\nas well:\n\n\n\n\n\n\n\n\nLog Level\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nerror\n\n\nLog only errors\n\n\n\n\n\n\nwarn\n\n\nLog errors and anything out of place\n\n\n\n\n\n\ninfo\n\n\nLog errors, warnings, and workflow messages\n\n\n\n\n\n\ndebug\n\n\nLog everything\n\n\n\n\n\n\n\n\nFor example, the following two commands may look slightly different, but they\nare functionally the same, both printing a list of volumes using the \ndebug\n\nlog level:\n\n\nUse the \ndebug\n log level - Example 1\n\n\nrexray volume -l debug ls\n\n\n\n\nUse the \ndebug\n log level - Example 2\n\n\nenv REXRAY_LOGLEVEL=debug rexray volume ls\n\n\n\n\nVerbose mode\n\n\nTo enable the most verbose logging, use the following configuration snippet:\n\n\nrexray:\n  logLevel:        debug\nlibstorage:\n  logging:\n    level:         debug\n    httpRequests:  true\n    httpResponses: true\n\n\n\n\nThe following command line example is the equivalent to the above configuration\nexample:\n\n\n$ REXRAY_DEBUG=true \\\n  LIBSTORAGE_LOGGING_HTTPREQUESTS=true \\\n  LIBSTORAGE_LOGGING_HTTPRESPONSES=true \\\n  rexray ...", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/config/#configuring-rex-ray", 
            "text": "Tweak this, turn that, peek behind the curtain...", 
            "title": "Configuring REX-Ray"
        }, 
        {
            "location": "/user-guide/config/#overview", 
            "text": "This page reviews how to configure REX-Ray to suit any environment, beginning\nwith the most common use cases, exploring recommended guidelines, and\nfinally, delving into the details of more advanced settings.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/config/#basic-configuration", 
            "text": "This section outlines the two most common configuration scenarios encountered\nby REX-Ray's users:   REX-Ray as a stand-alone CLI tool  REX-Ray as a service    note  Please remember to replace the placeholders in the following examples\nwith values valid for the systems on which the examples are executed.  The example below specifies the  volumePath  property as $HOME/VirtualBox/Volumes . While the text  $HOME  will be replaced with\nthe actual value for that environment variable at runtime, the path may\nstill be invalid. The  volumePath  property should reflect a path on the\nsystem on which the VirtualBox server is running, and that is not always\nthe same system on which the  libStorage  server is running.  So please, make sure to update the  volumePath  property for the VirtualBox\ndriver to a path valid on the system on which the VirtualBox server is\nrunning.  The same goes for VirtualBox property  endpoint  as the VirtualBox\nweb service is not always available at  10.0.2.2:18083 .", 
            "title": "Basic Configuration"
        }, 
        {
            "location": "/user-guide/config/#stand-alone-cli-mode", 
            "text": "It is possible to use REX-Ray directly from the command line without any\nconfiguration files. The following example uses REX-Ray to list the storage\nvolumes available to a Linux VM hosted by VirtualBox:   note  The examples below assume that the VirtualBox web server is running on the\nhost OS with authentication disabled and accessible to the guest OS. For\nmore information please refer to the VirtualBox storage driver documentation .   $ rexray volume --service virtualbox ls\nID                                    Name             Status    Size\n1b819454-a280-4cff-aff5-141f4e8fd154  libStorage.vmdk  attached  16  In addition to listing volumes, the REX-Ray CLI can be used to create and\nremove them as well as manage volume snapshots. For an end-to-end example of\nvolume creation, see  Hello REX-Ray .", 
            "title": "Stand-alone CLI Mode"
        }, 
        {
            "location": "/user-guide/config/#embedded-server-mode", 
            "text": "When operating as a stand-alone CLI, REX-Ray actually loads an embedded\nlibStorage server for the duration of the CLI process and is accessible by\nonly the process that hosts it. This is known as  Embedded Server Mode .  While commonly used when executing one-off commands with REX-Ray as a\nstand-alone CLI tool, Embedded Server Mode can be utilized when configuring\nREX-Ray to advertise a static libStorage server as well. The following\nqualifications must be met for Embedded Server Mode to be activated:    The property  libstorage.host  must not be defined via configuration file,\n   environment variable, or CLI flag    If the  libstorage.host  property  is  defined then the property\n    libstorage.embedded  can be set to  true  to explicitly activate\n   Embedded Server Mode.    If the  libstorage.host  property is set and  libtorage.embedded  is\n   set to true, Embedded Server Mode will still only activate if the address\n   specified by  libstorage.host  (whether a UNIX socket or TCP port) is\n   not currently in use.", 
            "title": "Embedded Server Mode"
        }, 
        {
            "location": "/user-guide/config/#auto-service-mode", 
            "text": "The Stand-alone CLI Mode  example  also uses the --service  flag. This flag's argument sets the  libstorage.service  property,\nwhich has a special meaning inside of REX-Ray -- it serves to enabled Auto Service Mode .  Services represent unique libStorage endpoints that are available to libStorage\nclients. Each service is associated with a storage driver. Thus\nAuto Service Mode minimizes configuration for simple environments.  The value of the  libstorage.service  property is used to create a default\nservice configured with a storage driver. This special mode is only activated\nif all of the following conditions are met:   The  libstorage.service  property is set via:  The CLI flags  -s|--service  or  --libstorageService  The environment variable  LIBSTORAGE_SERVICE  The configuration file property  libstorage.service    The  libstorage.host  property is  not  set. This property can be set via:  The CLI flags  -h|--host  or  --libstorageHost  The environment variable  LIBSTORAGE_HOST  The configuration file property  libstorage.host    The configuration property  libstorage.server.services  must  not  be set.\n    This property is only configurable via a configuration file.   Because the above example met the auto service mode conditions, REX-Ray\ncreated a service named  virtualbox  configured to use the  virtualbox  driver.\nThis service runs on the libStorage server embedded inside of REX-Ray and is\naccessible only by the executing CLI process for the duration of said process.\nWhen used in this manner, the service name must also be a valid driver name.", 
            "title": "Auto Service Mode"
        }, 
        {
            "location": "/user-guide/config/#service-mode", 
            "text": "REX-Ray can also run as a persistent service that advertises both Docker Volume Plug-in \nand  libStorage  endpoints.", 
            "title": "Service Mode"
        }, 
        {
            "location": "/user-guide/config/#docker-volume-plug-in", 
            "text": "This section refers to the only operational mode that REX-Ray supported in\nversions 0.3.3 and prior. A UNIX socket is created by REX-Ray that serves as a\nDocker Volume Plugin compliant API endpoint. Docker is able to leverage this\nendpoint to deliver on-demand, persistent storage to containers.  The following is a simple example of a configuration file that should be\nlocated at  /etc/rexray/config.yml . This file can be used to configure the\nsame options that were specified in the previous CLI example. Please see the advanced section  for a complete list of\nconfiguration options.  libstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  Once the configuration file is in place,  rexray service start  can be used to\nstart the service. Sometimes it is also useful to add  -l debug  to enable\nmore verbose logging. Additionally, it's also occasionally beneficial to\nstart the service in the foreground with the  -f  flag.  $ rexray start\n\nStarting REX-Ray...SUCCESS!\n\n  The REX-Ray daemon is now running at PID 15724. To\n  shutdown the daemon execute the following command:\n\n    sudo /usr/bin/rexray stop  At this point requests can now be made to the default Docker Volume Plugin\nand Volume Driver advertised by the UNIX socket  rexray  at /run/docker/plugins/rexray.sock . More details on configuring the Docker\nVolume Plug-in are available on the  Schedulers  page.", 
            "title": "Docker Volume Plug-in"
        }, 
        {
            "location": "/user-guide/config/#libstorage-server-and-client", 
            "text": "In addition to  Embedded Server Mode , REX-Ray can also\nexpose the libStorage API statically. This enables REX-Ray to serve a\nlibStorage server and perform only a storage abstraction role.  If the desire is to establish a centralized REX-Ray server that is called\non from remote REX-Ray instances then the following example will be useful.\nThe first configuration is for running REX-Ray purely as a libStorage server.\nThe second defines how one would would use one or more REX-Ray instances in a\nlibStorage client role.  The following examples require multiple systems in order to fulfill these\ndifferent roles. The  Hello REX-Ray  section on\nthe front page has an end-to-end illustration of this use case that leverages\nVagrant to provide and configure the necessary systems.", 
            "title": "libStorage Server and Client"
        }, 
        {
            "location": "/user-guide/config/#libstorage-server", 
            "text": "The example below illustrates the necessary settings for configuring REX-Ray\nas a libStorage server:  rexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host: tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      virtualbox:\n        driver: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  In the above sample, the default Docker module is disabled. This means that\nwhile the REX-Ray service would be running, it would not be available to\nDocker on that host.  The  libstorage  section defines the settings that configure the libStorage\nserver:     Property  Description      libstorage.host  Instructs local clients which libStorage endpoint to access    libstorage.embedded  Indicates the libStorage server should be started even though the  libstorage.host  property is defined    libstorage.client.type  When set to  controller  this property indicates local clients perform no integration activities    libstorage.server.endpoints  The available libStorage server HTTP endpoints    libstorage.server.services  The configured libStorage services     Start the REX-Ray service with  rexray service start .", 
            "title": "libStorage Server"
        }, 
        {
            "location": "/user-guide/config/#libstorage-client", 
            "text": "On a separate OS instance running REX-Ray, the follow command can be used to\nlist the instance's available VirtualBox storage volumes:  $ rexray volume -h tcp://REXRAY_SERVER:7979 -s virtualbox  An alternative to the above CLI flags is to add them as persistent settings\nto the  /etc/rexray/config.yml  configuration file on this instance:  libstorage:\n  host:    tcp://REXRAY_SERVER:7979\n  service: virtualbox  Now the above command can be simplified further:  $ rexray volume  Once more, the REX-Ray service can be started with  rexray service start  and\nthe REX-Ray Docker Volume Plug-in endpoint will utilize the remote libStorage\nserver as its method for communicating with VirtualBox.  Again, a complete end-to-end Vagrant environment for the above example is\navailable at  Hello REX-Ray .", 
            "title": "libStorage Client"
        }, 
        {
            "location": "/user-guide/config/#example-sans-modules", 
            "text": "Lets review the major sections of the configuration file:  rexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        create:\n          default:\n            size: 1\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  Settings occur in three primary areas:   rexray  libstorage  virtualbox   The  rexray  section contains all properties specific to REX-Ray. The\nYAML property path  rexray.logLevel  defines the log level for REX-Ray and its\nchild components. All of the  rexray  properties are documented  below.  Next, the  libstorage  section defines the service with which REX-Ray will\ncommunicate via the property  libstorage.service . This property also enables\nthe  Auto Service Mode  discussed above since this\nconfiguration example does not define a host or services section. For all\ninformation related to libStorage and its properties, please refer to the libStorage documentation .  Finally, the  virtualbox  section configures the VirtualBox driver selected\nor loaded by REX-Ray, as indicated via the  libstorage.service  property. The\nlibStorage Storage Drivers page has information about the configuration details\nof  each driver ,\nincluding  VirtualBox .", 
            "title": "Example sans Modules"
        }, 
        {
            "location": "/user-guide/config/#logging", 
            "text": "The  -l|--logLevel  option or  rexray.logLevel  configuration key can be set\nto any of the following values to increase or decrease the verbosity of the\ninformation logged to the console or the REX-Ray log file (defaults to /var/log/rexray/rexray.log ).   panic  fatal  error  warn  info  debug", 
            "title": "Logging"
        }, 
        {
            "location": "/user-guide/config/#troubleshooting", 
            "text": "The command  rexray env  can be used to print out the runtime interpretation\nof the environment, including configured properties, in order to help diagnose\nconfiguration issues.  $ rexray env | grep DEFAULT | sort -r\nREXRAY_MODULES_DEFAULT-DOCKER_TYPE=docker\nREXRAY_MODULES_DEFAULT-DOCKER_SPEC=/etc/docker/plugins/rexray.spec\nREXRAY_MODULES_DEFAULT-DOCKER_LIBSTORAGE_SERVICE=vfs\nREXRAY_MODULES_DEFAULT-DOCKER_HOST=unix:///run/docker/plugins/rexray.sock\nREXRAY_MODULES_DEFAULT-DOCKER_DISABLED=false\nREXRAY_MODULES_DEFAULT-DOCKER_DESC=The default docker module.\nREXRAY_MODULES_DEFAULT-ADMIN_TYPE=admin\nREXRAY_MODULES_DEFAULT-ADMIN_HOST=unix:///var/run/rexray/server.sock\nREXRAY_MODULES_DEFAULT-ADMIN_DISABLED=false\nREXRAY_MODULES_DEFAULT-ADMIN_DESC=The default admin module.\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_TYPE=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_SIZE=16\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_IOPS=\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_FSTYPE=ext4\nLIBSTORAGE_INTEGRATION_VOLUME_OPERATIONS_CREATE_DEFAULT_AVAILABILITYZONE=", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/config/#advanced-configuration", 
            "text": "The following sections detail every last aspect of how REX-Ray works and can\nbe configured.", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/user-guide/config/#example-with-modules", 
            "text": "Modules enable a single REX-Ray instance to present multiple personalities or\nvolume endpoints, serving hosts that require access to multiple storage\nplatforms.", 
            "title": "Example with Modules"
        }, 
        {
            "location": "/user-guide/config/#defining-modules", 
            "text": "The following example demonstrates a basic configuration that presents two\nmodules using the VirtualBox driver:  default-docker  and  vb2-module .  rexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      type: docker\n      desc: The default docker module.\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\n    vb2-module:\n      type: docker\n      desc: The second docker module.\n      host: unix:///run/docker/plugins/vb2.sock\n      libstorage:\n        service: virtualbox\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n      virtualbox:\n        volumePath: $HOME/VirtualBox/Volumes\nlibstorage:\n  service: virtualbox  Whereas the previous example did not use modules and the example above does,\nthey both begin by defining the root section  rexray . Unlike the previous\nexample, however, the majority of the  libstorage  section and all of the virtualbox  section are no longer at the root. Instead the section rexray.modules  is defined. The  modules  key in the  rexray  section is where\nall modules are configured. Each key that is a child of  modules  represents the\nname of a module.   note  Please note that while most of the  libstorage  section has been relocated\nas a child of each module, the  libstorage.service  property is still\ndefined at the root to activate  Auto Service Mode  as\na quick-start method of property configuring the embedded libStorage server.   The above example defines two modules:    default-module  This is a special module, and it's always defined, even if not explicitly\nlisted. In the previous example without modules, the  libstorage  and virtualbox  sections at the root actually informed the configuration of\nthe implicit  default-docker  module. In this example the explicit\ndeclaration of the  default-docker  module enables several of its\nproperties to be overridden and given desired values. The Advanced\nConfiguration section has more information on Default Modules .    vb2-module  This is a new, custom module configured almost identically to the default-module  with the exception of a unique host address as defined\nby the module's  host  key.    Notice that both modules share many of the same properties and values. In fact,\nwhen defining both modules, the top-level  libstorage  and  virtualbox  sections\nwere simply copied into each module as sub-sections. This is perfectly valid\nas any configuration path that begins from the root of the REX-Ray\nconfiguration file can be duplicated beginning as a child of a module\ndefinition. This allows global settings to be overridden just for a specific\nmodules.  As noted, each module shares identical values with the exception of the module's\nname and host. The host is the address used by Docker to communicate with\nREX-Ray. The base name of the socket file specified in the address can be\nused with  docker --volume-driver= . With the current example the value of the --volume-driver  parameter would be either  vb1  of  vb2 .", 
            "title": "Defining Modules"
        }, 
        {
            "location": "/user-guide/config/#modules-and-inherited-properties", 
            "text": "There is also another way to write the previous example while reducing the\nnumber of repeated, identical properties shared by two modules.  rexray:\n  logLevel: warn\n  modules:\n    default-docker:\n      host: unix:///run/docker/plugins/vb1.sock\n      libstorage:\n        integration:\n          volume:\n            operations:\n              create:\n                default:\n                  size: 1\n    vb2:\n      type: docker\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  The above example may look strikingly different than the previous one, but it's\nactually the same with just a few tweaks.  While there are still two modules defined, the second one has been renamed from vb2-module  to  vb2 . The change is a more succinct way to represent the same\nintent, and it also provides a nice side-effect. If the  host  key is omitted\nfrom a Docker module, the value for the  host  key is automatically generated\nusing the module's name. Therefore since there is no  host  key for the  vb2 \nmodule, the value will be  unix:///run/docker/plugins/vb2.sock .  Additionally,  virtualbox  sections from each module definition have been\nremoved and now only a single, global  virtualbox  section is present at the\nroot. When accessing properties, a module will first attempt to access a\nproperty defined in the context of the module, but if that fails the property\nlookup will resolve against globally defined keys as well.  Finally, the  libstorage  section has been completely removed from the  vb2 \nmodule whereas it still remains in the  default-docker  section. Volume\ncreation requests without an explicit size value sent to the  default-docker \nmodule will result in 1GB volumes whereas the same request sent to the  vb2 \nmodule will result in 16GB volumes (since 16GB is the default value for the libstorage.integration.volume.operations.create.default.size  property).", 
            "title": "Modules and Inherited Properties"
        }, 
        {
            "location": "/user-guide/config/#defining-service-endpoints", 
            "text": "Multiple libStorage services can be defined in order to leverage several\ndifferent combinations of storage provider drivers and their respective\nconfigurations. The following section illustrates how to define two separate\nservices, one using the ScaleIO driver and one using VirtualBox:  rexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  server:\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  Once the services have been defined, it is then up to the modules to specify\nwhich service to use. Notice how the  default-docker  module specifies\nthe  virtualbox  service as its  libstorage.service . Any requests to the\nDocker Volume Plug-in endpoint  /run/docker/plugins/virtualbox.sock  will\nutilize the libStorage service  virtualbox  on the backend.", 
            "title": "Defining Service Endpoints"
        }, 
        {
            "location": "/user-guide/config/#defining-a-libstorage-server", 
            "text": "The following example is very similar to the previous one, but in this instance\nthere is a centralized REX-Ray server which services requests from many\nREX-Ray clients.  rexray:\n  modules:\n    default-docker:\n      disabled: true\nlibstorage:\n  host:     tcp://127.0.0.1:7979\n  embedded: true\n  client:\n    type: controller\n  server:\n    endpoints:\n      public:\n        address: tcp://:7979\n    services:\n      scaleio:\n        driver: scaleio\n      virtualbox:\n        driver: virtualbox\nscaleio:\n  endpoint:             https://SCALEIO_GATEWAY/api\n  insecure:             true\n  userName:             SCALEIO_USER\n  password:             SCALEIO_PASS\n  systemName:           SCALEIO_SYSTEM_NAME\n  protectionDomainName: SCALEIO_DOMAIN_NAME\n  storagePoolName:      SCALEIO_STORAG_NAME\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  One of the larger differences between the above example and the previous one is\nthe removal of the module definitions. Docker does not communicate with the\ncentral REX-Ray server directly; instead Docker interacts with the REX-Ray\nservices running on the clients via their Docker Volume Endpoints. The client\nREX-Ray instances then send all storage-related requests to the central REX-Ray\nserver.  Additionally, the above sample configuration introduces a few new properties:     Property  Description      libstorage.host  Instructs local clients which libStorage endpoint to access    libstorage.embedded  Indicates the libStorage server should be started even though the  libstorage.host  property is defined    libstorage.client.type  When set to  controller  this property indicates local clients perform no integration activities    libstorage.server.endpoints  The available libStorage server HTTP endpoints", 
            "title": "Defining a libStorage Server"
        }, 
        {
            "location": "/user-guide/config/#defining-a-libstorage-client", 
            "text": "The client configuration is still rather simple. As mentioned in the previous\nsection, the  rexray.modules  configuration occurs here. This enables the Docker\nengines running on remote instances to communicate with local REX-Ray exposed\nDocker Volume endpoints that then handle the storage-related requests via the\ncentralized REX-Ray server.  rexray:\n  modules:\n    default-docker:\n      host:     unix:///run/docker/plugins/virtualbox.sock\n      spec:     /etc/docker/plugins/virtualbox.spec\n      libstorage:\n        service: virtualbox\n    scaleio-docker:\n      type:     docker\n      host:     unix:///run/docker/plugins/scaleio.sock\n      spec:     /etc/docker/plugins/scaleio.spec\n      libstorage:\n        service: scaleio\nlibstorage:\n  host: tcp://REXRAY_SERVER:7979", 
            "title": "Defining a libStorage Client"
        }, 
        {
            "location": "/user-guide/config/#libstorage-configuration", 
            "text": "REX-Ray embeds both the libStorage client as well as the libStorage server. For\ninformation on configuring the following, please refer to the libStorage documentation :   Volume options \n   such as preemption, disabling operations, etc.  Fine-tuning  logging  Configuring \n   OS, integration, and storage drivers", 
            "title": "libStorage Configuration"
        }, 
        {
            "location": "/user-guide/config/#data-directories", 
            "text": "The first time REX-Ray is executed it will create several directories if\nthey do not already exist:   /etc/rexray  /var/log/rexray  /var/run/rexray  /var/lib/rexray   The above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable  REXRAY_HOME .  REXRAY_HOME  can be used to define a custom home directory for REX-Ray.\nThis directory is irrespective of the actual REX-Ray binary. Instead, the\ndirectory specified in  REXRAY_HOME  is the root directory where the REX-Ray\nbinary expects all of the program's data directories to be located.  For example, the following command sets a custom value for  REXRAY_HOME  and\nthen gets a volume list:  env REXRAY_HOME=/tmp/rexray rexray volume  The above command would produce a list of volumes and create the following\ndirectories in the process:   /tmp/rexray/etc/rexray  /tmp/rexray/var/log/rexray  /tmp/rexray/var/run/rexray  /tmp/rexray/var/lib/rexray   The entire configuration section will refer to the global configuration file as\na file located inside of  /etc/rexray , but it should be noted that if REXRAY_HOME  is set the location of the global configuration file can be\nchanged.", 
            "title": "Data Directories"
        }, 
        {
            "location": "/user-guide/config/#configuration-methods", 
            "text": "There are three ways to configure REX-Ray:   Command line options  Environment variables  Configuration files   The order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.", 
            "title": "Configuration Methods"
        }, 
        {
            "location": "/user-guide/config/#configuration-files", 
            "text": "There are two REX-Ray configuration files - global and user:   /etc/rexray/config.yml  $HOME/.rexray/config.yml   Please note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts REX-Ray. And\nif REX-Ray is being started as a service, then  sudo  is likely being used,\nwhich means that  $HOME/.rexray/config.yml  won't point to  your  home\ndirectory, but rather  /root/.rexray/config.yml .  The next section has an example configuration with the default configuration.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/user-guide/config/#configuration-properties", 
            "text": "The section  Configuration Methods  mentions there are\nthree ways to configure REX-Ray: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.  Here is a sample REX-Ray configuration:  rexray:\n  logLevel: warn\nlibstorage:\n  service: virtualbox\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes  The properties  rexray.logLevel ,  libstorage.service , and virtualbox.volumePath  are strings. These values can also be set via\nenvironment variables or command line interface (CLI) flags, but to do so\nrequires knowing the names of the environment variables or CLI flags to use.\nLuckily those are very easy to figure out just by knowing the property names.  All properties that might appear in the REX-Ray configuration file\nfall under some type of heading. For example, take the configuration above:  The rule for environment variables is as follows:   Each nested level becomes a part of the environment variable name followed\n    by an underscore  _  except for the terminating part.  The entire environment variable name is uppercase.   Nested properties follow these rules for CLI flags:   The root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.  The remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.  All levels are then concatenated together.  See the verbose help for exact global flags using  rexray --help -v \n    as they may be chopped to minimize verbosity.   The following table illustrates the transformations:     Property Name  Environment Variable  CLI Flag      rexray.logLevel  REXRAY_LOGLEVEL  --logLevel    libstorage.service  LIBSTORAGE_SERVICE  --libstorageService    virtualbox.volumePath  VIRTUALBOX_VOLUMEPATH  --virtualboxVolumePath", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/user-guide/config/#logging-configuration", 
            "text": "The REX-Ray log file is, by default, stored at  /var/log/rexray/rexray.log .", 
            "title": "Logging Configuration"
        }, 
        {
            "location": "/user-guide/config/#log-levels", 
            "text": "The REX-Ray log level determines the level of verbosity emitted by the\ninternal logger. The default level is  warn , but there are three other levels\nas well:     Log Level  Description      error  Log only errors    warn  Log errors and anything out of place    info  Log errors, warnings, and workflow messages    debug  Log everything     For example, the following two commands may look slightly different, but they\nare functionally the same, both printing a list of volumes using the  debug \nlog level:  Use the  debug  log level - Example 1  rexray volume -l debug ls  Use the  debug  log level - Example 2  env REXRAY_LOGLEVEL=debug rexray volume ls", 
            "title": "Log Levels"
        }, 
        {
            "location": "/user-guide/config/#verbose-mode", 
            "text": "To enable the most verbose logging, use the following configuration snippet:  rexray:\n  logLevel:        debug\nlibstorage:\n  logging:\n    level:         debug\n    httpRequests:  true\n    httpResponses: true  The following command line example is the equivalent to the above configuration\nexample:  $ REXRAY_DEBUG=true \\\n  LIBSTORAGE_LOGGING_HTTPREQUESTS=true \\\n  LIBSTORAGE_LOGGING_HTTPRESPONSES=true \\\n  rexray ...", 
            "title": "Verbose mode"
        }, 
        {
            "location": "/user-guide/storage-providers/", 
            "text": "Storage Providers\n\n\nConnecting storage and platforms...\n\n\n\n\nOverview\n\n\nThe list of storage providers supported by REX-Ray now mirrors the validated\nstorage platform table from the libStorage project.\n\n\nSupported Providers\n\n\nThe following storage providers and platforms are supported by REX-Ray.\n\n\n\n\n\n\n\n\nProvider\n\n\nStorage Platform(s)\n\n\n\n\n\n\n\n\n\n\nAmazon EC2\n\n\nEBS\n, \nEFS\n, \nS3FS\n\n\n\n\n\n\nCeph\n\n\nRBD\n\n\n\n\n\n\nDell EMC\n\n\nScaleIO\n, \nIsilon\n\n\n\n\n\n\nDigitalOcean\n\n\nBlock Storage\n\n\n\n\n\n\nFittedCloud\n\n\nEBS Optimizer\n\n\n\n\n\n\nGoogle\n\n\nGCE Persistent Disk\n\n\n\n\n\n\nMicrosoft\n\n\nAzure Unmanaged Disk\n\n\n\n\n\n\nVirtualBox\n\n\nVirtual Media", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#storage-providers", 
            "text": "Connecting storage and platforms...", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#overview", 
            "text": "The list of storage providers supported by REX-Ray now mirrors the validated\nstorage platform table from the libStorage project.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/storage-providers/#supported-providers", 
            "text": "The following storage providers and platforms are supported by REX-Ray.     Provider  Storage Platform(s)      Amazon EC2  EBS ,  EFS ,  S3FS    Ceph  RBD    Dell EMC  ScaleIO ,  Isilon    DigitalOcean  Block Storage    FittedCloud  EBS Optimizer    Google  GCE Persistent Disk    Microsoft  Azure Unmanaged Disk    VirtualBox  Virtual Media", 
            "title": "Supported Providers"
        }, 
        {
            "location": "/user-guide/schedulers/", 
            "text": "Schedulers\n\n\nScheduling storage one resource at a time...\n\n\n\n\nOverview\n\n\nThis page reviews the scheduling systems supported by REX-Ray.\n\n\nDocker\n\n\nThe majority of the documentation for the Docker integration driver has been\n\nrelocated\n\nto the libStorage project.\n\n\nExternal Access\n\n\nBy default, REX-Ray's embedded Docker Volume Plug-in endpoint handles\nrequests from the local Docker service via a UNIX socket. Doing so\nrestricts the endpoint to the localhost, increasing network security by removing\na possible attack vector. If an externally accessible Docker Volume Plug-in\nendpoint is required, it's still possible to create one by overriding the\naddress for the \ndefault-docker\n module in REX-Ray's configuration file:\n\n\nrexray:\n  modules:\n    default-docker:\n      host: tcp://:7981\n\n\n\n\nThe above example illustrates how to override the \ndefault-docker\n module's\nendpoint address. The value \ntcp://:7981\n instructs the Docker Volume Plug-in\nto listen on port 7981 for all configured interfaces.\n\n\nUsing a TCP endpoint has a side-effect however -- the local Docker instance\nwill not know about the Volume Plug-in endpoint as there is no longer a UNIX\nsocket file in the directory the Docker service continually scans.\n\n\nOn the local system, and in fact on all systems where the Docker service needs\nto know about this externally accessible Volume Plug-in endpoint, a spec file\nmust be created at \n/etc/docker/plug-ins/rexray.spec\n. Inside this file simply\ninclude a single line with the network address of the endpoint. For example:\n\n\ntcp://192.168.56.20:7981\n\n\n\n\nWith a spec file located at \n/etc/docker/plug-ins/rexray.spec\n that contains\nthe above contents, Docker instances will query the Volume Plug-in endpoint at\n\ntcp://192.168.56.20:7981\n when volume requests are received.\n\n\nVolume Management\n\n\nThe \nvolume\n sub-command for Docker 1.12+ should look similar to the following:\n\n\n$ docker volume\n\nUsage:  docker volume [OPTIONS] [COMMAND]\n\nManage Docker volumes\n\nCommands:\n  create                   Create a volume\n  inspect                  Return low-level information on a volume\n  ls                       List volumes\n  rm                       Remove a volume\n\n\n\n\nList Volumes\n\n\nThe list command reviews a list of available volumes that have been discovered\nvia Docker Volume Plug-in endpoints such as REX-Ray. Each volume name is\nexpected to be unique. Thus volume names must also be unique across all\nendpoints, and in turn, across all storage platforms exposed by REX-Ray.\n\n\nWith the exception of the \nlocal\n driver, the list of returned volumes is\ngenerated by the backend storage platform to which the configured driver\ncommunicates:\n\n\n$ docker volume ls\nDRIVER              VOLUME NAME\nlocal               local1\nscaleio             Volume-001\nvirtualbox          vbox1\n\n\n\n\nInspect Volume\n\n\nThe inspect command can be used to retrieve details about a volume related to\nboth Docker and the underlying storage platform. The fields listed under\n\nStatus\n are all generated by REX-Ray, including \nSize in GB\n, \nVolume Type\n,\nand \nAvailability Zone\n.\n\n\nThe \nScope\n parameter ensures that when the specified volume driver is\ninspected by multiple Docker hosts, the volumes tagged as \nglobal\n are all\ninterpreted as the same volume. This reduces unnecessary round-trips in\nsituations where an application such as Docker Swarm is connected to hosts\nconfigured with REX-Ray.\n\n\n$ docker volume inspect vbox1\n[\n    {\n        \nName\n: \nvbox1\n,\n        \nDriver\n: \nvirtualbox\n,\n        \nMountpoint\n: \n,\n        \nStatus\n: {\n            \navailabilityZone\n: \n,\n            \nfields\n: null,\n            \niops\n: 0,\n            \nname\n: \nvbox1\n,\n            \nserver\n: \nvirtualbox\n,\n            \nservice\n: \nvirtualbox\n,\n            \nsize\n: 8,\n            \ntype\n: \n\n        },\n        \nLabels\n: {},\n        \nScope\n: \nglobal\n\n    }\n]\n\n\n\n\nCreate Volume\n\n\nDocker's \nvolume create\n command enables the creation of new volumes on the\nunderlying storage platform. Newly created volumes are available immediately\nto be attached and mounted. The \nvolume create\n command also supports the CLI\nflag \n-o|--opt\n in order to support providing custom data to the volume creation\nworkflow:\n\n\n$ docker volume create --driver=virtualbox --name=vbox2 --opt=size=2\nvbox2\n\n\n\n\nAdditional, valid options for the \n-o|--opt\n parameter include:\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nsize\n\n\nSize in GB\n\n\n\n\n\n\nIOPS\n\n\nIOPS\n\n\n\n\n\n\nvolumeType\n\n\nType of Volume or Storage Pool\n\n\n\n\n\n\nvolumeName\n\n\nCreate from an existing volume name\n\n\n\n\n\n\nvolumeID\n\n\nCreate from an existing volume ID\n\n\n\n\n\n\nsnapshotName\n\n\nCreate from an existing snapshot name\n\n\n\n\n\n\nsnapshotID\n\n\nCreate from an existing snapshot ID\n\n\n\n\n\n\n\n\nRemove Volume\n\n\nA volume may be removed once it is no longer in use by a container, running or\notherwise. The process of removing a container actually causes the volume to\nbe removed if that is the last container to leverage said volume:\n\n\n$ docker volume rm vbox2\n\n\n\n\nContainers with Volumes\n\n\nPlease review the \nApplications\n section for information on\nconfiguring popular applications with persistent storage via Docker and REX-Ray.\n\n\nKubernetes\n\n\nREX-Ray can be integrated with \nKubernetes\n allowing\npods to consume data stored on volumes that are orchestrated by REX-Ray. Using\nKubernetes' \nFlexVolume\n\nplug-in, REX-Ray can provide uniform access to storage operatations such as attach,\nmount, detach, and unmount for any configured storage provider.  REX-Ray provides an\nadapter script called \nFlexRex\n which integrates with the FlexVolume to interact\nwith the backing storage system.\n\n\nPre-Requisites\n\n\n\n\nKubernetes\n 1.5 or higher\n\n\nREX-Ray 0.7 or higher\n\n\njq binary\n\n\n\n\nInstallation\n\n\nIt is assumed that you have a Kubernetes cluster at your disposal. On each\nKubernetes node (running the kubelet), do the followings:\n\n\n\n\nInstall and configure the REX-Ray binary as prescribed in the\n\nInstallation\n section.  \n\n\nNext, validate the REX-Ray installation by running \nrexray volume ls\n\nas shown in the the following:\n\n\n\n\n# rexray volume ls\nID                Name   Status     Size\n925def7200000006  vol01  available  32\n925def7100000005  vol02  available  32\n\n\n\n\nIf there is no issue, you should see an output, similar to above, which shows\na list of previously created volumes. If instead you get an error,\n\nensure that REX-Ray is properly configured for the intended storage system.\n\n\nNext, using the REX-Ray binary,  install the \nFlexRex\n adapter script on the node\nas shown below.  \n\n\n# rexray flexrex install\n\n\n\n\nThis should produce the following output showing that the FlexRex script is\ninstalled successfully:\n\n\nPath                                                                        Installed  Modified\n/usr/libexec/kubernetes/kubelet-plug-ins/volume/exec/rexray~flexrex/flexrex  true       false\n\n\n\n\nThe path shown above is the default location where the FlexVolume plug-in will\nexpect to find its integration code.  If you are not using the default location\nwith FlexVolume, you can install the  \nFlexRex\n in an arbitrary location using:\n\n\n# rexray flexrex install --path /opt/plug-ins/rexray~flexrex/flexrex\n\n\n\n\nNext, restart the kublet process on the node:\n\n\n# systemctl restart kubelet\n\n\n\n\nYou can validate that the FlexRex script has been started successfully by searching\nthe kubelet log for an entry similar to the following:\n\n\nI0208 10:56:57.412207    5348 plug-ins.go:350] Loaded volume plug-in \nrexray/flexrex\n\n\n\n\n\nPods and Persistent Volumes\n\n\nYou can now deploy pods and persistent volumes that use storage systems orchestrated\nby REX-Ray.  It is worth pointing out that the Kubernetes FlexVolme plug-in can only\nattach volumes that already exist in the storge system.  Any volume that is to be used\nby a Kubernetes resource must be listed in a \nrexray volume ls\n command.\n\n\nPod with REX-Ray volume\n\n\nThe following YAML file shows the definition of a pod that uses FlexRex to attach a volume\nto be used by the pod.\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-0\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: pod-0\n    volumeMounts:\n    - mountPath: /test-pd\n      name: vol-0\n  volumes:\n  - name: vol-0\n    flexVolume:\n      driver: rexray/flexrex\n      fsType: ext4\n      options:\n        volumeID: test-vol-1\n        forceAttach: \ntrue\n\n        forceAttachDelay: \n15\n\n\n\n\n\nNotice in the section under \nflexVolume\n the name of the driver attribute\n\ndriver: rexray/flexrex\n. This is used by the FlexVolume plug-in to launch REX-Ray.\nAdditional options can be provided in the \noptions:\n as follows:\n\n\n\n\n\n\n\n\nOption\n\n\nDesription\n\n\n\n\n\n\n\n\n\n\nvolumeID\n\n\nReference name of the volume in REX-Ray\n\n\n\n\n\n\nforceAttach\n\n\nWhen true ensures the volume is availble before attahing (optinal, defaults to false)\n\n\n\n\n\n\nforceAttachDelay\n\n\nTotal amount of time (in sec) to attempt attachment with 5 sec interval between tries (optional)\n\n\n\n\n\n\n\n\nREX-Ray PersistentVolume\n\n\nThe next example shows a YAML definition of Persistent Volume (PV) managed\nby REX-Ray.\n\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: vol01\nspec:\n  capacity:\n    storage: 32Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  flexVolume:\n    driver: rexray/flexrex\n    fsType: xfs\n    options:\n      volumeID: redis01\n      forceAttach: \ntrue\n\n      forceAttachDelay: \n15\n\n\n\n\n\nThe next YAML shows a \nPersistent Volume Claim\n (PVC) that carves out \n10Gi\n out of\nthe PV defined above.\n\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: vol01\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n\n\n\n\nThe claim can then be used by a pod in a YAML definition as shown below:\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-1\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: pod-1\n    volumeMounts:\n    - mountPath: /test-pd\n      name: vol01\n  volumes:\n  - name: vol01\n    persistentVolumeClaim:\n      claimName: vol01\n\n\n\n\nMesos\n\n\nIn Mesos the frameworks are responsible for receiving requests from\nconsumers and then proceeding to schedule and manage tasks. While some\nframeworks, like Marathon, are open to run any workload for sustained periods\nof time, others are use case specific, such as Cassandra. Frameworks may\nalso receive requests from other platforms in addition to schedulers instead of\nconsumers such as Cloud Foundry, Kubernetes, and Swarm.\n\n\nOnce a resource offer is accepted from Mesos, tasks are launched to support the\nassociated workloads. These tasks are eventually distributed to Mesos agents in\norder to spin up containers.\n\n\nREX-Ray enables on-demand storage allocation for agents receiving tasks via\ntwo deployment configurations:\n\n\n\n\n\n\nDocker Containerizer with Marathon\n\n\n\n\n\n\nMesos Containerizer with Marathon\n\n\n\n\n\n\nDocker Containerizer with Marathon\n\n\nWhen the framework leverages the Docker containerizer, Docker and REX-Ray\nshould both already be configured and working. The following example shows\nhow to use Marathon in order to bring an application online with external\nvolumes:\n\n\n{\n    \nid\n: \nnginx\n,\n    \ncontainer\n: {\n        \ndocker\n: {\n            \nimage\n: \nmillion12/nginx\n,\n            \nnetwork\n: \nBRIDGE\n,\n            \nportMappings\n: [{\n                \ncontainerPort\n: 80,\n                \nhostPort\n: 0,\n                \nprotocol\n: \ntcp\n\n            }],\n            \nparameters\n: [{\n                \nkey\n: \nvolume-driver\n,\n                \nvalue\n: \nrexray\n\n            }, {\n                \nkey\n: \nvolume\n,\n                \nvalue\n: \nnginx-data:/data/www\n\n            }]\n        }\n    },\n    \ncpus\n: 0.2,\n    \nmem\n: 32.0,\n    \ninstances\n: 1\n}\n\n\n\n\nMesos Containerizer with Marathon\n\n\nMesos 0.23+ includes modules that enable extensibility for different\nportions of the architecture. The \ndvdcli\n and\n\nmesos-module-dvdi\n projects are\nrequired to enable external volume support with the native containerizer.\n\n\nThe next example is similar to the one above, except in this instance the\nnative containerizer is preferred and volume requests are handled by the\n\nenv\n section.\n\n\n{\n  \nid\n: \nhello-play\n,\n  \ncmd\n: \nwhile [ true ] ; do touch /var/lib/rexray/volumes/test12345/hello ; sleep 5 ; done\n,\n  \nmem\n: 32,\n  \ncpus\n: 0.1,\n  \ninstances\n: 1,\n  \nenv\n: {\n    \nDVDI_VOLUME_NAME\n: \ntest12345\n,\n    \nDVDI_VOLUME_DRIVER\n: \nrexray\n,\n    \nDVDI_VOLUME_OPTS\n: \nsize=5,iops=150,volumetype=io1,newfstype=xfs,overwritefs=true\n\n  }\n}\n\n\n\n\nThis example also illustrates several important settings for the native method.\nWhile the VirtualBox driver is being used, any validated storage platform\nshould work. Additionally, there are two options recommended for this type of\nconfiguration:\n\n\n\n\n\n\n\n\nProperty\n\n\nRecommendation\n\n\n\n\n\n\n\n\n\n\nlibstorage.integration.volume.operations.mount.preempt\n\n\nSetting this flag to true ensures any host can preempt control of a volume from other hosts\n\n\n\n\n\n\nlibstorage.integration.volume.operations.unmount.ignoreUsedCount\n\n\nEnabling this flag declares that \nmesos-module-dvdi\n is the authoritative source for deciding when to unmount volumes\n\n\n\n\n\n\n\n\nPlease refer to the libStorage documentation for more information on\n\nVolume Configuration\n\noptions.\n\n\n\n\nnote\n\n\nThe \nlibstorage.integration.volume.operations.remove.disable\n property can\nprevent the scheduler from removing volumes. Setting this flag to \ntrue\n is\nrecommended when using Mesos with Docker 1.9.1 or earlier.\n\n\n\n\nlibstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        mount:\n          preempt: true\n        unmount:\n          ignoreusedcount: true\n        remove:\n          disable: true\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#schedulers", 
            "text": "Scheduling storage one resource at a time...", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#overview", 
            "text": "This page reviews the scheduling systems supported by REX-Ray.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/schedulers/#docker", 
            "text": "The majority of the documentation for the Docker integration driver has been relocated \nto the libStorage project.", 
            "title": "Docker"
        }, 
        {
            "location": "/user-guide/schedulers/#external-access", 
            "text": "By default, REX-Ray's embedded Docker Volume Plug-in endpoint handles\nrequests from the local Docker service via a UNIX socket. Doing so\nrestricts the endpoint to the localhost, increasing network security by removing\na possible attack vector. If an externally accessible Docker Volume Plug-in\nendpoint is required, it's still possible to create one by overriding the\naddress for the  default-docker  module in REX-Ray's configuration file:  rexray:\n  modules:\n    default-docker:\n      host: tcp://:7981  The above example illustrates how to override the  default-docker  module's\nendpoint address. The value  tcp://:7981  instructs the Docker Volume Plug-in\nto listen on port 7981 for all configured interfaces.  Using a TCP endpoint has a side-effect however -- the local Docker instance\nwill not know about the Volume Plug-in endpoint as there is no longer a UNIX\nsocket file in the directory the Docker service continually scans.  On the local system, and in fact on all systems where the Docker service needs\nto know about this externally accessible Volume Plug-in endpoint, a spec file\nmust be created at  /etc/docker/plug-ins/rexray.spec . Inside this file simply\ninclude a single line with the network address of the endpoint. For example:  tcp://192.168.56.20:7981  With a spec file located at  /etc/docker/plug-ins/rexray.spec  that contains\nthe above contents, Docker instances will query the Volume Plug-in endpoint at tcp://192.168.56.20:7981  when volume requests are received.", 
            "title": "External Access"
        }, 
        {
            "location": "/user-guide/schedulers/#volume-management", 
            "text": "The  volume  sub-command for Docker 1.12+ should look similar to the following:  $ docker volume\n\nUsage:  docker volume [OPTIONS] [COMMAND]\n\nManage Docker volumes\n\nCommands:\n  create                   Create a volume\n  inspect                  Return low-level information on a volume\n  ls                       List volumes\n  rm                       Remove a volume", 
            "title": "Volume Management"
        }, 
        {
            "location": "/user-guide/schedulers/#list-volumes", 
            "text": "The list command reviews a list of available volumes that have been discovered\nvia Docker Volume Plug-in endpoints such as REX-Ray. Each volume name is\nexpected to be unique. Thus volume names must also be unique across all\nendpoints, and in turn, across all storage platforms exposed by REX-Ray.  With the exception of the  local  driver, the list of returned volumes is\ngenerated by the backend storage platform to which the configured driver\ncommunicates:  $ docker volume ls\nDRIVER              VOLUME NAME\nlocal               local1\nscaleio             Volume-001\nvirtualbox          vbox1", 
            "title": "List Volumes"
        }, 
        {
            "location": "/user-guide/schedulers/#inspect-volume", 
            "text": "The inspect command can be used to retrieve details about a volume related to\nboth Docker and the underlying storage platform. The fields listed under Status  are all generated by REX-Ray, including  Size in GB ,  Volume Type ,\nand  Availability Zone .  The  Scope  parameter ensures that when the specified volume driver is\ninspected by multiple Docker hosts, the volumes tagged as  global  are all\ninterpreted as the same volume. This reduces unnecessary round-trips in\nsituations where an application such as Docker Swarm is connected to hosts\nconfigured with REX-Ray.  $ docker volume inspect vbox1\n[\n    {\n         Name :  vbox1 ,\n         Driver :  virtualbox ,\n         Mountpoint :  ,\n         Status : {\n             availabilityZone :  ,\n             fields : null,\n             iops : 0,\n             name :  vbox1 ,\n             server :  virtualbox ,\n             service :  virtualbox ,\n             size : 8,\n             type :  \n        },\n         Labels : {},\n         Scope :  global \n    }\n]", 
            "title": "Inspect Volume"
        }, 
        {
            "location": "/user-guide/schedulers/#create-volume", 
            "text": "Docker's  volume create  command enables the creation of new volumes on the\nunderlying storage platform. Newly created volumes are available immediately\nto be attached and mounted. The  volume create  command also supports the CLI\nflag  -o|--opt  in order to support providing custom data to the volume creation\nworkflow:  $ docker volume create --driver=virtualbox --name=vbox2 --opt=size=2\nvbox2  Additional, valid options for the  -o|--opt  parameter include:     option  description      size  Size in GB    IOPS  IOPS    volumeType  Type of Volume or Storage Pool    volumeName  Create from an existing volume name    volumeID  Create from an existing volume ID    snapshotName  Create from an existing snapshot name    snapshotID  Create from an existing snapshot ID", 
            "title": "Create Volume"
        }, 
        {
            "location": "/user-guide/schedulers/#remove-volume", 
            "text": "A volume may be removed once it is no longer in use by a container, running or\notherwise. The process of removing a container actually causes the volume to\nbe removed if that is the last container to leverage said volume:  $ docker volume rm vbox2", 
            "title": "Remove Volume"
        }, 
        {
            "location": "/user-guide/schedulers/#containers-with-volumes", 
            "text": "Please review the  Applications  section for information on\nconfiguring popular applications with persistent storage via Docker and REX-Ray.", 
            "title": "Containers with Volumes"
        }, 
        {
            "location": "/user-guide/schedulers/#kubernetes", 
            "text": "REX-Ray can be integrated with  Kubernetes  allowing\npods to consume data stored on volumes that are orchestrated by REX-Ray. Using\nKubernetes'  FlexVolume \nplug-in, REX-Ray can provide uniform access to storage operatations such as attach,\nmount, detach, and unmount for any configured storage provider.  REX-Ray provides an\nadapter script called  FlexRex  which integrates with the FlexVolume to interact\nwith the backing storage system.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/user-guide/schedulers/#pre-requisites", 
            "text": "Kubernetes  1.5 or higher  REX-Ray 0.7 or higher  jq binary", 
            "title": "Pre-Requisites"
        }, 
        {
            "location": "/user-guide/schedulers/#installation", 
            "text": "It is assumed that you have a Kubernetes cluster at your disposal. On each\nKubernetes node (running the kubelet), do the followings:   Install and configure the REX-Ray binary as prescribed in the Installation  section.    Next, validate the REX-Ray installation by running  rexray volume ls \nas shown in the the following:   # rexray volume ls\nID                Name   Status     Size\n925def7200000006  vol01  available  32\n925def7100000005  vol02  available  32  If there is no issue, you should see an output, similar to above, which shows\na list of previously created volumes. If instead you get an error, \nensure that REX-Ray is properly configured for the intended storage system.  Next, using the REX-Ray binary,  install the  FlexRex  adapter script on the node\nas shown below.    # rexray flexrex install  This should produce the following output showing that the FlexRex script is\ninstalled successfully:  Path                                                                        Installed  Modified\n/usr/libexec/kubernetes/kubelet-plug-ins/volume/exec/rexray~flexrex/flexrex  true       false  The path shown above is the default location where the FlexVolume plug-in will\nexpect to find its integration code.  If you are not using the default location\nwith FlexVolume, you can install the   FlexRex  in an arbitrary location using:  # rexray flexrex install --path /opt/plug-ins/rexray~flexrex/flexrex  Next, restart the kublet process on the node:  # systemctl restart kubelet  You can validate that the FlexRex script has been started successfully by searching\nthe kubelet log for an entry similar to the following:  I0208 10:56:57.412207    5348 plug-ins.go:350] Loaded volume plug-in  rexray/flexrex", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/schedulers/#pods-and-persistent-volumes", 
            "text": "You can now deploy pods and persistent volumes that use storage systems orchestrated\nby REX-Ray.  It is worth pointing out that the Kubernetes FlexVolme plug-in can only\nattach volumes that already exist in the storge system.  Any volume that is to be used\nby a Kubernetes resource must be listed in a  rexray volume ls  command.", 
            "title": "Pods and Persistent Volumes"
        }, 
        {
            "location": "/user-guide/schedulers/#pod-with-rex-ray-volume", 
            "text": "The following YAML file shows the definition of a pod that uses FlexRex to attach a volume\nto be used by the pod.  apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-0\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: pod-0\n    volumeMounts:\n    - mountPath: /test-pd\n      name: vol-0\n  volumes:\n  - name: vol-0\n    flexVolume:\n      driver: rexray/flexrex\n      fsType: ext4\n      options:\n        volumeID: test-vol-1\n        forceAttach:  true \n        forceAttachDelay:  15   Notice in the section under  flexVolume  the name of the driver attribute driver: rexray/flexrex . This is used by the FlexVolume plug-in to launch REX-Ray.\nAdditional options can be provided in the  options:  as follows:     Option  Desription      volumeID  Reference name of the volume in REX-Ray    forceAttach  When true ensures the volume is availble before attahing (optinal, defaults to false)    forceAttachDelay  Total amount of time (in sec) to attempt attachment with 5 sec interval between tries (optional)", 
            "title": "Pod with REX-Ray volume"
        }, 
        {
            "location": "/user-guide/schedulers/#rex-ray-persistentvolume", 
            "text": "The next example shows a YAML definition of Persistent Volume (PV) managed\nby REX-Ray.  apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: vol01\nspec:\n  capacity:\n    storage: 32Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  flexVolume:\n    driver: rexray/flexrex\n    fsType: xfs\n    options:\n      volumeID: redis01\n      forceAttach:  true \n      forceAttachDelay:  15   The next YAML shows a  Persistent Volume Claim  (PVC) that carves out  10Gi  out of\nthe PV defined above.  apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: vol01\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi  The claim can then be used by a pod in a YAML definition as shown below:  apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-1\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: pod-1\n    volumeMounts:\n    - mountPath: /test-pd\n      name: vol01\n  volumes:\n  - name: vol01\n    persistentVolumeClaim:\n      claimName: vol01", 
            "title": "REX-Ray PersistentVolume"
        }, 
        {
            "location": "/user-guide/schedulers/#mesos", 
            "text": "In Mesos the frameworks are responsible for receiving requests from\nconsumers and then proceeding to schedule and manage tasks. While some\nframeworks, like Marathon, are open to run any workload for sustained periods\nof time, others are use case specific, such as Cassandra. Frameworks may\nalso receive requests from other platforms in addition to schedulers instead of\nconsumers such as Cloud Foundry, Kubernetes, and Swarm.  Once a resource offer is accepted from Mesos, tasks are launched to support the\nassociated workloads. These tasks are eventually distributed to Mesos agents in\norder to spin up containers.  REX-Ray enables on-demand storage allocation for agents receiving tasks via\ntwo deployment configurations:    Docker Containerizer with Marathon    Mesos Containerizer with Marathon", 
            "title": "Mesos"
        }, 
        {
            "location": "/user-guide/schedulers/#docker-containerizer-with-marathon", 
            "text": "When the framework leverages the Docker containerizer, Docker and REX-Ray\nshould both already be configured and working. The following example shows\nhow to use Marathon in order to bring an application online with external\nvolumes:  {\n     id :  nginx ,\n     container : {\n         docker : {\n             image :  million12/nginx ,\n             network :  BRIDGE ,\n             portMappings : [{\n                 containerPort : 80,\n                 hostPort : 0,\n                 protocol :  tcp \n            }],\n             parameters : [{\n                 key :  volume-driver ,\n                 value :  rexray \n            }, {\n                 key :  volume ,\n                 value :  nginx-data:/data/www \n            }]\n        }\n    },\n     cpus : 0.2,\n     mem : 32.0,\n     instances : 1\n}", 
            "title": "Docker Containerizer with Marathon"
        }, 
        {
            "location": "/user-guide/schedulers/#mesos-containerizer-with-marathon", 
            "text": "Mesos 0.23+ includes modules that enable extensibility for different\nportions of the architecture. The  dvdcli  and mesos-module-dvdi  projects are\nrequired to enable external volume support with the native containerizer.  The next example is similar to the one above, except in this instance the\nnative containerizer is preferred and volume requests are handled by the env  section.  {\n   id :  hello-play ,\n   cmd :  while [ true ] ; do touch /var/lib/rexray/volumes/test12345/hello ; sleep 5 ; done ,\n   mem : 32,\n   cpus : 0.1,\n   instances : 1,\n   env : {\n     DVDI_VOLUME_NAME :  test12345 ,\n     DVDI_VOLUME_DRIVER :  rexray ,\n     DVDI_VOLUME_OPTS :  size=5,iops=150,volumetype=io1,newfstype=xfs,overwritefs=true \n  }\n}  This example also illustrates several important settings for the native method.\nWhile the VirtualBox driver is being used, any validated storage platform\nshould work. Additionally, there are two options recommended for this type of\nconfiguration:     Property  Recommendation      libstorage.integration.volume.operations.mount.preempt  Setting this flag to true ensures any host can preempt control of a volume from other hosts    libstorage.integration.volume.operations.unmount.ignoreUsedCount  Enabling this flag declares that  mesos-module-dvdi  is the authoritative source for deciding when to unmount volumes     Please refer to the libStorage documentation for more information on Volume Configuration \noptions.   note  The  libstorage.integration.volume.operations.remove.disable  property can\nprevent the scheduler from removing volumes. Setting this flag to  true  is\nrecommended when using Mesos with Docker 1.9.1 or earlier.   libstorage:\n  service: virtualbox\n  integration:\n    volume:\n      operations:\n        mount:\n          preempt: true\n        unmount:\n          ignoreusedcount: true\n        remove:\n          disable: true\nvirtualbox:\n  volumePath: $HOME/VirtualBox/Volumes", 
            "title": "Mesos Containerizer with Marathon"
        }, 
        {
            "location": "/user-guide/docker-plugins/", 
            "text": "Docker Volume Plug-ins\n\n\nPlug it in, plug it in...\n\n\n\n\nOverview\n\n\nThis page reviews the REX-Ray Docker volume plug-ins, available for\nDocker 1.13+.\n\n\nGetting Started\n\n\nThis section describes how to get started with REX-Ray Docker volume plug-ins!\n\n\nInstallation\n\n\nDocker plug-ins can be installed with following command:\n\n\n$ docker plugin install rexray/driver[:version]\n\n\n\n\nIn the above command line, if  \n[:version]\n is omitted, it's equivalent to\nthe following command:\n\n\n$ docker plugin install rexray/driver:latest\n\n\n\n\nThe \nlatest\n tag refers to the most recent, GA version of a plug-in. The\n\n[:version]\n component is known as a Docker \ntag\n. It follows the semantic\nversioning model. However, in addition to \nlatest\n, there is also the \nedge\n\ntag which refers to the most recent version built from the \nmaster\n development\nbranch.\n\n\n\n\nnote\n\n\nPlease note that most of REX-Ray's plug-ins must be configured and\ninstalled at the same time since Docker starts the plug-in when installed.\nOtherwise the plug-in will fail since it is not yet configured. Please\nsee the sections below for platform-specific configuration options.\n\n\n\n\nConfiguration\n\n\nDocker volume plug-ins are configured via environment variables, and all\nREX-Ray plug-ins share the following, common configuration options:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nREXRAY_FSTYPE\n\n\nThe type of file system to use\n\n\next4\n\n\n\n\n\n\nREXRAY_LOGLEVEL\n\n\nThe log level\n\n\nwarn\n\n\n\n\n\n\nREXRAY_PREEMPT\n\n\nEnable preemption\n\n\nfalse\n\n\n\n\n\n\n\n\nAmazon\n\n\nREX-Ray has plug-ins for multiple Amazon Web Services (AWS) storage services.\n\n\nElastic Block Service\n\n\nThe EBS plug-in can be installed with the following command:\n\n\n$ docker plugin install rexray/ebs \\\n  EBS_ACCESSKEY=abc \\\n  EBS_SECRETKEY=123\n\n\n\n\nPrivileges\n\n\nThe EBS plug-in requires the following privileges:\n\n\n\n\n\n\n\n\nType\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nnetwork\n\n\nhost\n\n\n\n\n\n\nmount\n\n\n/dev\n\n\n\n\n\n\nallow-all-devices\n\n\ntrue\n\n\n\n\n\n\ncapabilities\n\n\nCAP_SYS_ADMIN\n\n\n\n\n\n\n\n\nConfiguration\n\n\nThe following environment variables can be used to configure the EBS\nplug-in:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\nDefault\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nEBS_ACCESSKEY\n\n\nThe AWS access key\n\n\n\n\n\u2713\n\n\n\n\n\n\nEBS_SECRETKEY\n\n\nThe AWS secret key\n\n\n\n\n\u2713\n\n\n\n\n\n\nEBS_REGION\n\n\nThe AWS region\n\n\nus-east-1\n\n\n\n\n\n\n\n\n\n\nElastic File System\n\n\nThe EFS plug-in can be installed with the following command:\n\n\n$ docker plugin install rexray/efs \\\n  EFS_ACCESSKEY=abc \\\n  EFS_SECRETKEY=123 \\\n  EFS_SECURITYGROUPS=\nsg-123 sg-456\n \\\n  EFS_TAG=rexray\n\n\n\n\nRequirements\n\n\nThe EFS plug-in requires that nfs utilities be installed on the\nsame host on which Docker is running. You should be able to mount an\nnfs export to the host.\n\n\nPrivileges\n\n\nThe EFS plug-in requires the following privileges:\n\n\n\n\n\n\n\n\nType\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nnetwork\n\n\nhost\n\n\n\n\n\n\nmount\n\n\n/dev\n\n\n\n\n\n\nallow-all-devices\n\n\ntrue\n\n\n\n\n\n\ncapabilities\n\n\nCAP_SYS_ADMIN\n\n\n\n\n\n\n\n\nConfiguration\n\n\nThe following environment variables can be used to configure the EFS\nplug-in:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\nDefault\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nEFS_ACCESSKEY\n\n\nThe AWS access key\n\n\n\n\n\u2713\n\n\n\n\n\n\nEFS_SECRETKEY\n\n\nThe AWS secret key\n\n\n\n\n\u2713\n\n\n\n\n\n\nEFS_REGION\n\n\nThe AWS region\n\n\n\n\n\n\n\n\n\n\nEFS_SECURITYGROUPS\n\n\nThe AWS security groups to bind to\n\n\ndefault\n\n\n\n\n\n\n\n\nEFS_TAG\n\n\nOnly consume volumes with tag (tag\\volume_name)\n\n\n\n\n\n\n\n\n\n\nEFS_DISABLESESSIONCACHE\n\n\nnew AWS connection is established with every API call\n\n\nfalse\n\n\n\n\n\n\n\n\n\n\nSimple Storage Service\n\n\nThe S3FS plug-in can be installed with the following command:\n\n\n$ docker plugin install rexray/s3fs \\\n  S3FS_ACCESSKEY=abc \\\n  S3FS_SECRETKEY=123\n\n\n\n\nPrivileges\n\n\nThe S3FS plug-in requires the following privileges:\n\n\n\n\n\n\n\n\nType\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nnetwork\n\n\nhost\n\n\n\n\n\n\nmount\n\n\n/dev\n\n\n\n\n\n\nallow-all-devices\n\n\ntrue\n\n\n\n\n\n\ncapabilities\n\n\nCAP_SYS_ADMIN\n\n\n\n\n\n\n\n\nConfiguration\n\n\nThe following environment variables can be used to configure the S3FS\nplug-in:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\nDefault\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nS3FS_ACCESSKEY\n\n\nThe AWS access key\n\n\n\n\n\u2713\n\n\n\n\n\n\nS3FS_SECRETKEY\n\n\nThe AWS secret key\n\n\n\n\n\u2713\n\n\n\n\n\n\nS3S_REGION\n\n\nThe AWS region\n\n\n\n\n\n\n\n\n\n\n\n\nDell EMC\n\n\nREX-Ray includes plug-ins for several Dell EMC storage platforms.\n\n\nIsilon\n\n\nThe Isilon plug-in can be installed with the following command:\n\n\n$ docker plugin install rexray/isilon \\\n  ISILON_ENDPOINT=https://isilon:8080 \\\n  ISILON_USERNAME=user \\\n  ISILON_PASSWORD=pass \\\n  ISILON_VOLUMEPATH=/ifs/rexray \\\n  ISILON_NFSHOST=isilon_ip \\\n  ISILON_DATASUBNET=192.168.1.0/24\n\n\n\n\nRequirements\n\n\nThe Isilon plug-in requires that nfs utilities be installed on the\nsame host on which Docker is running. You should be able to mount an\nnfs export to the host.\n\n\nPrivileges\n\n\nThe Isilon plug-in requires the following privileges:\n\n\n\n\n\n\n\n\nType\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nnetwork\n\n\nhost\n\n\n\n\n\n\nmount\n\n\n/dev\n\n\n\n\n\n\nallow-all-devices\n\n\ntrue\n\n\n\n\n\n\ncapabilities\n\n\nCAP_SYS_ADMIN\n\n\n\n\n\n\n\n\nConfiguration\n\n\nThe following environment variables can be used to configure the Isilon\nplug-in:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\nDefault\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nISILON_ENDPOINT\n\n\nThe Isilon web interface endpoint\n\n\n\n\n\u2713\n\n\n\n\n\n\nISILON_INSECURE\n\n\nFlag for insecure gateway connection\n\n\nfalse\n\n\n\n\n\n\n\n\nISILON_USERNAME\n\n\nIsilon user for connection\n\n\n\n\n\u2713\n\n\n\n\n\n\nISILON_PASSWORD\n\n\nIsilon password\n\n\n\n\n\u2713\n\n\n\n\n\n\nISILON_VOLUMEPATH\n\n\nThe path for volumes (eg: /ifs/rexray)\n\n\n\n\n\u2713\n\n\n\n\n\n\nISILON_NFSHOST\n\n\nThe host or ip of your isilon nfs server\n\n\n\n\n\u2713\n\n\n\n\n\n\nISILON_DATASUBNET\n\n\nThe subnet for isilon nfs data traffic\n\n\n\n\n\u2713\n\n\n\n\n\n\nISILON_QUOTAS\n\n\nWanting to use quotas with isilon?\n\n\nfalse\n\n\n\n\n\n\n\n\n\n\nScaleIO\n\n\nThe ScaleIO plug-in can be installed with the following command:\n\n\n$ docker plugin install rexray/scaleio \\\n  SCALEIO_ENDPOINT=https://gateway/api \\\n  SCALEIO_USERNAME=user \\\n  SCALEIO_PASSWORD=pass \\\n  SCALEIO_SYSTEMNAME=scaleio \\\n  SCALEIO_PROTECTIONDOMAINNAME=default \\\n  SCALEIO_STORAGEPOOLNAME=default\n\n\n\n\nRequirements\n\n\nThe ScaleIO plug-in requires that the SDC toolkit must be installed on the\nsame host on which Docker is running.\n\n\nPrivileges\n\n\nThe ScaleIO plug-in requires the following privileges:\n\n\n\n\n\n\n\n\nType\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nnetwork\n\n\nhost\n\n\n\n\n\n\nmount\n\n\n/dev\n\n\n\n\n\n\n\n\n/bin/emc\n\n\n\n\n\n\n\n\n/opt/emc/scaleio/sdc\n\n\n\n\n\n\nallow-all-devices\n\n\ntrue\n\n\n\n\n\n\ncapabilities\n\n\nCAP_SYS_ADMIN\n\n\n\n\n\n\n\n\nConfiguration\n\n\nThe following environment variables can be used to configure the ScaleIO\nplug-in:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\nDefault\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nREXRAY_FSTYPE\n\n\nThe type of file system to use\n\n\nxfs\n\n\n\n\n\n\n\n\nSCALEIO_ENDPOINT\n\n\nThe ScaleIO gateway endpoint\n\n\n\n\n\u2713\n\n\n\n\n\n\nSCALEIO_INSECURE\n\n\nFlag for insecure gateway connection\n\n\ntrue\n\n\n\n\n\n\n\n\nSCALEIO_USECERTS\n\n\nFlag indicating to require certificate validation\n\n\nfalse\n\n\n\n\n\n\n\n\nSCALEIO_USERNAME\n\n\nScaleIO user for connection\n\n\n\n\n\u2713\n\n\n\n\n\n\nSCALEIO_PASSWORD\n\n\nScaleIO password\n\n\n\n\n\u2713\n\n\n\n\n\n\nSCALEIO_SYSTEMID\n\n\nThe ID of the ScaleIO system to use\n\n\n\n\nIf \nSCALEIO_SYSTEMID\n is omitted\n\n\n\n\n\n\nSCALEIO_SYSTEMNAME\n\n\nThe name of the ScaleIO system to use\n\n\n\n\nIf \nSCALEIO_SYSTEMNAME\n is omitted\n\n\n\n\n\n\nSCALEIO_PROTECTIONDOMAINID\n\n\nThe ID of the protection domain to use\n\n\n\n\nIf \nSCALEIO_PROTECTIONDOMAINNAME\n is omitted\n\n\n\n\n\n\nSCALEIO_PROTECTIONDOMAINNAME\n\n\nThe name of the protection domain to use\n\n\n\n\nIf \nSCALEIO_PROTECTIONDOMAINID\n is omitted\n\n\n\n\n\n\nSCALEIO_STORAGEPOOLID\n\n\nThe ID of the storage pool to use\n\n\n\n\nIf \nSCALEIO_STORAGEPOOLNAME\n is omitted\n\n\n\n\n\n\nSCALEIO_STORAGEPOOLNAME\n\n\nThe name of the storage pool to use\n\n\n\n\nIf \nSCALEIO_STORAGEPOOLID\n is omitted\n\n\n\n\n\n\nSCALEIO_THINORTHICK\n\n\nThe provision mode \n(Thin|Thick)Provisioned\n\n\n\n\n\n\n\n\n\n\nSCALEIO_VERSION\n\n\nThe version of ScaleIO system\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle\n\n\nREX-Ray ships with plug-ins for Google Compute Engine (GCE) as well.\n\n\nGCE Persistent Disk\n\n\nThe GCEPD plug-in can be installed with the following command:\n\n\n$ docker plugin install rexray/gcepd \\\n  GCEPD_TAG=rexray\n\n\n\n\nRequirements\n\n\nThe GCEPD plug-in requires that GCE compute instance has Read/Write Cloud API\naccess to the Compute Engine and Storage services.\n\n\nPrivileges\n\n\nThe GCEPD plug-in requires the following privileges:\n\n\n\n\n\n\n\n\nType\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nnetwork\n\n\nhost\n\n\n\n\n\n\nmount\n\n\n/dev\n\n\n\n\n\n\nallow-all-devices\n\n\ntrue\n\n\n\n\n\n\ncapabilities\n\n\nCAP_SYS_ADMIN\n\n\n\n\n\n\n\n\nConfiguration\n\n\nThe following environment variables can be used to configure the GCEPD\nplug-in:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\nDefault\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nGCEPD_DEFAULTDISKTYPE\n\n\nThe default disk type to consume\n\n\npd-ssd\n\n\n\n\n\n\n\n\nGCEPD_TAG\n\n\nOnly use volumes that are tagged with a label\n\n\n\n\n\n\n\n\n\n\nGCEPD_ZONE\n\n\nGCE Availability Zone\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\nThis section reviews examples of how to use the REX-Ray Docker Volume plug-ins.\nFor the purposes of the examples the EBS plug-in will be demonstrated, but\neach example would work for any of the plug-ins above.\n\n\nCreate a volume\n\n\nThe following example illustrates creating a volume:\n\n\n$ docker volume create --driver rexray/ebs --name test-vol-1\n\n\n\n\nVerify the volume was successfully created by listing the volumes:\n\n\n$ docker volume ls\nDRIVER          VOLUME NAME\nrexray/ebs      test-vol-1\n\n\n\n\nInspect a volume\n\n\nThe following example illustrates inspecting a volume:\n\n\n$ docker volume inspect test-vol-1\n\n\n\n\n[\n    {\n        \nDriver\n: \nrexray/ebs\n,\n        \nLabels\n: {},\n        \nMountpoint\n: \n/var/lib/docker/plug-ins/9f30ec546a4b1bb19574e491ef3e936c2583eda6be374682eb42d21bbeec0dd8/rootfs\n,\n        \nName\n: \ntest-vol-1\n,\n        \nOptions\n: {},\n        \nScope\n: \nglobal\n,\n        \nStatus\n: {\n            \navailabilityZone\n: \ndefault\n,\n            \nfields\n: null,\n            \niops\n: 0,\n            \nname\n: \ntest-vol-1\n,\n            \nserver\n: \nebs\n,\n            \nservice\n: \nebs\n,\n            \nsize\n: 16,\n            \ntype\n: \ndefault\n\n        }\n    }\n]\n\n\n\n\nUse a volume\n\n\nThe following example illustrates using a volume:\n\n\n$ docker run -v test-vol-1:/data busybox mount | grep \n/data\n\n/dev/xvdf on /data type ext4 (rw,seclabel,relatime,nouuid,attr2,inode64,noquota)\n\n\n\n\nRemove a volume\n\n\nThe following example illustrates removing a volume created:\n\n\n$ docker volume rm test-vol-1\n\n\n\n\nValidate the volume was deleted successfully by listing the volumes:\n\n\n$ docker volume ls\nDRIVER              VOLUME NAME", 
            "title": "Docker Plug-ins"
        }, 
        {
            "location": "/user-guide/docker-plugins/#docker-volume-plug-ins", 
            "text": "Plug it in, plug it in...", 
            "title": "Docker Volume Plug-ins"
        }, 
        {
            "location": "/user-guide/docker-plugins/#overview", 
            "text": "This page reviews the REX-Ray Docker volume plug-ins, available for\nDocker 1.13+.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/docker-plugins/#getting-started", 
            "text": "This section describes how to get started with REX-Ray Docker volume plug-ins!", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/docker-plugins/#installation", 
            "text": "Docker plug-ins can be installed with following command:  $ docker plugin install rexray/driver[:version]  In the above command line, if   [:version]  is omitted, it's equivalent to\nthe following command:  $ docker plugin install rexray/driver:latest  The  latest  tag refers to the most recent, GA version of a plug-in. The [:version]  component is known as a Docker  tag . It follows the semantic\nversioning model. However, in addition to  latest , there is also the  edge \ntag which refers to the most recent version built from the  master  development\nbranch.   note  Please note that most of REX-Ray's plug-ins must be configured and\ninstalled at the same time since Docker starts the plug-in when installed.\nOtherwise the plug-in will fail since it is not yet configured. Please\nsee the sections below for platform-specific configuration options.", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/docker-plugins/#configuration", 
            "text": "Docker volume plug-ins are configured via environment variables, and all\nREX-Ray plug-ins share the following, common configuration options:     Environment Variable  Description  Default Value      REXRAY_FSTYPE  The type of file system to use  ext4    REXRAY_LOGLEVEL  The log level  warn    REXRAY_PREEMPT  Enable preemption  false", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/docker-plugins/#amazon", 
            "text": "REX-Ray has plug-ins for multiple Amazon Web Services (AWS) storage services.", 
            "title": "Amazon"
        }, 
        {
            "location": "/user-guide/docker-plugins/#elastic-block-service", 
            "text": "The EBS plug-in can be installed with the following command:  $ docker plugin install rexray/ebs \\\n  EBS_ACCESSKEY=abc \\\n  EBS_SECRETKEY=123", 
            "title": "Elastic Block Service"
        }, 
        {
            "location": "/user-guide/docker-plugins/#privileges", 
            "text": "The EBS plug-in requires the following privileges:     Type  Value      network  host    mount  /dev    allow-all-devices  true    capabilities  CAP_SYS_ADMIN", 
            "title": "Privileges"
        }, 
        {
            "location": "/user-guide/docker-plugins/#configuration_1", 
            "text": "The following environment variables can be used to configure the EBS\nplug-in:     Environment Variable  Description  Default  Required      EBS_ACCESSKEY  The AWS access key   \u2713    EBS_SECRETKEY  The AWS secret key   \u2713    EBS_REGION  The AWS region  us-east-1", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/docker-plugins/#elastic-file-system", 
            "text": "The EFS plug-in can be installed with the following command:  $ docker plugin install rexray/efs \\\n  EFS_ACCESSKEY=abc \\\n  EFS_SECRETKEY=123 \\\n  EFS_SECURITYGROUPS= sg-123 sg-456  \\\n  EFS_TAG=rexray", 
            "title": "Elastic File System"
        }, 
        {
            "location": "/user-guide/docker-plugins/#requirements", 
            "text": "The EFS plug-in requires that nfs utilities be installed on the\nsame host on which Docker is running. You should be able to mount an\nnfs export to the host.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/docker-plugins/#privileges_1", 
            "text": "The EFS plug-in requires the following privileges:     Type  Value      network  host    mount  /dev    allow-all-devices  true    capabilities  CAP_SYS_ADMIN", 
            "title": "Privileges"
        }, 
        {
            "location": "/user-guide/docker-plugins/#configuration_2", 
            "text": "The following environment variables can be used to configure the EFS\nplug-in:     Environment Variable  Description  Default  Required      EFS_ACCESSKEY  The AWS access key   \u2713    EFS_SECRETKEY  The AWS secret key   \u2713    EFS_REGION  The AWS region      EFS_SECURITYGROUPS  The AWS security groups to bind to  default     EFS_TAG  Only consume volumes with tag (tag\\volume_name)      EFS_DISABLESESSIONCACHE  new AWS connection is established with every API call  false", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/docker-plugins/#simple-storage-service", 
            "text": "The S3FS plug-in can be installed with the following command:  $ docker plugin install rexray/s3fs \\\n  S3FS_ACCESSKEY=abc \\\n  S3FS_SECRETKEY=123", 
            "title": "Simple Storage Service"
        }, 
        {
            "location": "/user-guide/docker-plugins/#privileges_2", 
            "text": "The S3FS plug-in requires the following privileges:     Type  Value      network  host    mount  /dev    allow-all-devices  true    capabilities  CAP_SYS_ADMIN", 
            "title": "Privileges"
        }, 
        {
            "location": "/user-guide/docker-plugins/#configuration_3", 
            "text": "The following environment variables can be used to configure the S3FS\nplug-in:     Environment Variable  Description  Default  Required      S3FS_ACCESSKEY  The AWS access key   \u2713    S3FS_SECRETKEY  The AWS secret key   \u2713    S3S_REGION  The AWS region", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/docker-plugins/#dell-emc", 
            "text": "REX-Ray includes plug-ins for several Dell EMC storage platforms.", 
            "title": "Dell EMC"
        }, 
        {
            "location": "/user-guide/docker-plugins/#isilon", 
            "text": "The Isilon plug-in can be installed with the following command:  $ docker plugin install rexray/isilon \\\n  ISILON_ENDPOINT=https://isilon:8080 \\\n  ISILON_USERNAME=user \\\n  ISILON_PASSWORD=pass \\\n  ISILON_VOLUMEPATH=/ifs/rexray \\\n  ISILON_NFSHOST=isilon_ip \\\n  ISILON_DATASUBNET=192.168.1.0/24", 
            "title": "Isilon"
        }, 
        {
            "location": "/user-guide/docker-plugins/#requirements_1", 
            "text": "The Isilon plug-in requires that nfs utilities be installed on the\nsame host on which Docker is running. You should be able to mount an\nnfs export to the host.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/docker-plugins/#privileges_3", 
            "text": "The Isilon plug-in requires the following privileges:     Type  Value      network  host    mount  /dev    allow-all-devices  true    capabilities  CAP_SYS_ADMIN", 
            "title": "Privileges"
        }, 
        {
            "location": "/user-guide/docker-plugins/#configuration_4", 
            "text": "The following environment variables can be used to configure the Isilon\nplug-in:     Environment Variable  Description  Default  Required      ISILON_ENDPOINT  The Isilon web interface endpoint   \u2713    ISILON_INSECURE  Flag for insecure gateway connection  false     ISILON_USERNAME  Isilon user for connection   \u2713    ISILON_PASSWORD  Isilon password   \u2713    ISILON_VOLUMEPATH  The path for volumes (eg: /ifs/rexray)   \u2713    ISILON_NFSHOST  The host or ip of your isilon nfs server   \u2713    ISILON_DATASUBNET  The subnet for isilon nfs data traffic   \u2713    ISILON_QUOTAS  Wanting to use quotas with isilon?  false", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/docker-plugins/#scaleio", 
            "text": "The ScaleIO plug-in can be installed with the following command:  $ docker plugin install rexray/scaleio \\\n  SCALEIO_ENDPOINT=https://gateway/api \\\n  SCALEIO_USERNAME=user \\\n  SCALEIO_PASSWORD=pass \\\n  SCALEIO_SYSTEMNAME=scaleio \\\n  SCALEIO_PROTECTIONDOMAINNAME=default \\\n  SCALEIO_STORAGEPOOLNAME=default", 
            "title": "ScaleIO"
        }, 
        {
            "location": "/user-guide/docker-plugins/#requirements_2", 
            "text": "The ScaleIO plug-in requires that the SDC toolkit must be installed on the\nsame host on which Docker is running.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/docker-plugins/#privileges_4", 
            "text": "The ScaleIO plug-in requires the following privileges:     Type  Value      network  host    mount  /dev     /bin/emc     /opt/emc/scaleio/sdc    allow-all-devices  true    capabilities  CAP_SYS_ADMIN", 
            "title": "Privileges"
        }, 
        {
            "location": "/user-guide/docker-plugins/#configuration_5", 
            "text": "The following environment variables can be used to configure the ScaleIO\nplug-in:     Environment Variable  Description  Default  Required      REXRAY_FSTYPE  The type of file system to use  xfs     SCALEIO_ENDPOINT  The ScaleIO gateway endpoint   \u2713    SCALEIO_INSECURE  Flag for insecure gateway connection  true     SCALEIO_USECERTS  Flag indicating to require certificate validation  false     SCALEIO_USERNAME  ScaleIO user for connection   \u2713    SCALEIO_PASSWORD  ScaleIO password   \u2713    SCALEIO_SYSTEMID  The ID of the ScaleIO system to use   If  SCALEIO_SYSTEMID  is omitted    SCALEIO_SYSTEMNAME  The name of the ScaleIO system to use   If  SCALEIO_SYSTEMNAME  is omitted    SCALEIO_PROTECTIONDOMAINID  The ID of the protection domain to use   If  SCALEIO_PROTECTIONDOMAINNAME  is omitted    SCALEIO_PROTECTIONDOMAINNAME  The name of the protection domain to use   If  SCALEIO_PROTECTIONDOMAINID  is omitted    SCALEIO_STORAGEPOOLID  The ID of the storage pool to use   If  SCALEIO_STORAGEPOOLNAME  is omitted    SCALEIO_STORAGEPOOLNAME  The name of the storage pool to use   If  SCALEIO_STORAGEPOOLID  is omitted    SCALEIO_THINORTHICK  The provision mode  (Thin|Thick)Provisioned      SCALEIO_VERSION  The version of ScaleIO system", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/docker-plugins/#google", 
            "text": "REX-Ray ships with plug-ins for Google Compute Engine (GCE) as well.", 
            "title": "Google"
        }, 
        {
            "location": "/user-guide/docker-plugins/#gce-persistent-disk", 
            "text": "The GCEPD plug-in can be installed with the following command:  $ docker plugin install rexray/gcepd \\\n  GCEPD_TAG=rexray", 
            "title": "GCE Persistent Disk"
        }, 
        {
            "location": "/user-guide/docker-plugins/#requirements_3", 
            "text": "The GCEPD plug-in requires that GCE compute instance has Read/Write Cloud API\naccess to the Compute Engine and Storage services.", 
            "title": "Requirements"
        }, 
        {
            "location": "/user-guide/docker-plugins/#privileges_5", 
            "text": "The GCEPD plug-in requires the following privileges:     Type  Value      network  host    mount  /dev    allow-all-devices  true    capabilities  CAP_SYS_ADMIN", 
            "title": "Privileges"
        }, 
        {
            "location": "/user-guide/docker-plugins/#configuration_6", 
            "text": "The following environment variables can be used to configure the GCEPD\nplug-in:     Environment Variable  Description  Default  Required      GCEPD_DEFAULTDISKTYPE  The default disk type to consume  pd-ssd     GCEPD_TAG  Only use volumes that are tagged with a label      GCEPD_ZONE  GCE Availability Zone", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/docker-plugins/#examples", 
            "text": "This section reviews examples of how to use the REX-Ray Docker Volume plug-ins.\nFor the purposes of the examples the EBS plug-in will be demonstrated, but\neach example would work for any of the plug-ins above.", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/docker-plugins/#create-a-volume", 
            "text": "The following example illustrates creating a volume:  $ docker volume create --driver rexray/ebs --name test-vol-1  Verify the volume was successfully created by listing the volumes:  $ docker volume ls\nDRIVER          VOLUME NAME\nrexray/ebs      test-vol-1", 
            "title": "Create a volume"
        }, 
        {
            "location": "/user-guide/docker-plugins/#inspect-a-volume", 
            "text": "The following example illustrates inspecting a volume:  $ docker volume inspect test-vol-1  [\n    {\n         Driver :  rexray/ebs ,\n         Labels : {},\n         Mountpoint :  /var/lib/docker/plug-ins/9f30ec546a4b1bb19574e491ef3e936c2583eda6be374682eb42d21bbeec0dd8/rootfs ,\n         Name :  test-vol-1 ,\n         Options : {},\n         Scope :  global ,\n         Status : {\n             availabilityZone :  default ,\n             fields : null,\n             iops : 0,\n             name :  test-vol-1 ,\n             server :  ebs ,\n             service :  ebs ,\n             size : 16,\n             type :  default \n        }\n    }\n]", 
            "title": "Inspect a volume"
        }, 
        {
            "location": "/user-guide/docker-plugins/#use-a-volume", 
            "text": "The following example illustrates using a volume:  $ docker run -v test-vol-1:/data busybox mount | grep  /data \n/dev/xvdf on /data type ext4 (rw,seclabel,relatime,nouuid,attr2,inode64,noquota)", 
            "title": "Use a volume"
        }, 
        {
            "location": "/user-guide/docker-plugins/#remove-a-volume", 
            "text": "The following example illustrates removing a volume created:  $ docker volume rm test-vol-1  Validate the volume was deleted successfully by listing the volumes:  $ docker volume ls\nDRIVER              VOLUME NAME", 
            "title": "Remove a volume"
        }, 
        {
            "location": "/user-guide/applications/", 
            "text": "Applications\n\n\nPersistence for applications in containers.\n\n\n\n\nGetting Started\n\n\nThis tutorial will serve as a generic guide for taking Docker images found on\n\nDocker Hub\n and utilizing persistent external storage\nvia REX-Ray. This should provide guidance for certain applications, but also\ngenerically so you can add persistence properly to other applications.\n\n\nInstructions\n\n\nThe following are a set of instructions for investigating an existing container\nimage to determine how to properly apply persistence.\n\n\nThe first step is to determine which application you are looking to deploy, then\nproceed to its \nDocker Hub\n page. In this example, we\nwill be using \nPostgreSQL on Docker Hub\n.\n\n\nMost application vendors will post their Dockerfile on the main page for that\ngiven image. Many of them will also make them available by version minimally via\na download or via Github. Continuing with our PostgreSQL example, we will use\nthe \nDockerfile for version 9.3\n\nsince it happens to be the default version provided with Ubuntu 14.04.\n\n\nProperly written Dockerfiles will include the proper information that separates\npersistent information from the container image and deployed container. This is\nvisible when the author of the \nDockerfile\n includes a \nVOLUME\n statement to\ndefine where stateful information should be held.\n\n\nOpen the \nDockerfile\n and do a search for \nVOLUME\n and take note of the\nvolumes that will be created for this image. Then we can use REX-Ray or\n\nDocker\n to create the external persistent volume for this image. In the\nPostgreSQL 9.3 example, there is a single volume in the Dockerfile:\n\n\nVOLUME /var/lib/postgresql/data\n\n\n\n\nThe single path or paths listed refer to the volumes that should be attached\nwhen running the container. Following this you can create a volume and attach\nit to a container with the \n-v\n flag.\n\n\n\n\nPostgreSQL\n  \n\n\n\n\n$ docker volume create --driver=rexray --name=postgresql --opt=size=\nsizeInGB\n\n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres\n\n\n\n\nPopular Applications\n\n\nExternal persistent storage can be applied to any number of applications\nincluding but not limited the following examples.\n\n\n\n\nCassandra\n\n\n\n\nPostgreSQL\n\n\n$ docker volume create --driver=rexray --name=postgresql --opt=size=\nsizeInGB\n\n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres\n\n\n\n\n\n\n\nMariaDB\n\n\n\n\n\n\nMongoDB\n\n\n$ docker volume create --driver=rexray --name=mongodb --opt=size=\nsizeInGB\n\n$ docker run -d --volume-driver=rexray -v mongodb:/data/db mongo\n\n\n\n\n\n\n\nMySQL\n\n\n\n\nRedis\n$ docker volume create --driver=rexray --name=redis --opt=size=\nsizeInGB\n\n$ docker run -d --volume-driver=rexray -v redis:/data redis", 
            "title": "Applications"
        }, 
        {
            "location": "/user-guide/applications/#applications", 
            "text": "Persistence for applications in containers.", 
            "title": "Applications"
        }, 
        {
            "location": "/user-guide/applications/#getting-started", 
            "text": "This tutorial will serve as a generic guide for taking Docker images found on Docker Hub  and utilizing persistent external storage\nvia REX-Ray. This should provide guidance for certain applications, but also\ngenerically so you can add persistence properly to other applications.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/applications/#instructions", 
            "text": "The following are a set of instructions for investigating an existing container\nimage to determine how to properly apply persistence.  The first step is to determine which application you are looking to deploy, then\nproceed to its  Docker Hub  page. In this example, we\nwill be using  PostgreSQL on Docker Hub .  Most application vendors will post their Dockerfile on the main page for that\ngiven image. Many of them will also make them available by version minimally via\na download or via Github. Continuing with our PostgreSQL example, we will use\nthe  Dockerfile for version 9.3 \nsince it happens to be the default version provided with Ubuntu 14.04.  Properly written Dockerfiles will include the proper information that separates\npersistent information from the container image and deployed container. This is\nvisible when the author of the  Dockerfile  includes a  VOLUME  statement to\ndefine where stateful information should be held.  Open the  Dockerfile  and do a search for  VOLUME  and take note of the\nvolumes that will be created for this image. Then we can use REX-Ray or Docker  to create the external persistent volume for this image. In the\nPostgreSQL 9.3 example, there is a single volume in the Dockerfile:  VOLUME /var/lib/postgresql/data  The single path or paths listed refer to the volumes that should be attached\nwhen running the container. Following this you can create a volume and attach\nit to a container with the  -v  flag.   PostgreSQL      $ docker volume create --driver=rexray --name=postgresql --opt=size= sizeInGB \n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres", 
            "title": "Instructions"
        }, 
        {
            "location": "/user-guide/applications/#popular-applications", 
            "text": "External persistent storage can be applied to any number of applications\nincluding but not limited the following examples.   Cassandra   PostgreSQL  $ docker volume create --driver=rexray --name=postgresql --opt=size= sizeInGB \n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres    MariaDB    MongoDB  $ docker volume create --driver=rexray --name=mongodb --opt=size= sizeInGB \n$ docker run -d --volume-driver=rexray -v mongodb:/data/db mongo    MySQL   Redis $ docker volume create --driver=rexray --name=redis --opt=size= sizeInGB \n$ docker run -d --volume-driver=rexray -v redis:/data redis", 
            "title": "Popular Applications"
        }, 
        {
            "location": "/user-guide/demo/", 
            "text": "Demo\n\n\nEasy as 1, 2, 3...\n\n\n\n\nTwo-Node Client/Server\n\n\nThis demo consists of a two-node deployment with the first node configured as a\nREX-Ray/libStorage server and the second node as merely a client. Both nodes\nhave Docker installed and configured to leverage REX-Ray for persistent storage.\n\n\nThe below example does have a few requirements:\n\n\n\n\nVirtualBox 5.0+\n\n\nVagrant 1.8+\n\n\nRuby 2.0+\n\n\n\n\nStart REX-Ray Vagrant Environment\n\n\nBefore bringing the Vagrant environment online, please ensure it is accomplished\nin a clean directory\n\n\n$ cd $(mktemp -d)\n\n\n\n\nInside the newly created, temporary directory, download the REX-Ray\n\nVagrantfile\n:\n\n\n$ curl -fsSLO https://raw.githubusercontent.com/emccode/rexray/master/Vagrantfile\n\n\n\n\nNow it is time to bring the REX-Ray environment online:\n\n\n\n\nnote\n\n\nThe next step could potentially open up the system on which the command is\nexecuted to security vulnerabilities. The Vagrantfile brings the VirtualBox\nweb service online if it is not already running. However, in the name of\nsimplicity the Vagrantfile also disables the web server's authentication\nmodule. Please do not disable authentication for the VirtualBox web server\nif this example is being executed on an open network or without some type of\nfirewall in place.\n\n\n\n\n$ vagrant up\n\n\n\n\nThe above command should result in output similar to \nthis\nGist\n.\n\n\nOnce the command has been completed successfully there will be two VMs online\nnamed \nnode0\n and \nnode1\n. Both nodes are running Docker and REX-Ray with\n\nnode0\n configured to act as a libStorage server.\n\n\nNow that the environment is online it is time to showcase Docker leveraging REX-\nRay to create persistent storage as well as illustrating REX-Ray's distributed\ndeployment capabilities.\n\n\nNode 0\n\n\nFirst, SSH into \nnode0\n\n\n$ vagrant ssh node0\n\n\n\n\nFrom \nnode0\n use Docker with REX-Ray to create a new volume named\n\nhellopersistence\n:\n\n\nvagrant@node0:~$ docker volume create --driver rexray --opt size=1 \\\n                 --name hellopersistence\n\n\n\n\nAfter the volume is created, mount it to the host and container using the\n\n--volume-driver\n and \n-v\n flag in the \ndocker run\n command:\n\n\nvagrant@node0:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox\n\n\n\n\nCreate a new file named \nmyfile\n on the file system backed by the persistent\nvolume using \ndocker exec\n:\n\n\nvagrant@node0:~$ docker exec temp01 touch /mystore/myfile\n\n\n\n\nVerify the file was successfully created by listing the contents of the\npersistent volume:\n\n\nvagrant@node0:~$ docker exec temp01 ls /mystore\n\n\n\n\nRemove the container that was used to write the data to the persistent volume:\n\n\nvagrant@node0:~$ docker rm -f temp01\n\n\n\n\nFinally, exit the SSH session to \nnode0\n:\n\n\nvagrant@node0:~$ exit\n\n\n\n\nNode 1\n\n\nIt's time to connect to \nnode1\n and use the volume \nhellopersistence\n that was\ncreated in the previous section from \nnode0\n.\n\n\n\n\nnote\n\n\nWhile \nnode1\n runs both the Docker and REX-Ray services like \nnode0\n, the\nREX-Ray service on \nnode1\n in no way understands or is configured for the\nVirtualBox storage driver. All interactions with the VirtualBox web service\noccurs via \nnode0\n's libStorage server with which \nnode1\n communicates.\n\n\n\n\nUse the vagrant command to SSH into \nnode1\n:\n\n\n$ vagrant ssh node1\n\n\n\n\nNext, create a new container that mounts the existing volume,\n\nhellopersistence\n:\n\n\nvagrant@node1:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox\n\n\n\n\nThe next command validates the file \nmyfile\n created from \nnode0\n in the\nprevious section has persisted inside the volume across machines:\n\n\nvagrant@node1:~$ docker exec temp01 ls /mystore\n\n\n\n\nFinally, exit the SSH session to \nnode1\n:\n\n\nvagrant@node1:~$ exit\n\n\n\n\nCleaning Up\n\n\nBe sure to kill the VirtualBox web server with a quick \nkillall vboxwebsrv\n and\nto tear down the Vagrant environment with \nvagrant destroy\n. Omitting these\ncommands will leave the web service and REX-Ray Vagrant nodes online and consume\nadditional system resources.\n\n\nCongratulations\n\n\nREX-Ray has been used to provide persistence for stateless containers! Examples\nusing MongoDB, Postgres, and more with persistent storage can be found at\n\nApplication Examples\n or within the \n{code} Labs\nrepo\n.", 
            "title": "Demo"
        }, 
        {
            "location": "/user-guide/demo/#demo", 
            "text": "Easy as 1, 2, 3...", 
            "title": "Demo"
        }, 
        {
            "location": "/user-guide/demo/#two-node-clientserver", 
            "text": "This demo consists of a two-node deployment with the first node configured as a\nREX-Ray/libStorage server and the second node as merely a client. Both nodes\nhave Docker installed and configured to leverage REX-Ray for persistent storage.  The below example does have a few requirements:   VirtualBox 5.0+  Vagrant 1.8+  Ruby 2.0+", 
            "title": "Two-Node Client/Server"
        }, 
        {
            "location": "/user-guide/demo/#start-rex-ray-vagrant-environment", 
            "text": "Before bringing the Vagrant environment online, please ensure it is accomplished\nin a clean directory  $ cd $(mktemp -d)  Inside the newly created, temporary directory, download the REX-Ray Vagrantfile :  $ curl -fsSLO https://raw.githubusercontent.com/emccode/rexray/master/Vagrantfile  Now it is time to bring the REX-Ray environment online:   note  The next step could potentially open up the system on which the command is\nexecuted to security vulnerabilities. The Vagrantfile brings the VirtualBox\nweb service online if it is not already running. However, in the name of\nsimplicity the Vagrantfile also disables the web server's authentication\nmodule. Please do not disable authentication for the VirtualBox web server\nif this example is being executed on an open network or without some type of\nfirewall in place.   $ vagrant up  The above command should result in output similar to  this\nGist .  Once the command has been completed successfully there will be two VMs online\nnamed  node0  and  node1 . Both nodes are running Docker and REX-Ray with node0  configured to act as a libStorage server.  Now that the environment is online it is time to showcase Docker leveraging REX-\nRay to create persistent storage as well as illustrating REX-Ray's distributed\ndeployment capabilities.", 
            "title": "Start REX-Ray Vagrant Environment"
        }, 
        {
            "location": "/user-guide/demo/#node-0", 
            "text": "First, SSH into  node0  $ vagrant ssh node0  From  node0  use Docker with REX-Ray to create a new volume named hellopersistence :  vagrant@node0:~$ docker volume create --driver rexray --opt size=1 \\\n                 --name hellopersistence  After the volume is created, mount it to the host and container using the --volume-driver  and  -v  flag in the  docker run  command:  vagrant@node0:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox  Create a new file named  myfile  on the file system backed by the persistent\nvolume using  docker exec :  vagrant@node0:~$ docker exec temp01 touch /mystore/myfile  Verify the file was successfully created by listing the contents of the\npersistent volume:  vagrant@node0:~$ docker exec temp01 ls /mystore  Remove the container that was used to write the data to the persistent volume:  vagrant@node0:~$ docker rm -f temp01  Finally, exit the SSH session to  node0 :  vagrant@node0:~$ exit", 
            "title": "Node 0"
        }, 
        {
            "location": "/user-guide/demo/#node-1", 
            "text": "It's time to connect to  node1  and use the volume  hellopersistence  that was\ncreated in the previous section from  node0 .   note  While  node1  runs both the Docker and REX-Ray services like  node0 , the\nREX-Ray service on  node1  in no way understands or is configured for the\nVirtualBox storage driver. All interactions with the VirtualBox web service\noccurs via  node0 's libStorage server with which  node1  communicates.   Use the vagrant command to SSH into  node1 :  $ vagrant ssh node1  Next, create a new container that mounts the existing volume, hellopersistence :  vagrant@node1:~$ docker run -tid --volume-driver=rexray \\\n                 -v hellopersistence:/mystore \\\n                 --name temp01 busybox  The next command validates the file  myfile  created from  node0  in the\nprevious section has persisted inside the volume across machines:  vagrant@node1:~$ docker exec temp01 ls /mystore  Finally, exit the SSH session to  node1 :  vagrant@node1:~$ exit", 
            "title": "Node 1"
        }, 
        {
            "location": "/user-guide/demo/#cleaning-up", 
            "text": "Be sure to kill the VirtualBox web server with a quick  killall vboxwebsrv  and\nto tear down the Vagrant environment with  vagrant destroy . Omitting these\ncommands will leave the web service and REX-Ray Vagrant nodes online and consume\nadditional system resources.", 
            "title": "Cleaning Up"
        }, 
        {
            "location": "/user-guide/demo/#congratulations", 
            "text": "REX-Ray has been used to provide persistence for stateless containers! Examples\nusing MongoDB, Postgres, and more with persistent storage can be found at Application Examples  or within the  {code} Labs\nrepo .", 
            "title": "Congratulations"
        }, 
        {
            "location": "/user-guide/troubleshooting/", 
            "text": "Troubleshooting\n\n\nIt's not doing what I expected...\n\n\n\n\nSolving problems\n\n\nThis section details the usual places and methods to look and use when\ninvestigating a problem.\n\n\nIs REX-Ray running?\n\n\nConfirm REX-Ray is running with the following command:\n\n\n$ sudo rexray service status\n\nActive: active (running) since Sun 2017-02-12 02:19:16 UTC; 1 day 16h ago\n\n\n\n\nIs REX-Ray listening?\n\n\nConfirm REX-Ray is listening on a UNIX socket with the following command:\n\n\n$ sudo lsof -noPU -a -c rexray\nCOMMAND  PID USER   FD   TYPE             DEVICE OFFSET  NODE NAME\nrexray  3228 root    5u  unix 0xffff8800cc6eb800    0t0 38218 /var/run/libstorage/277666194.sock\nrexray  3228 root    7u  unix 0xffff880117792000    0t0 34741 socket\nrexray  3228 root    8u  unix 0xffff880117792400    0t0 38221 /var/run/libstorage/277666194.sock\nrexray  3228 root    9u  unix 0xffff8800cc6e8c00    0t0 38226 /run/docker/plugins/rexray.sock\n\n\n\n\nConfirm REX-Ray is listening on a TCP port with the following command:\n\n\n$ sudo lsof -noP -i -sTCP:LISTEN -a -c rexray\nCOMMAND  PID USER   FD   TYPE DEVICE OFFSET NODE NAME\nrexray  3400 root    4u  IPv4  38868    0t0  TCP 127.0.0.1:5002 (LISTEN)\n\n\n\n\nIs REX-Ray talking?\n\n\nAfter confirming that the service is listening as expected, you can use \ncurl\n\nto actually invoke the libStorage REST API. This example shows doing it using\nlocalhost but in a client server topology deployment you can also invoke it\nfrom your client nodes (using the server\u2019s external IP) to confirm that there\nare no routing or firewall issues. This example uses http, but if you have\ninstalled certificates, you should use https instead. This particular\ninvocation of the REST API lists the services (storage provider classes)\nthat are available to clients.\n\n\n$ curl http://127.0.0.1:5002/services\n\n\n\n\n{\n  \nebs\n: {\n    \nname\n: \nebs\n,\n    \ndriver\n: {\n      \nname\n: \nebs\n,\n      \ntype\n: \nblock\n,\n      \nnextDevice\n: {\n        \nignore\n: false,\n        \nprefix\n: \nxvd\n,\n        \npattern\n: \n[f-p]\n\n      }\n    }\n  }\n}\n\n\n\n\nCommon Errors\n\n\nThis section reviews common errors encountered when using REX-Ray.\n\n\n\n\nnote\n\n\nGeneral note: When running on a public cloud provider the error messages in\nthe log resulting from calls to a cloud provider API are repeated from the\ncloud provider\u2019s API. Sometimes these are not intuitive. For example a bad\ncredential might result in an error message related to a non-existent\nobject. Performing a google search in the context of your cloud provider,\nrather than REX-Ray can sometimes prove to be helpful.\n\n\n\n\nStarting REX-Ray\n\n\nThis error occurs when attempting to start REX-Ray service as a normal account,\nrather than root or with \nsudo\n:\n\n\n$ rexray service start\nFailed to start rexray.service: Interactive authentication required.\n\n\n\n\nOmitted service flag\n\n\nThis error occurs when there are multiple services configured and the service\nspecification is omitted from the command line:\n\n\n$ rexray volume ls\nFATA[0000] http error                                    status=404\n\n\n\n\nMissing REX-Ray config file\n\n\nThis error can occur when the REX-Ray configuration file is in the incorrect\nlocation or one does not exist at all:\n\n\n$ rexray volume ls\nERRO[0000] error starting libStorage server              error.configKey=libstorage.server.services error.obj=\nnil\n time=1486771348853`\n\n\n\n\nInvalid provider credentials\n\n\nThis error occurs when invalid credentials are provided for the storage\nprovider. The example below uses the EBS storage provider:\n\n\ntime=\n2017-02-10T22:34:44Z\n level=error msg=\nerror getting volume\n host=\ntcp://127.0.0.1:7979\n inner=\nAuthFailure: AWS was not able to validate the provided access credentials\\n\\tstatus code: 401, request id: ea6587a90-f29d-4f14-99da-6a7ec7cb05c1\n instanceID=\nebs=i-0213cc11c4ade43fb,availabilityZone=us-west-2a\nregion=us-west-2\n route=volumesForService server=dew-lady-tw service=ebs storageDriver=ebs task=0 time=1486766084055 tls=false txCR=1486726083 txID=05a5e1fd-094f-40ec-63f6-448d26ddde4f\n\n\n\n\nOmitted service definition\n\n\nThis error occurs when the configuration file omits a service definition or\none is named erroneously:\n\n\nConsole\n\n\nERRO[0000] error starting libStorage server              error.obj=\nnil\n error.configKey=libstorage.server.services time=1486772035132\n\n\n\n\nService Log\n\n\ntime=\n2017-02-11T00:13:25Z\n level=error msg=\nerror starting libStorage server\n error.configKey=libstorage.server.services error.obj=\nnil\n time=1486772005732\ntime=\n2017-02-11T00:13:25Z\n level=error msg=\ndefault module(s) failed to initialize\n error.obj=\nnil\n error.configKey=libstorage.server.services time=1486772005732\ntime=\n2017-02-11T00:13:25Z\n level=error msg=\ndaemon failed to initialize\n error.configKey=libstorage.server.services error.obj=\nnil\n time=1486772005732\ntime=\n2017-02-11T00:13:25Z\n level=error msg=\nerror starting rex-ray\n error.obj=\nnil\n error.configKey=libstorage.server.services time=1486772005732", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/troubleshooting/#troubleshooting", 
            "text": "It's not doing what I expected...", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/troubleshooting/#solving-problems", 
            "text": "This section details the usual places and methods to look and use when\ninvestigating a problem.", 
            "title": "Solving problems"
        }, 
        {
            "location": "/user-guide/troubleshooting/#is-rex-ray-running", 
            "text": "Confirm REX-Ray is running with the following command:  $ sudo rexray service status\n\nActive: active (running) since Sun 2017-02-12 02:19:16 UTC; 1 day 16h ago", 
            "title": "Is REX-Ray running?"
        }, 
        {
            "location": "/user-guide/troubleshooting/#is-rex-ray-listening", 
            "text": "Confirm REX-Ray is listening on a UNIX socket with the following command:  $ sudo lsof -noPU -a -c rexray\nCOMMAND  PID USER   FD   TYPE             DEVICE OFFSET  NODE NAME\nrexray  3228 root    5u  unix 0xffff8800cc6eb800    0t0 38218 /var/run/libstorage/277666194.sock\nrexray  3228 root    7u  unix 0xffff880117792000    0t0 34741 socket\nrexray  3228 root    8u  unix 0xffff880117792400    0t0 38221 /var/run/libstorage/277666194.sock\nrexray  3228 root    9u  unix 0xffff8800cc6e8c00    0t0 38226 /run/docker/plugins/rexray.sock  Confirm REX-Ray is listening on a TCP port with the following command:  $ sudo lsof -noP -i -sTCP:LISTEN -a -c rexray\nCOMMAND  PID USER   FD   TYPE DEVICE OFFSET NODE NAME\nrexray  3400 root    4u  IPv4  38868    0t0  TCP 127.0.0.1:5002 (LISTEN)", 
            "title": "Is REX-Ray listening?"
        }, 
        {
            "location": "/user-guide/troubleshooting/#is-rex-ray-talking", 
            "text": "After confirming that the service is listening as expected, you can use  curl \nto actually invoke the libStorage REST API. This example shows doing it using\nlocalhost but in a client server topology deployment you can also invoke it\nfrom your client nodes (using the server\u2019s external IP) to confirm that there\nare no routing or firewall issues. This example uses http, but if you have\ninstalled certificates, you should use https instead. This particular\ninvocation of the REST API lists the services (storage provider classes)\nthat are available to clients.  $ curl http://127.0.0.1:5002/services  {\n   ebs : {\n     name :  ebs ,\n     driver : {\n       name :  ebs ,\n       type :  block ,\n       nextDevice : {\n         ignore : false,\n         prefix :  xvd ,\n         pattern :  [f-p] \n      }\n    }\n  }\n}", 
            "title": "Is REX-Ray talking?"
        }, 
        {
            "location": "/user-guide/troubleshooting/#common-errors", 
            "text": "This section reviews common errors encountered when using REX-Ray.   note  General note: When running on a public cloud provider the error messages in\nthe log resulting from calls to a cloud provider API are repeated from the\ncloud provider\u2019s API. Sometimes these are not intuitive. For example a bad\ncredential might result in an error message related to a non-existent\nobject. Performing a google search in the context of your cloud provider,\nrather than REX-Ray can sometimes prove to be helpful.", 
            "title": "Common Errors"
        }, 
        {
            "location": "/user-guide/troubleshooting/#starting-rex-ray", 
            "text": "This error occurs when attempting to start REX-Ray service as a normal account,\nrather than root or with  sudo :  $ rexray service start\nFailed to start rexray.service: Interactive authentication required.", 
            "title": "Starting REX-Ray"
        }, 
        {
            "location": "/user-guide/troubleshooting/#omitted-service-flag", 
            "text": "This error occurs when there are multiple services configured and the service\nspecification is omitted from the command line:  $ rexray volume ls\nFATA[0000] http error                                    status=404", 
            "title": "Omitted service flag"
        }, 
        {
            "location": "/user-guide/troubleshooting/#missing-rex-ray-config-file", 
            "text": "This error can occur when the REX-Ray configuration file is in the incorrect\nlocation or one does not exist at all:  $ rexray volume ls\nERRO[0000] error starting libStorage server              error.configKey=libstorage.server.services error.obj= nil  time=1486771348853`", 
            "title": "Missing REX-Ray config file"
        }, 
        {
            "location": "/user-guide/troubleshooting/#invalid-provider-credentials", 
            "text": "This error occurs when invalid credentials are provided for the storage\nprovider. The example below uses the EBS storage provider:  time= 2017-02-10T22:34:44Z  level=error msg= error getting volume  host= tcp://127.0.0.1:7979  inner= AuthFailure: AWS was not able to validate the provided access credentials\\n\\tstatus code: 401, request id: ea6587a90-f29d-4f14-99da-6a7ec7cb05c1  instanceID= ebs=i-0213cc11c4ade43fb,availabilityZone=us-west-2a region=us-west-2  route=volumesForService server=dew-lady-tw service=ebs storageDriver=ebs task=0 time=1486766084055 tls=false txCR=1486726083 txID=05a5e1fd-094f-40ec-63f6-448d26ddde4f", 
            "title": "Invalid provider credentials"
        }, 
        {
            "location": "/user-guide/troubleshooting/#omitted-service-definition", 
            "text": "This error occurs when the configuration file omits a service definition or\none is named erroneously:", 
            "title": "Omitted service definition"
        }, 
        {
            "location": "/user-guide/troubleshooting/#console", 
            "text": "ERRO[0000] error starting libStorage server              error.obj= nil  error.configKey=libstorage.server.services time=1486772035132", 
            "title": "Console"
        }, 
        {
            "location": "/user-guide/troubleshooting/#service-log", 
            "text": "time= 2017-02-11T00:13:25Z  level=error msg= error starting libStorage server  error.configKey=libstorage.server.services error.obj= nil  time=1486772005732\ntime= 2017-02-11T00:13:25Z  level=error msg= default module(s) failed to initialize  error.obj= nil  error.configKey=libstorage.server.services time=1486772005732\ntime= 2017-02-11T00:13:25Z  level=error msg= daemon failed to initialize  error.configKey=libstorage.server.services error.obj= nil  time=1486772005732\ntime= 2017-02-11T00:13:25Z  level=error msg= error starting rex-ray  error.obj= nil  error.configKey=libstorage.server.services time=1486772005732", 
            "title": "Service Log"
        }, 
        {
            "location": "/dev-guide/project-guidelines/", 
            "text": "Project Guidelines\n\n\nThese are important.\n\n\n\n\nPeople contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.\n\n\nDocumentation\n\n\nThere are two types of documentation: source and markdown.\n\n\nSource Code\n\n\nAll source code should be documented in accordance with the\n\nGo's documentation rules\n.\n\n\nMarkdown\n\n\nWhen creating or modifying the project's \nREADME.md\n file or any of the\ndocumentation in the \n.docs\n directory, please keep the following rules in\nmind:\n\n\n\n\nAll links to internal resources should be relative.\n\n\nAll links to markdown files should include the file extension.\n\n\n\n\nFor example, the below link points to the anchor \nbasic-configuration\n on the\n\nConfiguration\n page:\n\n\n\n\n/user-guide/config#basic-configuration\n\n\nHowever, when the above link is followed when viewing this page directly from\nthe Github repository instead of the generated site documentation, the link\nwill return a 404.\n\n\nWhile it's recommended that users view the generated site documentation instead\nof the source Markdown directly, we can still fix it so that the above link\nwill work regardless. To fix the link, simply make it relative and add the\nMarkdown file extension:\n\n\n\n\n../user-guide/config.md#basic-configuration\n\n\nNow the link will work regardless from where it's viewed.\n\n\nStyle \n Syntax\n\n\nAll source files should be processed by the following tools prior to being\ncommitted. Any errors or warnings produced by the tools should be corrected\nbefore the source is committed.\n\n\n\n\n\n\n\n\nTool\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngofmt\n\n\nA golang source formatting tool\n\n\n\n\n\n\ngolint\n\n\nA golang linter\n\n\n\n\n\n\ngovet\n\n\nA golang source optimization tool\n\n\n\n\n\n\ngocyclo\n\n\nA golang cyclomatic complexity detection tool. No function should have a score above 0.15\n\n\n\n\n\n\n\n\nIf \nAtom\n is your IDE of choice, install the\n\ngo-plus\n package, and it will execute all of\nthe tools above less gocyclo upon saving a file.\n\n\nIn lieu of using Atom as the IDE, the project's \nMakefile\n automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.\n\n\nAnother option is to use a client-side, pre-commit hook to ensure that the\nsources meet the required standards. For example, in the project's \n.git/hooks\n\ndirectory create a file called \npre-commit\n and mark it as executable. Then\npaste the following content inside the file:\n\n\n#!/bin/sh\nmake fmt 1\n /dev/null\n\n\n\n\nThe above script will execute prior to a Git commit operation, prior to even\nthe commit message dialog. The script will invoke the \nMakefile\n's \nfmt\n\ntarget, formatting the sources. If the command returns a non-zero exit code,\nthe commit operation will abort with the error.\n\n\nCode Coverage\n\n\nAll new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.\n\n\nThis project uses\n\nCoveralls\n for code coverage, and\nall pull requests are processed just as a build from \nmaster\n. If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.\n\n\nIt's also possible to test the project locally while outputting the code\ncoverage. On the command line, from the project's root directory, execute the\nfollowing:\n\n\n$ .build/test.sh\nok      github.com/codedellemc/rexray/rexray/cli    0.039s  coverage: 33.6% of statements\nok      github.com/codedellemc/rexray/test  0.080s  coverage: 94.0% of statements in github.com/codedellemc/rexray, github.com/codedellemc/rexray/core\n...\nok      github.com/codedellemc/rexray/util  0.024s  coverage: 100.0% of statements\n[0]akutz@pax:rexray$\n\n\n\n\nThe file \ntest.sh\n in the \n.build\n directory is the same script executed during\nthe project's \nautomated build system\n. The only\ndifference is when executed locally the results are not submitted to Coveralls.\nStill, using the \ntest.sh\n file one can easily determine if a package's coverage\nhas decreased and if additional testing is necessary.\n\n\nCommit Messages\n\n\nCommit messages should follow the guide \n5 Useful Tips For a Better Commit\nMessage\n.\nThe two primary rules to which to adhere are:\n\n\n\n\n\n\nCommit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.\n\n\n\n\n\n\nThe commit message's body should not have a width that exceeds 72\n     characters.\n\n\n\n\n\n\nFor example, the following commit has a very useful message that is succinct\nwithout losing utility.\n\n\ncommit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz \nsakutz@gmail.com\n\nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.\n\n\n\n\nPlease note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s\n -s\nAdded --format,-f option for CLI\n\n\n\n\nIt's also equally simple to print the commit's subject and body together:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s%n%n%b\n -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.\n\n\n\n\nSubmitting Changes\n\n\nAll developers are required to follow the\n\nGitHub Flow model\n when\nproposing new features or even submitting fixes.\n\n\nPlease note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a \nfork\n of this project, not from within a\nbranch of this project itself.\n\n\nPull requests submitted to this project should adhere to the following\nguidelines:\n\n\n\n\n\n\nBranches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.\n\n\n\n\n\n\nUnless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#project-guidelines", 
            "text": "These are important.   People contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#documentation", 
            "text": "There are two types of documentation: source and markdown.", 
            "title": "Documentation"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#source-code", 
            "text": "All source code should be documented in accordance with the Go's documentation rules .", 
            "title": "Source Code"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#markdown", 
            "text": "When creating or modifying the project's  README.md  file or any of the\ndocumentation in the  .docs  directory, please keep the following rules in\nmind:   All links to internal resources should be relative.  All links to markdown files should include the file extension.   For example, the below link points to the anchor  basic-configuration  on the Configuration  page:   /user-guide/config#basic-configuration  However, when the above link is followed when viewing this page directly from\nthe Github repository instead of the generated site documentation, the link\nwill return a 404.  While it's recommended that users view the generated site documentation instead\nof the source Markdown directly, we can still fix it so that the above link\nwill work regardless. To fix the link, simply make it relative and add the\nMarkdown file extension:   ../user-guide/config.md#basic-configuration  Now the link will work regardless from where it's viewed.", 
            "title": "Markdown"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#style-syntax", 
            "text": "All source files should be processed by the following tools prior to being\ncommitted. Any errors or warnings produced by the tools should be corrected\nbefore the source is committed.     Tool  Description      gofmt  A golang source formatting tool    golint  A golang linter    govet  A golang source optimization tool    gocyclo  A golang cyclomatic complexity detection tool. No function should have a score above 0.15     If  Atom  is your IDE of choice, install the go-plus  package, and it will execute all of\nthe tools above less gocyclo upon saving a file.  In lieu of using Atom as the IDE, the project's  Makefile  automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.  Another option is to use a client-side, pre-commit hook to ensure that the\nsources meet the required standards. For example, in the project's  .git/hooks \ndirectory create a file called  pre-commit  and mark it as executable. Then\npaste the following content inside the file:  #!/bin/sh\nmake fmt 1  /dev/null  The above script will execute prior to a Git commit operation, prior to even\nthe commit message dialog. The script will invoke the  Makefile 's  fmt \ntarget, formatting the sources. If the command returns a non-zero exit code,\nthe commit operation will abort with the error.", 
            "title": "Style &amp; Syntax"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#code-coverage", 
            "text": "All new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.  This project uses Coveralls  for code coverage, and\nall pull requests are processed just as a build from  master . If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.  It's also possible to test the project locally while outputting the code\ncoverage. On the command line, from the project's root directory, execute the\nfollowing:  $ .build/test.sh\nok      github.com/codedellemc/rexray/rexray/cli    0.039s  coverage: 33.6% of statements\nok      github.com/codedellemc/rexray/test  0.080s  coverage: 94.0% of statements in github.com/codedellemc/rexray, github.com/codedellemc/rexray/core\n...\nok      github.com/codedellemc/rexray/util  0.024s  coverage: 100.0% of statements\n[0]akutz@pax:rexray$  The file  test.sh  in the  .build  directory is the same script executed during\nthe project's  automated build system . The only\ndifference is when executed locally the results are not submitted to Coveralls.\nStill, using the  test.sh  file one can easily determine if a package's coverage\nhas decreased and if additional testing is necessary.", 
            "title": "Code Coverage"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#commit-messages", 
            "text": "Commit messages should follow the guide  5 Useful Tips For a Better Commit\nMessage .\nThe two primary rules to which to adhere are:    Commit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.    The commit message's body should not have a width that exceeds 72\n     characters.    For example, the following commit has a very useful message that is succinct\nwithout losing utility.  commit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz  sakutz@gmail.com \nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.  Please note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s  -s\nAdded --format,-f option for CLI  It's also equally simple to print the commit's subject and body together:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s%n%n%b  -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.", 
            "title": "Commit Messages"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#submitting-changes", 
            "text": "All developers are required to follow the GitHub Flow model  when\nproposing new features or even submitting fixes.  Please note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a  fork  of this project, not from within a\nbranch of this project itself.  Pull requests submitted to this project should adhere to the following\nguidelines:    Branches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.    Unless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Submitting Changes"
        }, 
        {
            "location": "/dev-guide/build-reference/", 
            "text": "Build Reference\n\n\nHow to build REX-Ray\n\n\n\n\nBasic Builds\n\n\nThe following one-line command is the quickest, simplest, and most\ndeterministic approach to building REX-Ray:\n\n\n$ git clone https://github.com/codedellemc/rexray \n make -C rexray\n\n\n\n\n\n\nnote\n\n\nThe above \nmake\n command defaults to the \ndocker-build\n target only if\nDocker is detected and running on a host, otherwise the \nbuild\n target is\nused. For more information about the \nbuild\n target, please see the\n\nAdvanced Builds\n section.\n\n\n\n\nBasic Build Requirements\n\n\nBuilding REX-Ray with Docker has the following requirements:\n\n\n\n\n\n\n\n\nRequirement\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nOperating System\n\n\nLinux, OS X\n\n\n\n\n\n\nDocker\n\n\n=1.11\n\n\n\n\n\n\nGNU Make\n\n\n=3.80\n\n\n\n\n\n\nGit\n\n\n= 1.7\n\n\n\n\n\n\n\n\nOS X ships with a very old version of GNU Make, and a package manager like\n\nHomebrew\n can be used to install the required version.\n\n\nBasic Build Targets\n\n\nThe following targets are available when building REX-Ray with Docker:\n\n\n\n\n\n\n\n\nTarget\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndocker-build\n\n\nBuilds REX-Ray inside a Docker container.\n\n\n\n\n\n\ndocker-test\n\n\nExecutes all of the REX-Ray tests inside the container.\n\n\n\n\n\n\ndocker-clean\n\n\nThis target stops and removes the default container used for REX-Ray builds. The name of the default container is \nbuild-rexray\n.\n\n\n\n\n\n\ndocker-clobber\n\n\nThis target stops and removes all Docker containers that have a name that matches the name of the configured container prefix (default prefix is \nbuild-rexray\n).\n\n\n\n\n\n\ndocker-list\n\n\nLists all Docker containers that have a name that matches the name of the configured prefix (default prefix is \nbuild-rexray\n).\n\n\n\n\n\n\n\n\nBasic Build Options\n\n\nThe following options (via environment variables) can be used to influence\nhow REX-Ray is built with Docker:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDRIVERS\n\n\nThis variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command \n$ DRIVERS=\"ebs scaleio\" make docker-build\n would build REX-Ray for only the EBS and ScaleIO storage platforms.\n\n\n\n\n\n\nDBUILD_ONCE\n\n\nWhen set to \n1\n, this environment variable instructs the Makefile to create a temporary, one-time use container for the subsequent build. The container is removed upon a successful build. If the build fails the container is not removed. This is because Makefile error logic is lacking. However, \nmake docker-clobber\n can be used to easily clean up these containers. The containers will follow a given pattern using the container prefix (\nbuild-rexray\n is the default prefix value). The one-time containers use \nPREFIX-EPOCH\n. For example, \nbuild-rexray-1474691232\n.\n\n\n\n\n\n\nDGOOS\n\n\nThis sets the OS target for which to build the REX-Ray binaries. Valid values are \nlinux\n and \ndarwin\n. If omitted the host OS value returned from \nuname -s\n is used instead.\n\n\n\n\n\n\nDLOCAL_IMPORTS\n\n\nSpecify a list of space-delimited import paths that will be copied from the host OS's \nGOPATH\n into the container build's vendor area, overriding the dependency code that would normally be fetched by Glide.\nFor example, the project's \nglide.yaml\n file might specify to build REX-Ray with libStorage v0.2.1. However, the following command will build REX-Ray using the libStorage sources on the host OS at \n$GOPATH/src/github.com/codedellemc/libstorage\n:\n$ DLOCAL_IMPORTS=github.com/codedellemc/libstorage make docker-build\nUsing local sources can sometimes present a problem due to missing dependencies. Please see the next environment variable for instructions on how to overcome this issue.\n\n\n\n\n\n\nDGLIDE_YAML\n\n\nSpecify a file that will be used for the container build in place of the standard \nglide.yaml\n file.\nThis is necessary for occasions when sources injected into the build via the \nDLOCAL_IMPORTS\n variable import packages that are not imported by the package specified in the project's standard \nglide.yaml\n file.\nFor example, if \nglide.yaml\n specifies that REX-Ray depends upon AWS SDK v1.2.2, but \nDLOCAL_IMPORTS\n specifies the value \ngithub.com/aws/aws-sdk-go\n and the AWS SDK source code on the host includes a new dependency not present in the v1.2.2 version, Glide will not fetch the new dependency when doing the container build.\nSo it may be necessary to use \nDGLIDE_YAML\n to provide a superset of the project's standard \nglide.yaml\n file which also includes the dependencies necessary to build the packages specified in \nDLOCAL_IMPORTS\n.\n\n\n\n\n\n\n\n\nAdvanced Builds\n\n\nWhile building REX-Ray with Docker is simple, it ultimately relies on the\nsame \nMakefile\n included in the REX-Ray repository and so it's entirely\npossible (and often desirable) to build REX-Ray directly.\n\n\nAdvanced Build Requirements\n\n\nThis project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to\n\nbuild\n REX-Ray, not run it.\n\n\n\n\n\n\n\n\nRequirement\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nOperating System\n\n\nLinux, OS X\n\n\n\n\n\n\nGo\n\n\n=1.6\n\n\n\n\n\n\nGNU Make\n\n\n=3.80\n\n\n\n\n\n\nGlide\n\n\n=0.10\n\n\n\n\n\n\nX-Code Command Line Tools (OS X only)\n\n\n= OS X 10.9\n\n\n\n\n\n\nLinux Kernel Headers (Linux only)\n\n\n=Linux Kernel 3.13\n\n\n\n\n\n\nGNU C Compiler\n (Linux only)\n\n\n= 4.8\n\n\n\n\n\n\nPerl\n\n\n= 5.0\n\n\n\n\n\n\nGit\n\n\n= 1.7\n\n\n\n\n\n\n\n\nOS X ships with a very old version of GNU Make, and a package manager like\n\nHomebrew\n can be used to install the required version.\n\n\nIt's also possible to use GCC as the Cgo compiler for OS X or to use Clang on\nLinux, but by default Clang is used on OS X and GCC on Linux.\n\n\nAdvanced Build Targets\n\n\nThe following targets are available when building REX-Ray directly:\n\n\n\n\n\n\n\n\nTarget\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbuild\n\n\nBuilds REX-Ray.\n\n\n\n\n\n\ntest\n\n\nExecutes all of the REX-Ray tests.\n\n\n\n\n\n\nclean\n\n\nThis target removes all of the source file markers.\n\n\n\n\n\n\nclobber\n\n\nThis is the same as \nclean\n but also removes any produced artifacts.\n\n\n\n\n\n\n\n\nAdvanced Build Options\n\n\nThe following options (via environment variables) can be used to influence\nhow REX-Ray is built:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDRIVERS\n\n\nThis variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command \n$ DRIVERS=\"ebs scaleio\" make build\n would build REX-Ray for only the EBS and ScaleIO storage platforms.\n\n\n\n\n\n\n\n\nVersion File\n\n\nThere is a file at the root of the project named \nVERSION\n. The file contains\na single line with the \ntarget\n version of the project in the file. The version\nfollows the format:\n\n\n(?\nmajor\n\\d+)\\.(?\nminor\n\\d+)\\.(?\npatch\n\\d+)(-rc\\d+)?\n\n\nFor example, during active development of version \n0.1.0\n the file would\ncontain the version \n0.1.0\n. When it's time to create \n0.1.0\n's first\nrelease candidate the version in the file will be changed to \n0.1.0-rc1\n. And\nwhen it's time to release \n0.1.0\n the version is changed back to \n0.1.0\n.\n\n\nSo what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the \nVERSION\n file in fact has two purposes:\n\n\n\n\n\n\nFirst and foremost updating the \nVERSION\n file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of \nmaster\n would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the \nVERSION\n file is much cleaner.\n\n\n\n\n\n\nThe contents of the \nVERSION\n file are also used during the build process\n     as a means of overriding the output of a \ngit describe\n. This enables the\n     semantic version injected into the produced binary to be created using\n     the \ntargeted\n version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-reference", 
            "text": "How to build REX-Ray", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-builds", 
            "text": "The following one-line command is the quickest, simplest, and most\ndeterministic approach to building REX-Ray:  $ git clone https://github.com/codedellemc/rexray   make -C rexray   note  The above  make  command defaults to the  docker-build  target only if\nDocker is detected and running on a host, otherwise the  build  target is\nused. For more information about the  build  target, please see the Advanced Builds  section.", 
            "title": "Basic Builds"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build-requirements", 
            "text": "Building REX-Ray with Docker has the following requirements:     Requirement  Version      Operating System  Linux, OS X    Docker  =1.11    GNU Make  =3.80    Git  = 1.7     OS X ships with a very old version of GNU Make, and a package manager like Homebrew  can be used to install the required version.", 
            "title": "Basic Build Requirements"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build-targets", 
            "text": "The following targets are available when building REX-Ray with Docker:     Target  Description      docker-build  Builds REX-Ray inside a Docker container.    docker-test  Executes all of the REX-Ray tests inside the container.    docker-clean  This target stops and removes the default container used for REX-Ray builds. The name of the default container is  build-rexray .    docker-clobber  This target stops and removes all Docker containers that have a name that matches the name of the configured container prefix (default prefix is  build-rexray ).    docker-list  Lists all Docker containers that have a name that matches the name of the configured prefix (default prefix is  build-rexray ).", 
            "title": "Basic Build Targets"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build-options", 
            "text": "The following options (via environment variables) can be used to influence\nhow REX-Ray is built with Docker:     Environment Variable  Description      DRIVERS  This variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command  $ DRIVERS=\"ebs scaleio\" make docker-build  would build REX-Ray for only the EBS and ScaleIO storage platforms.    DBUILD_ONCE  When set to  1 , this environment variable instructs the Makefile to create a temporary, one-time use container for the subsequent build. The container is removed upon a successful build. If the build fails the container is not removed. This is because Makefile error logic is lacking. However,  make docker-clobber  can be used to easily clean up these containers. The containers will follow a given pattern using the container prefix ( build-rexray  is the default prefix value). The one-time containers use  PREFIX-EPOCH . For example,  build-rexray-1474691232 .    DGOOS  This sets the OS target for which to build the REX-Ray binaries. Valid values are  linux  and  darwin . If omitted the host OS value returned from  uname -s  is used instead.    DLOCAL_IMPORTS  Specify a list of space-delimited import paths that will be copied from the host OS's  GOPATH  into the container build's vendor area, overriding the dependency code that would normally be fetched by Glide. For example, the project's  glide.yaml  file might specify to build REX-Ray with libStorage v0.2.1. However, the following command will build REX-Ray using the libStorage sources on the host OS at  $GOPATH/src/github.com/codedellemc/libstorage : $ DLOCAL_IMPORTS=github.com/codedellemc/libstorage make docker-build Using local sources can sometimes present a problem due to missing dependencies. Please see the next environment variable for instructions on how to overcome this issue.    DGLIDE_YAML  Specify a file that will be used for the container build in place of the standard  glide.yaml  file. This is necessary for occasions when sources injected into the build via the  DLOCAL_IMPORTS  variable import packages that are not imported by the package specified in the project's standard  glide.yaml  file. For example, if  glide.yaml  specifies that REX-Ray depends upon AWS SDK v1.2.2, but  DLOCAL_IMPORTS  specifies the value  github.com/aws/aws-sdk-go  and the AWS SDK source code on the host includes a new dependency not present in the v1.2.2 version, Glide will not fetch the new dependency when doing the container build. So it may be necessary to use  DGLIDE_YAML  to provide a superset of the project's standard  glide.yaml  file which also includes the dependencies necessary to build the packages specified in  DLOCAL_IMPORTS .", 
            "title": "Basic Build Options"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-builds", 
            "text": "While building REX-Ray with Docker is simple, it ultimately relies on the\nsame  Makefile  included in the REX-Ray repository and so it's entirely\npossible (and often desirable) to build REX-Ray directly.", 
            "title": "Advanced Builds"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-build-requirements", 
            "text": "This project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to build  REX-Ray, not run it.     Requirement  Version      Operating System  Linux, OS X    Go  =1.6    GNU Make  =3.80    Glide  =0.10    X-Code Command Line Tools (OS X only)  = OS X 10.9    Linux Kernel Headers (Linux only)  =Linux Kernel 3.13    GNU C Compiler  (Linux only)  = 4.8    Perl  = 5.0    Git  = 1.7     OS X ships with a very old version of GNU Make, and a package manager like Homebrew  can be used to install the required version.  It's also possible to use GCC as the Cgo compiler for OS X or to use Clang on\nLinux, but by default Clang is used on OS X and GCC on Linux.", 
            "title": "Advanced Build Requirements"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-build-targets", 
            "text": "The following targets are available when building REX-Ray directly:     Target  Description      build  Builds REX-Ray.    test  Executes all of the REX-Ray tests.    clean  This target removes all of the source file markers.    clobber  This is the same as  clean  but also removes any produced artifacts.", 
            "title": "Advanced Build Targets"
        }, 
        {
            "location": "/dev-guide/build-reference/#advanced-build-options", 
            "text": "The following options (via environment variables) can be used to influence\nhow REX-Ray is built:     Environment Variable  Description      DRIVERS  This variable can be set to a space-delimited list of driver names in order to indicate which storage platforms to support. For example, the command  $ DRIVERS=\"ebs scaleio\" make build  would build REX-Ray for only the EBS and ScaleIO storage platforms.", 
            "title": "Advanced Build Options"
        }, 
        {
            "location": "/dev-guide/build-reference/#version-file", 
            "text": "There is a file at the root of the project named  VERSION . The file contains\na single line with the  target  version of the project in the file. The version\nfollows the format:  (? major \\d+)\\.(? minor \\d+)\\.(? patch \\d+)(-rc\\d+)?  For example, during active development of version  0.1.0  the file would\ncontain the version  0.1.0 . When it's time to create  0.1.0 's first\nrelease candidate the version in the file will be changed to  0.1.0-rc1 . And\nwhen it's time to release  0.1.0  the version is changed back to  0.1.0 .  So what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the  VERSION  file in fact has two purposes:    First and foremost updating the  VERSION  file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of  master  would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the  VERSION  file is much cleaner.    The contents of the  VERSION  file are also used during the build process\n     as a means of overriding the output of a  git describe . This enables the\n     semantic version injected into the produced binary to be created using\n     the  targeted  version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Version File"
        }, 
        {
            "location": "/dev-guide/release-process/", 
            "text": "Release Process\n\n\nHow to release REX-Ray\n\n\n\n\nProject Stages\n\n\nThis project has three parallels stages of release:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nunstable\n\n\nThe tip or HEAD of the \nmaster\n branch is referred to as \nunstable\n\n\n\n\n\n\nstaged\n\n\nA commit tagged with the suffix \n-rc\\d+\n such as \nv0.3.1-rc2\n is a \nstaged\n release. These are release candidates.\n\n\n\n\n\n\nstable\n\n\nA commit tagged with a version sans \n-rc\\d+\n suffix such as \nv0.3.1\n is a \nstable\n release.\n\n\n\n\n\n\n\n\nThere are no steps necessary to create an \nunstable\n release as that happens\nautomatically whenever an untagged commit is pushed to \nmaster\n. However, the\nfollowing workflow should be used when tagging a \nstaged\n release candidate\nor \nstable\n release.\n\n\n\n\nReview outstanding issues \n pull requests\n\n\nPrepare release notes\n\n\nUpdate the version file\n\n\nCommit \n pull request\n\n\nTag the release\n\n\n\n\nReview Issues \n Pull Requests\n\n\nThe first step to a release is to review the outstanding\n\nissues\n and\n\npull requests\n that are tagged for\nthe release in question.\n\n\nIf there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.\n\n\nIt is \nhighly\n recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of \nmaster\n. Remember, while\nGitHub will update a pull request as in conflict if a change to \nmaster\n\nresults in a merge conflict with the pull request, GitHub will \nnot\n force a\nnew build to spawn unless the pull request is actually updated.\n\n\nAt the very minimum a pull request's build should be re-executed prior to the\npull request being merged if \nmaster\n has changed since the pull request was\nopened.\n\n\nPrepare Release Notes\n\n\nUpdate the release notes at \n.docs/about/release-notes.md\n. This file is the\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.\n\n\nThe most recent, \nstable\n version of the release notes are always available\nonline at\n\nREX-Ray's documentation site\n.\n\n\nUpdate Version File\n\n\nThe \nVERSION\n file exists at the root of the project and should be updated to\nreflect the value of the intended release.\n\n\nFor example, if creating the first release candidate for version 0.3.1, the\ncontents of the \nVERSION\n file should be a single line \n0.3.1-rc1\n followed by\na newline character:\n\n\n$ cat VERSION\n0.3.1-rc1\n\n\n\n\nIf releasing version 0.3.1 proper then the contents of the \nVERSION\n file\nshould be \n0.3.1\n followed by a newline character:\n\n\n$ cat VERSION\n0.3.1\n\n\n\n\nCommit \n Pull Request\n\n\nOnce all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.\n\n\nPlease make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.\n\n\nA release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:\n\n\nRelease Candidate 0.3.1-rc1\n\nThis patch marks release candidate 0.3.1-rc1.\n\n\n\n\nIf the commit message is longer it should simply reflect the same information\nfrom the release notes.\n\n\nOnce committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.\n\n\nTag the Release\n\n\nOnce the pull request marking the \nstaged\n or \nstable\n release has been merged\ninto \nupstream\n's \nmaster\n it's time to tag the release.\n\n\nTag Format\n\n\nThe release tag should follow a prescribed format depending upon the release\ntype:\n\n\n\n\n\n\n\n\nRelease Type\n\n\nTag Format\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nstaged\n\n\nvMAJOR.MINOR.PATCH-rc[0-9]\n\n\nv0.3.1-rc1\n\n\n\n\n\n\nstable\n\n\nvMAJOR.MINOR.PATCH\n\n\nv0.3.1\n\n\n\n\n\n\n\n\nTag Methods\n\n\nThere are two ways to tag a release:\n\n\n\n\nGitHub Releases\n\n\nCommand Line\n\n\n\n\nCommand Line\n\n\nIf tagging a release via the command line be sure to fetch the latest changes\nfrom \nupstream\n's \nmaster\n and either merge them into your local copy of\n\nmaster\n or reset the local copy to reflect \nupstream\n prior to creating\nany tags.\n\n\nThe following combination of commands can be used to create a tag for\n0.3.1 Release Candidate 1:\n\n\ngit fetch upstream \n \\\n  git checkout master \n \\\n  git reset --hard upstream/master \n \\\n  git tag -a -m v0.3.1-rc1 v0.3.1-rc1\n\n\n\n\nThe above example combines a few operations:\n\n\n\n\nThe first command fetches the \nupstream\n changes\n\n\nThe local \nmaster\n branch is checked out\n\n\nThe local \nmaster\n branch is hard reset to \nupstream/master\n\n\nAn annotated tag is created on \nmaster\n for \nv0.3.1-rc1\n, or 0.3.1 Release\n     Candidate 1, with a tag message of \nv0.3.1-rc1\n.\n\n\n\n\nPlease note that the third step will erase any changes that exist only in the\nlocal \nmaster\n branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.\n\n\nThe above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a \nstaged\n or\n\nstable\n release. For \nstable\n releases the project's documentation will also be\nupdated.\n\n\nOnce positive everything looks good simply execute the following command to\npush the tag to the \nupstream\n repository:\n\n\ngit push upstream v0.3.1-rc1", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#release-process", 
            "text": "How to release REX-Ray", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#project-stages", 
            "text": "This project has three parallels stages of release:     Name  Description      unstable  The tip or HEAD of the  master  branch is referred to as  unstable    staged  A commit tagged with the suffix  -rc\\d+  such as  v0.3.1-rc2  is a  staged  release. These are release candidates.    stable  A commit tagged with a version sans  -rc\\d+  suffix such as  v0.3.1  is a  stable  release.     There are no steps necessary to create an  unstable  release as that happens\nautomatically whenever an untagged commit is pushed to  master . However, the\nfollowing workflow should be used when tagging a  staged  release candidate\nor  stable  release.   Review outstanding issues   pull requests  Prepare release notes  Update the version file  Commit   pull request  Tag the release", 
            "title": "Project Stages"
        }, 
        {
            "location": "/dev-guide/release-process/#review-issues-pull-requests", 
            "text": "The first step to a release is to review the outstanding issues  and pull requests  that are tagged for\nthe release in question.  If there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.  It is  highly  recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of  master . Remember, while\nGitHub will update a pull request as in conflict if a change to  master \nresults in a merge conflict with the pull request, GitHub will  not  force a\nnew build to spawn unless the pull request is actually updated.  At the very minimum a pull request's build should be re-executed prior to the\npull request being merged if  master  has changed since the pull request was\nopened.", 
            "title": "Review Issues &amp; Pull Requests"
        }, 
        {
            "location": "/dev-guide/release-process/#prepare-release-notes", 
            "text": "Update the release notes at  .docs/about/release-notes.md . This file is the\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.  The most recent,  stable  version of the release notes are always available\nonline at REX-Ray's documentation site .", 
            "title": "Prepare Release Notes"
        }, 
        {
            "location": "/dev-guide/release-process/#update-version-file", 
            "text": "The  VERSION  file exists at the root of the project and should be updated to\nreflect the value of the intended release.  For example, if creating the first release candidate for version 0.3.1, the\ncontents of the  VERSION  file should be a single line  0.3.1-rc1  followed by\na newline character:  $ cat VERSION\n0.3.1-rc1  If releasing version 0.3.1 proper then the contents of the  VERSION  file\nshould be  0.3.1  followed by a newline character:  $ cat VERSION\n0.3.1", 
            "title": "Update Version File"
        }, 
        {
            "location": "/dev-guide/release-process/#commit-pull-request", 
            "text": "Once all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.  Please make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.  A release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:  Release Candidate 0.3.1-rc1\n\nThis patch marks release candidate 0.3.1-rc1.  If the commit message is longer it should simply reflect the same information\nfrom the release notes.  Once committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.", 
            "title": "Commit &amp; Pull Request"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-the-release", 
            "text": "Once the pull request marking the  staged  or  stable  release has been merged\ninto  upstream 's  master  it's time to tag the release.", 
            "title": "Tag the Release"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-format", 
            "text": "The release tag should follow a prescribed format depending upon the release\ntype:     Release Type  Tag Format  Example      staged  vMAJOR.MINOR.PATCH-rc[0-9]  v0.3.1-rc1    stable  vMAJOR.MINOR.PATCH  v0.3.1", 
            "title": "Tag Format"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-methods", 
            "text": "There are two ways to tag a release:   GitHub Releases  Command Line", 
            "title": "Tag Methods"
        }, 
        {
            "location": "/dev-guide/release-process/#command-line", 
            "text": "If tagging a release via the command line be sure to fetch the latest changes\nfrom  upstream 's  master  and either merge them into your local copy of master  or reset the local copy to reflect  upstream  prior to creating\nany tags.  The following combination of commands can be used to create a tag for\n0.3.1 Release Candidate 1:  git fetch upstream   \\\n  git checkout master   \\\n  git reset --hard upstream/master   \\\n  git tag -a -m v0.3.1-rc1 v0.3.1-rc1  The above example combines a few operations:   The first command fetches the  upstream  changes  The local  master  branch is checked out  The local  master  branch is hard reset to  upstream/master  An annotated tag is created on  master  for  v0.3.1-rc1 , or 0.3.1 Release\n     Candidate 1, with a tag message of  v0.3.1-rc1 .   Please note that the third step will erase any changes that exist only in the\nlocal  master  branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.  The above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a  staged  or stable  release. For  stable  releases the project's documentation will also be\nupdated.  Once positive everything looks good simply execute the following command to\npush the tag to the  upstream  repository:  git push upstream v0.3.1-rc1", 
            "title": "Command Line"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing to REX-Ray\n\n\nAn introduction to contributing to the REX-Ray project\n\n\n\n\nThe REX-Ray project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:\n\n\n\n\nCode patches via pull requests\n\n\nDocumentation improvements\n\n\nBug reports and patch reviews\n\n\nOS, Storage, and Volume Drivers\n\n\nA distributed server/client model with profile support\n\n\n\n\nReporting an Issue\n\n\nPlease include as much detail as you can. This includes:\n\n\n\n\nThe OS type and version\n\n\nThe REX-Ray version\n\n\nThe storage system in question\n\n\nA set of logs with debug-logging enabled that show the problem\n\n\n\n\nTesting the Development Version\n\n\nIf you want to just install and try out the latest development version of\nREX-Ray you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master. It is \nstrongly\n recommended\nthat you do this within a virtual environment.\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- unstable\n\n\n\n\nInstalling for Development\n\n\nFirst you'll need to fork and clone the repository. Once you have a local\ncopy, run the following command.\n\n\nmake deps \n make\n\n\n\n\nThis will install REX-Ray into your \nGOPATH\n and you'll be able to make changes\nlocally, test them, and commit ideas and fixes back to your fork of the\nrepository.\n\n\nRunning the tests\n\n\nTo run the tests, run the following commands:\n\n\nmake test\n\n\n\n\nSubmitting Pull Requests\n\n\nOnce you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing-to-rex-ray", 
            "text": "An introduction to contributing to the REX-Ray project   The REX-Ray project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:   Code patches via pull requests  Documentation improvements  Bug reports and patch reviews  OS, Storage, and Volume Drivers  A distributed server/client model with profile support", 
            "title": "Contributing to REX-Ray"
        }, 
        {
            "location": "/about/contributing/#reporting-an-issue", 
            "text": "Please include as much detail as you can. This includes:   The OS type and version  The REX-Ray version  The storage system in question  A set of logs with debug-logging enabled that show the problem", 
            "title": "Reporting an Issue"
        }, 
        {
            "location": "/about/contributing/#testing-the-development-version", 
            "text": "If you want to just install and try out the latest development version of\nREX-Ray you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master. It is  strongly  recommended\nthat you do this within a virtual environment.  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s -- unstable", 
            "title": "Testing the Development Version"
        }, 
        {
            "location": "/about/contributing/#installing-for-development", 
            "text": "First you'll need to fork and clone the repository. Once you have a local\ncopy, run the following command.  make deps   make  This will install REX-Ray into your  GOPATH  and you'll be able to make changes\nlocally, test them, and commit ideas and fixes back to your fork of the\nrepository.", 
            "title": "Installing for Development"
        }, 
        {
            "location": "/about/contributing/#running-the-tests", 
            "text": "To run the tests, run the following commands:  make test", 
            "title": "Running the tests"
        }, 
        {
            "location": "/about/contributing/#submitting-pull-requests", 
            "text": "Once you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Submitting Pull Requests"
        }, 
        {
            "location": "/about/license/", 
            "text": "Licensing\n\n\nThe legal stuff\n\n\n\n\nREX-Ray License\n\n\nLicensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at \nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/about/license/#licensing", 
            "text": "The legal stuff", 
            "title": "Licensing"
        }, 
        {
            "location": "/about/license/#rex-ray-license", 
            "text": "Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "REX-Ray License"
        }, 
        {
            "location": "/about/release-notes/", 
            "text": "Release Notes\n\n\n\n\nUpgrading\n\n\nTo upgrade REX-Ray to the latest version, use \ncurl install\n:\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh\n\n\n\nUse \nrexray version\n to determine the currently installed version of REX-Ray:\n\n\n$ rexray version\nREX-Ray\n-------\nBinary: /Users/akutz/Projects/go/bin/rexray\nSemVer: 0.4.0\nOsArch: Linux-x86_64\nBranch: v0.4.0\nCommit: c83f0237e60792cfe89c4255d7149b5670965539\nFormed: Mon, 20 Jun 2016 20:56:48 CDT\n\nlibStorage\n----------\nSemVer: 0.1.3\nOsArch: Linux-x86_64\nBranch: v0.1.3\nCommit: 182a626937677a081b89651598ee2eac839308e7\nFormed: Wed, 15 Jun 2016 16:27:36 CDT\n\n\n\nVersion 0.8.1 (2017/02/24)\n\n\nThis is a minor release that reintroduces support for Go1.6 via\nlibStorage 0.5.1.\n\n\nBug Fixes\n\n\n\n\nGo1.6 support (\n#444\n)\n\n\n\n\nVersion 0.8.0 (2017/02/24)\n\n\nThis is one of the largest releases in a while, including support for five new\nstorage platforms!\n\n\nNew Features\n\n\n\n\nAmazon Simple Storage Service FUSE (S3FS) support (\n#397\n, \n#409\n)\n\n\nGoogle Compute Engine Persistent Disk (GCEPD) support (\n#394\n, \n#416\n)\n\n\nDigitalOcean support (\n#392\n)\n\n\nMicrosoft Azure unmanaged disk support (\n#421\n)\n\n\nFittedCloud support (\n#408\n)\n\n\nDocker Volume Plug-in for EBS (\n#720\n)\n\n\nDocker Volume Plug-in for EFS (\n#729\n)\n\n\nDocker Volume Plug-in for Isilon (\n#727\n)\n\n\nDocker Volume Plug-in for S3FS (\n#724\n)\n\n\nDocker Volume Plug-in for ScaleIO (\n#725\n)\n\n\nREX-Ray on Alpine Linux support (\n#724\n)\n\n\nStorage-platform specific mount/unmount support (\n#399\n)\n\n\nThe ScaleIO tool \ndrv_cfg\n is now an optional client-side dependency instead of required (\n#414\n)\n\n\nMulti-cluster support for ScaleIO (\n#420\n)\n\n\nForced volume remove support (\n#717\n)\n\n\n\n\nBug Fixes\n\n\n\n\nPreemption fix (\n#413\n)\n\n\nCeph RBD monitored IP fix (\n#412\n, \n#424\n)\n\n\nCeph RBD dashes in names fix (\n#425\n)\n\n\nFix for \nlsx-OS wait\n argument count (\n#401\n)\n\n\nBuild fixes (\n#403\n)\n\n\n\n\nThank You\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nChris Duchesne\n\n\nChris is my partner in crime when it comes to libStorage and REX-Ray. Without him I would have absolutely no one to take the fall for the heist I'm planning. So is Chris invaluable? Yeah, in that way, as the patsy who will do at least a dime while I'm on the beach sipping my drink, yeah, he's invaluable.\n\n\n\n\n\n\nTravis Rhoden\n\n\nTravis, or as I call him, T-Dawg, is essential to \"taking care of business.\" He comes to work to chew bubblegum and kick butt, and he leaves the gum at home!\n\n\n\n\n\n\nVladimir Vivien\n\n\nA little known fact about Vladimir is that he's been seeded in the top 10 of the last US Opens, but has had to withdrawal at the last minute before each of those tournaments due to other responsibilities. What those are? Who can say? Are they contracts on people's lives? Perhaps. Are they appearances for Make a Wish? Probably. The only thing we know for sure is that when he is seen again, Vladimir seems rejuvenated and ready to conquer the tennis world yet again.\n\n\n\n\n\n\nSteve Wong\n\n\nI've known Steve for a very long time, and in that time I can say I've never once seen him in the same room as President Barack Obama. Now, does that mean that I can definitively state that Steve and President Obama are in fact the same person. No, of course not. There are obvious differences. The most glaring of course being that Steve wears glasses and President Obama does not. However, other than that the two men are nearly identical. I guess we'll never know if Steve Wong lives a double life as the 44th President of these United States, but I personally would like to think that yeah, he does.\n\n\n\n\n\n\nDan Norris\n\n\nDan \"The Man\" Norris is well known in the underground street-swimming circuit. Last year he tied Michael Phelps in the Santa Monica Sewer 120 meter medley. He would have won if not for stopping to create the DigitalOcean driver for libStorage.\n\n\n\n\n\n\nAlexey Morlang\n\n\nAs a third-chair oboe player in the Moscow orchestra it is surprising that Alexey still finds time to contribute to the project, but coming from a long line of oboligarchs (oboe playing oligarchs), it's just in his nature. As is creating storage drivers. That, and, well, playing the oboe.\n\n\n\n\n\n\nAndrey Pavlov\n\n\nThere is no Andrey. You have not met him. He does not exist. Don't look behind you. He is not there. He is writing storage drivers. Then just like that, he's vanished.\n\n\n\n\n\n\nLax Kota\n\n\nLax is a rock star in the Slack channel, helping others by answering their questions before the project's developers can take a stab. We do not want to upset him. It's rumored he beats those who upset him in order to provide inspiration for his true passion -- corporal poetry. Every punch thrown is another verse towards his masterpiece.\n\n\n\n\n\n\nJack Huang\n\n\nJack is not his job. Jack is not the amount of money he has in the bank. Jack is not the car he drives. Jack is not the clothes he wears. Jack is a supernova, accelerating at the speed of light beyond the bounds of quantifiable space and time. Jack is not the stuff above. Jack is not the stuff below. Jack is not the stuff in between. Jack is not the empty void. Jack. just. is.\n\n\n\n\n\n\n\n\nVersion 0.7.0 (2017/01/23)\n\n\nThis feature release includes support for libStorage 0.4.0 and the Ceph RBD\nstorage platform.\n\n\nEnhancements\n\n\n\n\nlibStorage 0.4.0\n\n\nCeph/RBD storage platform (\n#347\n)\n\n\n\n\nBug Fixes\n\n\n\n\nPrevent unnecessary removal of directory by FlexRex (\n#699\n)\n\n\nUpdate \nvolume attach\n to check for \n--force\n flag (\n#696\n)\n\n\nFix installer to correctly parse new Bintray HTML (\n#687\n)\n\n\n\n\nVersion 0.6.4 (2017/01/05)\n\n\nThis release includes the new script manager and FlexVol REX-Ray plug-in.\n\n\nEnhancements\n\n\n\n\nlibStorage 0.3.8\n\n\nScript manager (\n#669\n)\n\n\nFlexVol plug-in for Kubernetes (\n#641\n)\n\n\n\n\nBug Fixes\n\n\n\n\nPanic on \n$ rexray-client volume mount\n (\n#673\n)\n\n\n\n\nVersion 0.6.3 (2016/12/07)\n\n\nThis release includes the ability to specify a custom encryption key when\ncreating volumes and makes the \nvolume attach\n command idempotent.\n\n\nEnhancements\n\n\n\n\nlibStorage 0.3.5\n\n\nSupport for creating encrypted volumes (\n#649\n, \n#652\n)\n\n\nIdempotent volume attach command (\n#651\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFix volume status for detach op (\n#654\n)\n\n\n\n\nVersion 0.6.2 (2016/12/05)\n\n\nWhile a patch release, this new version includes some much-requested features\nand updates.\n\n\nEnhancements\n\n\n\n\nlibStorage 0.3.4\n\n\nAuto-detect running service (\n#642\n)\n\n\nPrettier error messages (\n#645\n)\n\n\n\n\nBug Fixes\n\n\n\n\nGraceful exit with SystemD (\n#644\n)\n\n\n\n\nVersion 0.6.1 (2016/12/01)\n\n\nThis release includes some minor fixes as well as a new and improved version of\nthe \nvolume ls\n command.\n\n\nEnhancements\n\n\n\n\nlibStorage 0.3.3\n\n\nEnhanced \nvolume ls\n command (\n#634\n)\n\n\n\n\nBug Fixes\n\n\n\n\nEFS Mounting Issues (\n#609\n)\n\n\nVirtualBox Attach Issues (\n#610\n)\n\n\nInstaller upgrade fix (\n#637\n)\n\n\nBuild deployment fix (\n#638\n)\n\n\n\n\nVersion 0.6.0 (2016/10/20)\n\n\nThis release reintroduces the Elastic Block Storage (EBS) driver, formerly known\nas the EC2 driver. All vestigial EC2 configuration properties are still\nsupported.\n\n\nEnhancements\n\n\n\n\nlibStorage 0.3.0 (\n#docs\n)\n\n\nAmazon Elastic Block Storage (EBS) Support (\n#522\n)\n\n\nNew CLI Output (\n#579\n, \n#603\n, \n#606\n)\n\n\nSupport for ScaleIO 2.0.1 (\n#599\n)\n\n\n\n\nBug Fixes\n\n\n\n\nHandle phantom mounts for EBS (formerly EC2) (\n#410\n)\n\n\n\n\nVersion 0.5.1 (2016/09/14)\n\n\nThis is a minor release, but includes a few important patches.\n\n\nEnhancements\n\n\n\n\nlibStorage 0.2.1 (\n#docs\n)\n\n\nScaleIO 2.0.0.2 Support (\n#555\n)\n\n\n\n\nBug Fixes\n\n\n\n\nEFS Volume / Tag Creation Bug (\n#261\n)\n\n\n\n\nVersion 0.5.0 (2016/09/07)\n\n\nBeginning with this release, REX-Ray's versions will increment the MINOR\ncomponent with the introduction of a new storage driver via libStorage in\nconcert with the \nguidelines\n set forth by semantic\nversioning.\n\n\nNew Features\n\n\n\n\nAmazon Elastic File System (EFS) Support (\n#525\n)\n\n\n\n\nEnhancements\n\n\n\n\nSupport for Go 1.7 (\n#541\n)\n\n\nEnhanced Isilon Support (\n#520\n, \n#521\n)\n\n\n\n\nThank You\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nChris Duchesne\n\n\nChris not only took on the role of project manager for libStorage and REX-Ray, he still provides ongoing test plan execution and release validation. Thank you Chris!\n\n\n\n\n\n\nKenny Cole\n\n\nKenny's tireless effort to support users and triage submitted issues is such a cornerstone to libStorage and REX-Ray that I'm not sure what this project would do without him!\n\n\n\n\n\n\nMartin Hrabovcin\n\n\nMartin, along with Kasisnu, definitely win the \"Community Members of the Month\" award! Their hard work and dedication resulted in the introduction of the Amazon EFS storage driver. Thank you Martin \n Kasisnu!\n\n\n\n\n\n\nKasisnu Singh\n\n\nHave I mentioned we have the best community around? Seriously, thank you again Kasisnu! Your work, along with Martin's, is a milestone in the growth of libStorage and REX-Ray.\n\n\n\n\n\n\n\n\nVersion 0.4.2 (2016/07/12)\n\n\nThis minor update represents a \nmajor\n performance boost for REX-Ray.\nOperations that use to take up to minutes now take seconds or less. The memory\nfootprint has been reduced from the magnitude of phenomenal cosmic powers to\nthe size of an itty bitty living space!\n\n\nEnhancements\n\n\n\n\nlibStorage 0.1.5 (\n#TBA\n)\n\n\nImproved volume path caching (\n#500\n)\n\n\n\n\nVersion 0.4.1 (2016/07/08)\n\n\nAlthough a minor release, 0.4.1 provides some meaningful and useful enhancements\nand fixes, further strengthening the foundation of the REX-Ray platform.\n\n\nEnhancements\n\n\n\n\nImproved build process (\n#474\n, \n#492\n)\n\n\nlibStorage\n 0.1.4 (\n#493\n)\n\n\nRemoved Docker spec file (\n#486\n)\n\n\nImproved REX-Ray 0.3.3 Config Backwards Compatibility (\n#481\n)\n\n\nImproved install script (\n#439\n, \n#495\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixed input validation bug when creating volume sans name (\n#478\n)\n\n\n\n\nVersion 0.4.0 (2016/06/20)\n\n\nREX-Ray 0.4.0 introduces centralized configuration and control along with\na new client/server architecture -- features made possible by\n\nlibStorage\n. Users are no longer\nrequired to configure storage drivers or store privileged information on all\nsystems running the REX-Ray client. The new client delegates storage-platform\nrelated operations to a remote, libStorage-compatible server such as REX-Ray\nor \nPoly\n.\n\n\nPlease note that the initial release of REX-Ray 0.4 includes support for only\nthe following storage platforms:\n\n\n\n\nScaleIO\n\n\nVirtualBox\n\n\n\n\nSupport for the full compliment of drivers present in earlier versions of\nREX-Ray will be reintroduced over the course of several, incremental updates,\nbeginning with 0.4.1.\n\n\nNew Features\n\n\n\n\nDistributed architecture (\n#399\n, \n#401\n, \n#411\n, \n#417\n, \n#418\n, \n#419\n, \n#420\n, \n#423\n)\n\n\nVolume locking mechanism (\n#171\n)\n\n\nVolume creation with initial data (\n#169\n)\n\n\n\n\nEnhancements\n\n\n\n\nImproved storage driver logging (\n#396\n)\n\n\nDocker mount path (\n#403\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixed issue with install script (\n#409\n)\n\n\nFixed volume ls filter (\n#400\n)\n\n\nFixed panic during access attempt of offline REX-Ray daemon (\n#148\n)\n\n\n\n\nThank You\n\n\nYes, the author is so lazy as to blatantly\n\ncopy\n\nthis section. So sue me :)\n\n\n\n\n\n\n\n\nName\n\n\nBlame\n\n\n\n\n\n\n\n\n\n\nClint Kitson\n\n\nHis vision come to fruition. That's \nhis\n vision, thus please assign \nall\n bugs to Clint :)\n\n\n\n\n\n\nVladimir Vivien\n\n\nA nascent player, Vlad had to hit the ground running and has been a key contributor\n\n\n\n\n\n\nKenny Coleman\n\n\nWhile some come close, none are comparable to Kenny's handlebar\n\n\n\n\n\n\nJonas Rosland\n\n\nAlways good for a sanity check and keeping things on the straight and narrow\n\n\n\n\n\n\nSteph Carlson\n\n\nSteph keeps the convention train chugging along...\n\n\n\n\n\n\nAmanda Katona\n\n\nAnd Amanda is the one keeping the locomotive from going off the rails\n\n\n\n\n\n\nDrew Smith\n\n\nDrew is always ready to lend a hand, no matter the problem\n\n\n\n\n\n\nChris Duchesne\n\n\nHis short time with the team is in complete opposition to the value he has added to this project\n\n\n\n\n\n\nDavid vonThenen\n\n\nDavid has been a go-to guy for debugging the most difficult of issues\n\n\n\n\n\n\nSteve Wong\n\n\nSteve stays on top of the things and keeps use cases in sync with industry needs\n\n\n\n\n\n\nTravis Rhoden\n\n\nAnother keen mind, Travis is also a great font of technical know-how\n\n\n\n\n\n\nPeter Blum\n\n\nAbsent Peter, the EMC World demo would not have been ready\n\n\n\n\n\n\nMegan Hyland\n\n\nAnd absent Megan, Peter's work would only have taken things halfway there\n\n\n\n\n\n\nEugene Chupriyanov\n\n\nFor helping with the EC2 planning\n\n\n\n\n\n\nMatt Farina\n\n\nWithout Glide, it all comes crashing down\n\n\n\n\n\n\nJosh Bernstein\n\n\nThe shadowy figure behind the curtain...\n\n\n\n\n\n\n\n\nVersion 0.3.3 (2016/04/21)\n\n\nNew Features\n\n\n\n\nScaleIO v2 support (\n#355\n)\n\n\nEC2 Tags added to Volumes \n Snapshots (\n#314\n)\n\n\n\n\nEnhancements\n\n\n\n\nUse of official Amazon EC2 SDK (\n#359\n)\n\n\nAdded a disable feature for create/remove volume (\n#366\n)\n\n\nAdded ScaleIO troubleshooting information (\n#367\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixes URLs for documentation when viewed via Github (\n#337\n)\n\n\nFixes logging bug on Ubuntu 14.04 (\n#377\n)\n\n\nFixes module start timeout error (\n#376\n)\n\n\nFixes ScaleIO authentication loop bug (\n#375\n)\n\n\n\n\nThank You\n\n\n\n\nPhilipp Franke\n\n\nEugene Chupriyanov\n\n\nPeter Blum\n\n\nMegan Hyland\n\n\n\n\nVersion 0.3.2 (2016-03-04)\n\n\nNew Features\n\n\n\n\nSupport for Docker 1.10 and Volume Plugin Interface 1.2 (\n#273\n)\n\n\nStale PID File Prevents Service Start (\n#258\n)\n\n\nModule/Personality Support (\n#275\n)\n\n\nIsilon Preemption (\n#231\n)\n\n\nIsilon Snapshots (\n#260\n)\n\n\nboot2Docker Support (\n#263\n)\n\n\nScaleIO Dynamic Storage Pool Support (\n#267\n)\n\n\n\n\nEnhancements\n\n\n\n\nImproved installation documentation (\n#331\n)\n\n\nScaleIO volume name limitation (\n#304\n)\n\n\nDocker cache volumes for path operations (\n#306\n)\n\n\nConfig file validation (\n#312\n)\n\n\nBetter logging (\n#296\n)\n\n\nDocumentation Updates (\n#285\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixes issue with daemon process getting cleaned as part of SystemD Cgroup (\n#327\n)\n\n\nFixes regression in 0.3.2 RC3/RC4 resulting in no log file (\n#319\n)\n\n\nFixes no volumes returned on empty list (\n#322\n)\n\n\nFixes \"Unsupported FS\" when mounting/unmounting with EC2 (\n#321\n)\n\n\nScaleIO re-authentication issue (\n#303\n)\n\n\nDocker XtremIO create volume issue (\n#307\n)\n\n\nService status is reported correctly (\n#310\n)\n\n\n\n\nUpdates\n\n\n\n\nGo 1.6 (\n#308\n)\n\n\n\n\nThank You\n\n\n\n\nDan Forrest\n\n\nKapil Jain\n\n\nAlex Kamalov\n\n\n\n\nVersion 0.3.1 (2015-12-30)\n\n\nNew Features\n\n\n\n\nSupport for VirtualBox (\n#209\n)\n\n\nAdded Developer's Guide (\n#226\n)\n\n\n\n\nEnhancements\n\n\n\n\nMount/Unmount Accounting (\n#212\n)\n\n\nSupport for Sub-Path Volume Mounts / Permissions (\n#215\n)\n\n\n\n\nMilestone Issues\n\n\nThis release also includes many other small enhancements and bug fixes. For a\ncomplete list click \nhere\n.\n\n\nDownloads\n\n\nClick \nhere\n for the 0.3.1\nbinaries.\n\n\nVersion 0.3.0 (2015-12-08)\n\n\nNew Features\n\n\n\n\nPre-Emption support (\n#190\n)\n\n\nSupport for VMAX (\n#197\n)\n\n\nSupport for Isilon (\n#198\n)\n\n\nSupport for Google Compute Engine (GCE) (\n#194\n)\n\n\n\n\nEnhancements\n\n\n\n\nAdded driver example configurations (\n#201\n)\n\n\nNew configuration file format (\n#188\n)\n\n\n\n\nTweaks\n\n\n\n\nChopped flags \n--rexrayLogLevel\n becomes \nlogLevel\n (\n#196\n)\n\n\n\n\nPre-Emption Support\n\n\nPre-Emption is an important feature when using persistent volumes and container\nschedulers.  Without pre-emption, the default behavior of the storage drivers is\nto deny the attaching operation if the volume is already mounted elsewhere.\n\nIf it is desired that a host should be able to pre-empt from other hosts, then\nthis feature can be used to enable any host to pre-empt from another.\n\n\nMilestone Issues\n\n\nThis release also includes many other small enhancements and bug fixes. For a\ncomplete list click \nhere\n.\n\n\nDownloads\n\n\nClick \nhere\n for the 0.3.0\nbinaries.\n\n\nVersion 0.2.1 (2015-10-27)\n\n\nREX-Ray release 0.2.1 includes OpenStack support, vastly improved documentation,\nand continued foundation changes for future features.\n\n\nNew Features\n\n\n\n\nSupport for OpenStack (\n#111\n)\n\n\nCreate volume from volume using existing settings (\n#129\n)\n\n\n\n\nEnhancements\n\n\n\n\nA+ \nGoReport Card\n\n\nA+ \nCode Coverage\n\n\nGoDoc Support\n\n\nAbility to load REX-Ray as an independent storage platform (\n#127\n)\n\n\nNew documentation at http://rexray.readthedocs.org (\n#145\n)\n\n\nMore foundation updates\n\n\n\n\nTweaks\n\n\n\n\nCommand aliases for \nget\n and \ndelete\n - \nls\n and \nrm\n (\n#107\n)\n\n\n\n\nVersion 0.2.0 (2015-09-30)\n\n\nInstallation, SysV, SystemD Support\n\n\nREX-Ray now includes built-in support for installing itself as a service on\nLinux distributions that support either SystemV or SystemD initialization\nsystems. This feature has been tested successfully on both CentOS 7 Minimal\n(SystemD) and Ubuntu 14.04 Server (SystemV) distributions.\n\n\nTo install REX-Ray on a supported Linux distribution, all that is required\nnow is to download the binary and execute:\n\n\nsudo ./rexray service install\n\n\n\nWhat does that do? In short the above command will determine if the Linux\ndistribution uses systemctl, update-rc.d, or chkconfig to manage system\nservices. After that the following steps occur:\n\n\n\n\nThe path /opt/rexray is created and chowned to root:root with permissions\n set to 0755.\n\n\nThe binary is copied to /opt/rexray/rexray and chowned to root:root with\n permissions set to 4755. This is important, because this means that any\n non-privileged user can execute the rexray binary as root without requiring\n sudo privileges. For more information on this feature, please read about the\n \nLinux kernel's super-user ID (SUID) bit\n.\n\n\n\n\nBecause the REX-Ray binary can now be executed with root privileges by\n non-root users, the binary can be used by non-root users to easily attach\n and mount external storage.\n\n\n\n\nThe directory /etc/rexray is created and chowned to root:root.\n\n\n\n\nThe next steps depends on the type of Linux distribution. However, it's\nimportant to know that the new version of the REX-Ray binary now supports\nmanaging its own PID (at \n/var/run/rexray.pid\n) when run as a service as well\nas supports the standard SysV control commands such as \nstart\n, \nstop\n,\n\nstatus\n, and \nrestart\n.\n\n\nFor SysV Linux distributions that use \nchkconfig\n or \nupdate-rc.d\n, a symlink\nof the REX-Ray binary is created in \n/etc/init.d\n and then either\n\nchkconfig rexray on\n or \nupdate-rc.d rexray defaults\n is executed.\n\n\nModern Linux distributions have moved to SystemD for controlling services.\nIf the \nsystemctl\n command is detected when installing REX-Ray then a unit\nfile is written to \n/etc/systemd/system/rexray.servic\ne with the following\ncontents:\n\n\n[Unit]\nDescription=rexray\nBefore=docker.service\n\n[Service]\nEnvironmentFile=/etc/rexray/rexray.env\nExecStart=/usr/local/bin/rexray start -f\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\n\n[Install]\nWantedBy=docker.service\n\n\n\nThe REX-Ray service is not started immediately upon installation. The install\ncommand completes by informing the users that they should visit the\n\nREX-Ray website\n for information on how to\nconfigure REX-Ray's storage drivers. The text to the users also explains how\nto start the REX-Ray service once it's configured using the service command\nparticular to the Linux distribution.\n\n\nSingle Service\n\n\nThis release also removes the need for REX-Ray to be configured as multiple\nservice instances in order to provide multiple end-points to such consumers\nsuch as \nDocker\n. REX-Ray's backend now supports an internal, modular design\nwhich enables it to host multiple module instances of any module, such as the\nDockerVolumeDriverModule. In fact, one of the default, included modules is...\n\n\nAdmin Module \n HTTP JSON API\n\n\nThe AdminModule enables an HTTP JSON API for managing REX-Ray's module system\nas well as provides a UI to view the currently running modules. Simply start\nthe REX-Ray server and then visit the URL http://localhost:7979 in your favorite\nbrowser to see what's loaded. Or you can access either of the currently\nsupported REST URLs:\n\n\nhttp://localhost:7979/r/module/types\n\n\n\nand\n\n\nhttp://localhost:7979/r/module/instances\n\n\n\nActually, those aren't the \nonly\n two URLs, but the others are for internal\nusers as of this point. However, the source \nis\n open, so... :)\n\n\nIf you want to know what modules are available by using the CLI, after starting\nthe REX-Ray service simply type:\n\n\n[0]akutz@poppy:rexray$ rexray service module types\n[\n  {\n    \"id\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"addresses\": [\n      \"unix:///run/docker/plugins/rexray.sock\",\n      \"tcp://:7980\"\n    ]\n  },\n  {\n    \"id\": 1,\n    \"name\": \"AdminModule\",\n    \"addresses\": [\n      \"tcp://:7979\"\n    ]\n  }\n]\n[0]akutz@poppy:rexray$\n\n\n\nTo get a list of the \nrunning\n modules you would type:\n\n\n[0]akutz@poppy:rexray$ rexray service module instance get\n[\n  {\n    \"id\": 1,\n    \"typeId\": 1,\n    \"name\": \"AdminModule\",\n    \"address\": \"tcp://:7979\",\n    \"description\": \"The REX-Ray admin module\",\n    \"started\": true\n  },\n  {\n    \"id\": 2,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"unix:///run/docker/plugins/rexray.sock\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  },\n  {\n    \"id\": 3,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"tcp://:7980\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  }\n]\n[0]akutz@poppy:rexray$\n\n\n\nHmmm, you know, the REX-Ray CLI looks a little different in the above examples,\ndoesn't it? About that...\n\n\nCommand Line Interface\n\n\nThe CLI has also been enhanced to present a more simplified view up front to\nusers. The commands are now categorized into logical groups:\n\n\n[0]akutz@pax:~$ rexray\nREX-Ray:\n  A guest-based storage introspection tool that enables local\n  visibility and management from cloud and storage platforms.\n\nUsage:\n  rexray [flags]\n  rexray [command]\n\nAvailable Commands:\n  volume      The volume manager\n  snapshot    The snapshot manager\n  device      The device manager\n  adapter     The adapter manager\n  service     The service controller\n  version     Print the version\n  help        Help about any command\n\nGlobal Flags:\n  -c, --config=\"/Users/akutz/.rexray/config.yaml\": The REX-Ray configuration file\n  -?, --help[=false]: Help for rexray\n  -h, --host=\"tcp://:7979\": The REX-Ray service address\n  -l, --logLevel=\"info\": The log level (panic, fatal, error, warn, info, debug)\n  -v, --verbose[=false]: Print verbose help information\n\nUse \"rexray [command] --help\" for more information about a command.\n\n\n\nTravis-CI Support\n\n\nREX-Ray now supports Travis-CI builds either from the primary REX-Ray repository\nor via a fork. All builds should be executed through the Makefile, which is a\nTravis-CI default. For the Travis-CI settings please be sure to set the\nenvironment variable \nGO15VENDOREXPERIMENT\n to \n1\n.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#upgrading", 
            "text": "To upgrade REX-Ray to the latest version, use  curl install :  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh  Use  rexray version  to determine the currently installed version of REX-Ray:  $ rexray version\nREX-Ray\n-------\nBinary: /Users/akutz/Projects/go/bin/rexray\nSemVer: 0.4.0\nOsArch: Linux-x86_64\nBranch: v0.4.0\nCommit: c83f0237e60792cfe89c4255d7149b5670965539\nFormed: Mon, 20 Jun 2016 20:56:48 CDT\n\nlibStorage\n----------\nSemVer: 0.1.3\nOsArch: Linux-x86_64\nBranch: v0.1.3\nCommit: 182a626937677a081b89651598ee2eac839308e7\nFormed: Wed, 15 Jun 2016 16:27:36 CDT", 
            "title": "Upgrading"
        }, 
        {
            "location": "/about/release-notes/#version-081-20170224", 
            "text": "This is a minor release that reintroduces support for Go1.6 via\nlibStorage 0.5.1.", 
            "title": "Version 0.8.1 (2017/02/24)"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes", 
            "text": "Go1.6 support ( #444 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-080-20170224", 
            "text": "This is one of the largest releases in a while, including support for five new\nstorage platforms!", 
            "title": "Version 0.8.0 (2017/02/24)"
        }, 
        {
            "location": "/about/release-notes/#new-features", 
            "text": "Amazon Simple Storage Service FUSE (S3FS) support ( #397 ,  #409 )  Google Compute Engine Persistent Disk (GCEPD) support ( #394 ,  #416 )  DigitalOcean support ( #392 )  Microsoft Azure unmanaged disk support ( #421 )  FittedCloud support ( #408 )  Docker Volume Plug-in for EBS ( #720 )  Docker Volume Plug-in for EFS ( #729 )  Docker Volume Plug-in for Isilon ( #727 )  Docker Volume Plug-in for S3FS ( #724 )  Docker Volume Plug-in for ScaleIO ( #725 )  REX-Ray on Alpine Linux support ( #724 )  Storage-platform specific mount/unmount support ( #399 )  The ScaleIO tool  drv_cfg  is now an optional client-side dependency instead of required ( #414 )  Multi-cluster support for ScaleIO ( #420 )  Forced volume remove support ( #717 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_1", 
            "text": "Preemption fix ( #413 )  Ceph RBD monitored IP fix ( #412 ,  #424 )  Ceph RBD dashes in names fix ( #425 )  Fix for  lsx-OS wait  argument count ( #401 )  Build fixes ( #403 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you", 
            "text": "Name  Blame      Chris Duchesne  Chris is my partner in crime when it comes to libStorage and REX-Ray. Without him I would have absolutely no one to take the fall for the heist I'm planning. So is Chris invaluable? Yeah, in that way, as the patsy who will do at least a dime while I'm on the beach sipping my drink, yeah, he's invaluable.    Travis Rhoden  Travis, or as I call him, T-Dawg, is essential to \"taking care of business.\" He comes to work to chew bubblegum and kick butt, and he leaves the gum at home!    Vladimir Vivien  A little known fact about Vladimir is that he's been seeded in the top 10 of the last US Opens, but has had to withdrawal at the last minute before each of those tournaments due to other responsibilities. What those are? Who can say? Are they contracts on people's lives? Perhaps. Are they appearances for Make a Wish? Probably. The only thing we know for sure is that when he is seen again, Vladimir seems rejuvenated and ready to conquer the tennis world yet again.    Steve Wong  I've known Steve for a very long time, and in that time I can say I've never once seen him in the same room as President Barack Obama. Now, does that mean that I can definitively state that Steve and President Obama are in fact the same person. No, of course not. There are obvious differences. The most glaring of course being that Steve wears glasses and President Obama does not. However, other than that the two men are nearly identical. I guess we'll never know if Steve Wong lives a double life as the 44th President of these United States, but I personally would like to think that yeah, he does.    Dan Norris  Dan \"The Man\" Norris is well known in the underground street-swimming circuit. Last year he tied Michael Phelps in the Santa Monica Sewer 120 meter medley. He would have won if not for stopping to create the DigitalOcean driver for libStorage.    Alexey Morlang  As a third-chair oboe player in the Moscow orchestra it is surprising that Alexey still finds time to contribute to the project, but coming from a long line of oboligarchs (oboe playing oligarchs), it's just in his nature. As is creating storage drivers. That, and, well, playing the oboe.    Andrey Pavlov  There is no Andrey. You have not met him. He does not exist. Don't look behind you. He is not there. He is writing storage drivers. Then just like that, he's vanished.    Lax Kota  Lax is a rock star in the Slack channel, helping others by answering their questions before the project's developers can take a stab. We do not want to upset him. It's rumored he beats those who upset him in order to provide inspiration for his true passion -- corporal poetry. Every punch thrown is another verse towards his masterpiece.    Jack Huang  Jack is not his job. Jack is not the amount of money he has in the bank. Jack is not the car he drives. Jack is not the clothes he wears. Jack is a supernova, accelerating at the speed of light beyond the bounds of quantifiable space and time. Jack is not the stuff above. Jack is not the stuff below. Jack is not the stuff in between. Jack is not the empty void. Jack. just. is.", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-070-20170123", 
            "text": "This feature release includes support for libStorage 0.4.0 and the Ceph RBD\nstorage platform.", 
            "title": "Version 0.7.0 (2017/01/23)"
        }, 
        {
            "location": "/about/release-notes/#enhancements", 
            "text": "libStorage 0.4.0  Ceph/RBD storage platform ( #347 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_2", 
            "text": "Prevent unnecessary removal of directory by FlexRex ( #699 )  Update  volume attach  to check for  --force  flag ( #696 )  Fix installer to correctly parse new Bintray HTML ( #687 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-064-20170105", 
            "text": "This release includes the new script manager and FlexVol REX-Ray plug-in.", 
            "title": "Version 0.6.4 (2017/01/05)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_1", 
            "text": "libStorage 0.3.8  Script manager ( #669 )  FlexVol plug-in for Kubernetes ( #641 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_3", 
            "text": "Panic on  $ rexray-client volume mount  ( #673 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-063-20161207", 
            "text": "This release includes the ability to specify a custom encryption key when\ncreating volumes and makes the  volume attach  command idempotent.", 
            "title": "Version 0.6.3 (2016/12/07)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_2", 
            "text": "libStorage 0.3.5  Support for creating encrypted volumes ( #649 ,  #652 )  Idempotent volume attach command ( #651 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_4", 
            "text": "Fix volume status for detach op ( #654 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-062-20161205", 
            "text": "While a patch release, this new version includes some much-requested features\nand updates.", 
            "title": "Version 0.6.2 (2016/12/05)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_3", 
            "text": "libStorage 0.3.4  Auto-detect running service ( #642 )  Prettier error messages ( #645 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_5", 
            "text": "Graceful exit with SystemD ( #644 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-061-20161201", 
            "text": "This release includes some minor fixes as well as a new and improved version of\nthe  volume ls  command.", 
            "title": "Version 0.6.1 (2016/12/01)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_4", 
            "text": "libStorage 0.3.3  Enhanced  volume ls  command ( #634 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_6", 
            "text": "EFS Mounting Issues ( #609 )  VirtualBox Attach Issues ( #610 )  Installer upgrade fix ( #637 )  Build deployment fix ( #638 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-060-20161020", 
            "text": "This release reintroduces the Elastic Block Storage (EBS) driver, formerly known\nas the EC2 driver. All vestigial EC2 configuration properties are still\nsupported.", 
            "title": "Version 0.6.0 (2016/10/20)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_5", 
            "text": "libStorage 0.3.0 ( #docs )  Amazon Elastic Block Storage (EBS) Support ( #522 )  New CLI Output ( #579 ,  #603 ,  #606 )  Support for ScaleIO 2.0.1 ( #599 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_7", 
            "text": "Handle phantom mounts for EBS (formerly EC2) ( #410 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-051-20160914", 
            "text": "This is a minor release, but includes a few important patches.", 
            "title": "Version 0.5.1 (2016/09/14)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_6", 
            "text": "libStorage 0.2.1 ( #docs )  ScaleIO 2.0.0.2 Support ( #555 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_8", 
            "text": "EFS Volume / Tag Creation Bug ( #261 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-050-20160907", 
            "text": "Beginning with this release, REX-Ray's versions will increment the MINOR\ncomponent with the introduction of a new storage driver via libStorage in\nconcert with the  guidelines  set forth by semantic\nversioning.", 
            "title": "Version 0.5.0 (2016/09/07)"
        }, 
        {
            "location": "/about/release-notes/#new-features_1", 
            "text": "Amazon Elastic File System (EFS) Support ( #525 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_7", 
            "text": "Support for Go 1.7 ( #541 )  Enhanced Isilon Support ( #520 ,  #521 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#thank-you_1", 
            "text": "Name  Blame      Chris Duchesne  Chris not only took on the role of project manager for libStorage and REX-Ray, he still provides ongoing test plan execution and release validation. Thank you Chris!    Kenny Cole  Kenny's tireless effort to support users and triage submitted issues is such a cornerstone to libStorage and REX-Ray that I'm not sure what this project would do without him!    Martin Hrabovcin  Martin, along with Kasisnu, definitely win the \"Community Members of the Month\" award! Their hard work and dedication resulted in the introduction of the Amazon EFS storage driver. Thank you Martin   Kasisnu!    Kasisnu Singh  Have I mentioned we have the best community around? Seriously, thank you again Kasisnu! Your work, along with Martin's, is a milestone in the growth of libStorage and REX-Ray.", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-042-20160712", 
            "text": "This minor update represents a  major  performance boost for REX-Ray.\nOperations that use to take up to minutes now take seconds or less. The memory\nfootprint has been reduced from the magnitude of phenomenal cosmic powers to\nthe size of an itty bitty living space!", 
            "title": "Version 0.4.2 (2016/07/12)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_8", 
            "text": "libStorage 0.1.5 ( #TBA )  Improved volume path caching ( #500 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#version-041-20160708", 
            "text": "Although a minor release, 0.4.1 provides some meaningful and useful enhancements\nand fixes, further strengthening the foundation of the REX-Ray platform.", 
            "title": "Version 0.4.1 (2016/07/08)"
        }, 
        {
            "location": "/about/release-notes/#enhancements_9", 
            "text": "Improved build process ( #474 ,  #492 )  libStorage  0.1.4 ( #493 )  Removed Docker spec file ( #486 )  Improved REX-Ray 0.3.3 Config Backwards Compatibility ( #481 )  Improved install script ( #439 ,  #495 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_9", 
            "text": "Fixed input validation bug when creating volume sans name ( #478 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#version-040-20160620", 
            "text": "REX-Ray 0.4.0 introduces centralized configuration and control along with\na new client/server architecture -- features made possible by libStorage . Users are no longer\nrequired to configure storage drivers or store privileged information on all\nsystems running the REX-Ray client. The new client delegates storage-platform\nrelated operations to a remote, libStorage-compatible server such as REX-Ray\nor  Poly .  Please note that the initial release of REX-Ray 0.4 includes support for only\nthe following storage platforms:   ScaleIO  VirtualBox   Support for the full compliment of drivers present in earlier versions of\nREX-Ray will be reintroduced over the course of several, incremental updates,\nbeginning with 0.4.1.", 
            "title": "Version 0.4.0 (2016/06/20)"
        }, 
        {
            "location": "/about/release-notes/#new-features_2", 
            "text": "Distributed architecture ( #399 ,  #401 ,  #411 ,  #417 ,  #418 ,  #419 ,  #420 ,  #423 )  Volume locking mechanism ( #171 )  Volume creation with initial data ( #169 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_10", 
            "text": "Improved storage driver logging ( #396 )  Docker mount path ( #403 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_10", 
            "text": "Fixed issue with install script ( #409 )  Fixed volume ls filter ( #400 )  Fixed panic during access attempt of offline REX-Ray daemon ( #148 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you_2", 
            "text": "Yes, the author is so lazy as to blatantly copy \nthis section. So sue me :)     Name  Blame      Clint Kitson  His vision come to fruition. That's  his  vision, thus please assign  all  bugs to Clint :)    Vladimir Vivien  A nascent player, Vlad had to hit the ground running and has been a key contributor    Kenny Coleman  While some come close, none are comparable to Kenny's handlebar    Jonas Rosland  Always good for a sanity check and keeping things on the straight and narrow    Steph Carlson  Steph keeps the convention train chugging along...    Amanda Katona  And Amanda is the one keeping the locomotive from going off the rails    Drew Smith  Drew is always ready to lend a hand, no matter the problem    Chris Duchesne  His short time with the team is in complete opposition to the value he has added to this project    David vonThenen  David has been a go-to guy for debugging the most difficult of issues    Steve Wong  Steve stays on top of the things and keeps use cases in sync with industry needs    Travis Rhoden  Another keen mind, Travis is also a great font of technical know-how    Peter Blum  Absent Peter, the EMC World demo would not have been ready    Megan Hyland  And absent Megan, Peter's work would only have taken things halfway there    Eugene Chupriyanov  For helping with the EC2 planning    Matt Farina  Without Glide, it all comes crashing down    Josh Bernstein  The shadowy figure behind the curtain...", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-033-20160421", 
            "text": "", 
            "title": "Version 0.3.3 (2016/04/21)"
        }, 
        {
            "location": "/about/release-notes/#new-features_3", 
            "text": "ScaleIO v2 support ( #355 )  EC2 Tags added to Volumes   Snapshots ( #314 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_11", 
            "text": "Use of official Amazon EC2 SDK ( #359 )  Added a disable feature for create/remove volume ( #366 )  Added ScaleIO troubleshooting information ( #367 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_11", 
            "text": "Fixes URLs for documentation when viewed via Github ( #337 )  Fixes logging bug on Ubuntu 14.04 ( #377 )  Fixes module start timeout error ( #376 )  Fixes ScaleIO authentication loop bug ( #375 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#thank-you_3", 
            "text": "Philipp Franke  Eugene Chupriyanov  Peter Blum  Megan Hyland", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-032-2016-03-04", 
            "text": "", 
            "title": "Version 0.3.2 (2016-03-04)"
        }, 
        {
            "location": "/about/release-notes/#new-features_4", 
            "text": "Support for Docker 1.10 and Volume Plugin Interface 1.2 ( #273 )  Stale PID File Prevents Service Start ( #258 )  Module/Personality Support ( #275 )  Isilon Preemption ( #231 )  Isilon Snapshots ( #260 )  boot2Docker Support ( #263 )  ScaleIO Dynamic Storage Pool Support ( #267 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_12", 
            "text": "Improved installation documentation ( #331 )  ScaleIO volume name limitation ( #304 )  Docker cache volumes for path operations ( #306 )  Config file validation ( #312 )  Better logging ( #296 )  Documentation Updates ( #285 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes_12", 
            "text": "Fixes issue with daemon process getting cleaned as part of SystemD Cgroup ( #327 )  Fixes regression in 0.3.2 RC3/RC4 resulting in no log file ( #319 )  Fixes no volumes returned on empty list ( #322 )  Fixes \"Unsupported FS\" when mounting/unmounting with EC2 ( #321 )  ScaleIO re-authentication issue ( #303 )  Docker XtremIO create volume issue ( #307 )  Service status is reported correctly ( #310 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#updates", 
            "text": "Go 1.6 ( #308 )", 
            "title": "Updates"
        }, 
        {
            "location": "/about/release-notes/#thank-you_4", 
            "text": "Dan Forrest  Kapil Jain  Alex Kamalov", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-031-2015-12-30", 
            "text": "", 
            "title": "Version 0.3.1 (2015-12-30)"
        }, 
        {
            "location": "/about/release-notes/#new-features_5", 
            "text": "Support for VirtualBox ( #209 )  Added Developer's Guide ( #226 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_13", 
            "text": "Mount/Unmount Accounting ( #212 )  Support for Sub-Path Volume Mounts / Permissions ( #215 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#milestone-issues", 
            "text": "This release also includes many other small enhancements and bug fixes. For a\ncomplete list click  here .", 
            "title": "Milestone Issues"
        }, 
        {
            "location": "/about/release-notes/#downloads", 
            "text": "Click  here  for the 0.3.1\nbinaries.", 
            "title": "Downloads"
        }, 
        {
            "location": "/about/release-notes/#version-030-2015-12-08", 
            "text": "", 
            "title": "Version 0.3.0 (2015-12-08)"
        }, 
        {
            "location": "/about/release-notes/#new-features_6", 
            "text": "Pre-Emption support ( #190 )  Support for VMAX ( #197 )  Support for Isilon ( #198 )  Support for Google Compute Engine (GCE) ( #194 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_14", 
            "text": "Added driver example configurations ( #201 )  New configuration file format ( #188 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#tweaks", 
            "text": "Chopped flags  --rexrayLogLevel  becomes  logLevel  ( #196 )", 
            "title": "Tweaks"
        }, 
        {
            "location": "/about/release-notes/#pre-emption-support", 
            "text": "Pre-Emption is an important feature when using persistent volumes and container\nschedulers.  Without pre-emption, the default behavior of the storage drivers is\nto deny the attaching operation if the volume is already mounted elsewhere. \nIf it is desired that a host should be able to pre-empt from other hosts, then\nthis feature can be used to enable any host to pre-empt from another.", 
            "title": "Pre-Emption Support"
        }, 
        {
            "location": "/about/release-notes/#milestone-issues_1", 
            "text": "This release also includes many other small enhancements and bug fixes. For a\ncomplete list click  here .", 
            "title": "Milestone Issues"
        }, 
        {
            "location": "/about/release-notes/#downloads_1", 
            "text": "Click  here  for the 0.3.0\nbinaries.", 
            "title": "Downloads"
        }, 
        {
            "location": "/about/release-notes/#version-021-2015-10-27", 
            "text": "REX-Ray release 0.2.1 includes OpenStack support, vastly improved documentation,\nand continued foundation changes for future features.", 
            "title": "Version 0.2.1 (2015-10-27)"
        }, 
        {
            "location": "/about/release-notes/#new-features_7", 
            "text": "Support for OpenStack ( #111 )  Create volume from volume using existing settings ( #129 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_15", 
            "text": "A+  GoReport Card  A+  Code Coverage  GoDoc Support  Ability to load REX-Ray as an independent storage platform ( #127 )  New documentation at http://rexray.readthedocs.org ( #145 )  More foundation updates", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#tweaks_1", 
            "text": "Command aliases for  get  and  delete  -  ls  and  rm  ( #107 )", 
            "title": "Tweaks"
        }, 
        {
            "location": "/about/release-notes/#version-020-2015-09-30", 
            "text": "", 
            "title": "Version 0.2.0 (2015-09-30)"
        }, 
        {
            "location": "/about/release-notes/#installation-sysv-systemd-support", 
            "text": "REX-Ray now includes built-in support for installing itself as a service on\nLinux distributions that support either SystemV or SystemD initialization\nsystems. This feature has been tested successfully on both CentOS 7 Minimal\n(SystemD) and Ubuntu 14.04 Server (SystemV) distributions.  To install REX-Ray on a supported Linux distribution, all that is required\nnow is to download the binary and execute:  sudo ./rexray service install  What does that do? In short the above command will determine if the Linux\ndistribution uses systemctl, update-rc.d, or chkconfig to manage system\nservices. After that the following steps occur:   The path /opt/rexray is created and chowned to root:root with permissions\n set to 0755.  The binary is copied to /opt/rexray/rexray and chowned to root:root with\n permissions set to 4755. This is important, because this means that any\n non-privileged user can execute the rexray binary as root without requiring\n sudo privileges. For more information on this feature, please read about the\n  Linux kernel's super-user ID (SUID) bit .   Because the REX-Ray binary can now be executed with root privileges by\n non-root users, the binary can be used by non-root users to easily attach\n and mount external storage.   The directory /etc/rexray is created and chowned to root:root.   The next steps depends on the type of Linux distribution. However, it's\nimportant to know that the new version of the REX-Ray binary now supports\nmanaging its own PID (at  /var/run/rexray.pid ) when run as a service as well\nas supports the standard SysV control commands such as  start ,  stop , status , and  restart .  For SysV Linux distributions that use  chkconfig  or  update-rc.d , a symlink\nof the REX-Ray binary is created in  /etc/init.d  and then either chkconfig rexray on  or  update-rc.d rexray defaults  is executed.  Modern Linux distributions have moved to SystemD for controlling services.\nIf the  systemctl  command is detected when installing REX-Ray then a unit\nfile is written to  /etc/systemd/system/rexray.servic e with the following\ncontents:  [Unit]\nDescription=rexray\nBefore=docker.service\n\n[Service]\nEnvironmentFile=/etc/rexray/rexray.env\nExecStart=/usr/local/bin/rexray start -f\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\n\n[Install]\nWantedBy=docker.service  The REX-Ray service is not started immediately upon installation. The install\ncommand completes by informing the users that they should visit the REX-Ray website  for information on how to\nconfigure REX-Ray's storage drivers. The text to the users also explains how\nto start the REX-Ray service once it's configured using the service command\nparticular to the Linux distribution.", 
            "title": "Installation, SysV, SystemD Support"
        }, 
        {
            "location": "/about/release-notes/#single-service", 
            "text": "This release also removes the need for REX-Ray to be configured as multiple\nservice instances in order to provide multiple end-points to such consumers\nsuch as  Docker . REX-Ray's backend now supports an internal, modular design\nwhich enables it to host multiple module instances of any module, such as the\nDockerVolumeDriverModule. In fact, one of the default, included modules is...", 
            "title": "Single Service"
        }, 
        {
            "location": "/about/release-notes/#admin-module-http-json-api", 
            "text": "The AdminModule enables an HTTP JSON API for managing REX-Ray's module system\nas well as provides a UI to view the currently running modules. Simply start\nthe REX-Ray server and then visit the URL http://localhost:7979 in your favorite\nbrowser to see what's loaded. Or you can access either of the currently\nsupported REST URLs:  http://localhost:7979/r/module/types  and  http://localhost:7979/r/module/instances  Actually, those aren't the  only  two URLs, but the others are for internal\nusers as of this point. However, the source  is  open, so... :)  If you want to know what modules are available by using the CLI, after starting\nthe REX-Ray service simply type:  [0]akutz@poppy:rexray$ rexray service module types\n[\n  {\n    \"id\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"addresses\": [\n      \"unix:///run/docker/plugins/rexray.sock\",\n      \"tcp://:7980\"\n    ]\n  },\n  {\n    \"id\": 1,\n    \"name\": \"AdminModule\",\n    \"addresses\": [\n      \"tcp://:7979\"\n    ]\n  }\n]\n[0]akutz@poppy:rexray$  To get a list of the  running  modules you would type:  [0]akutz@poppy:rexray$ rexray service module instance get\n[\n  {\n    \"id\": 1,\n    \"typeId\": 1,\n    \"name\": \"AdminModule\",\n    \"address\": \"tcp://:7979\",\n    \"description\": \"The REX-Ray admin module\",\n    \"started\": true\n  },\n  {\n    \"id\": 2,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"unix:///run/docker/plugins/rexray.sock\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  },\n  {\n    \"id\": 3,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"tcp://:7980\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  }\n]\n[0]akutz@poppy:rexray$  Hmmm, you know, the REX-Ray CLI looks a little different in the above examples,\ndoesn't it? About that...", 
            "title": "Admin Module &amp; HTTP JSON API"
        }, 
        {
            "location": "/about/release-notes/#command-line-interface", 
            "text": "The CLI has also been enhanced to present a more simplified view up front to\nusers. The commands are now categorized into logical groups:  [0]akutz@pax:~$ rexray\nREX-Ray:\n  A guest-based storage introspection tool that enables local\n  visibility and management from cloud and storage platforms.\n\nUsage:\n  rexray [flags]\n  rexray [command]\n\nAvailable Commands:\n  volume      The volume manager\n  snapshot    The snapshot manager\n  device      The device manager\n  adapter     The adapter manager\n  service     The service controller\n  version     Print the version\n  help        Help about any command\n\nGlobal Flags:\n  -c, --config=\"/Users/akutz/.rexray/config.yaml\": The REX-Ray configuration file\n  -?, --help[=false]: Help for rexray\n  -h, --host=\"tcp://:7979\": The REX-Ray service address\n  -l, --logLevel=\"info\": The log level (panic, fatal, error, warn, info, debug)\n  -v, --verbose[=false]: Print verbose help information\n\nUse \"rexray [command] --help\" for more information about a command.", 
            "title": "Command Line Interface"
        }, 
        {
            "location": "/about/release-notes/#travis-ci-support", 
            "text": "REX-Ray now supports Travis-CI builds either from the primary REX-Ray repository\nor via a fork. All builds should be executed through the Makefile, which is a\nTravis-CI default. For the Travis-CI settings please be sure to set the\nenvironment variable  GO15VENDOREXPERIMENT  to  1 .", 
            "title": "Travis-CI Support"
        }
    ]
}