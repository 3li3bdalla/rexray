{
    "docs": [
        {
            "location": "/", 
            "text": "REX-Ray\n\n\nOpenly serious about storage\n\n\n\n\nREX-Ray\n delivers persistent storage access for container runtimes, such as\nDocker and Mesos, and provides an easy interface for enabling advanced storage\nfunctionality across common storage, virtualization and cloud platforms. For\nexample, here's how to list storage for a guest hosted on Amazon Web Services\n(AWS) with \nREX-Ray\n:\n\n\n$ export REXRAY_STORAGEDRIVERS=ec2\n$ export AWS_ACCESSKEY=access_key\n$ export AWS_SECRETKEY=secret_key\n$ rexray volume get\n\n- providername: ec2\n  instanceid: i-695bb6ab\n  volumeid: vol-dedbadc3\n  devicename: /dev/sda1\n  region: us-west-1\n  status: attached\n\n\n\n\nOverview\n\n\nREX-Ray\n is an abstraction layer between storage endpoints and container\nplatforms. The administration and orchestration of various storage platforms\ncan all be performed using the same set of commands.\n\n\nStorage Provider Support\n\n\nThe following storage providers and platforms are supported by \nREX-Ray\n.\n\n\n\n\n\n\n\n\nProvider\n\n\nStorage Platform(s)\n\n\n\n\n\n\n\n\n\n\nAmazon EC2\n\n\nEBS\n\n\n\n\n\n\nGoogle Compute Engine\n\n\nDisk\n\n\n\n\n\n\nOpen Stack\n\n\nCinder\n\n\n\n\n\n\nRackspace\n\n\nCinder\n\n\n\n\n\n\nEMC\n\n\nScaleIO\n, \nXtremIO\n, \nVMAX\n, \nIsilon\n\n\n\n\n\n\nVirtual Box\n\n\nVirtual Media\n\n\n\n\n\n\n\n\nOperating System Support\n\n\nThe following operating systems (OS) are supported by \nREX-Ray\n:\n\n\n\n\n\n\n\n\nOS\n\n\nCommand Line\n\n\nService\n\n\n\n\n\n\n\n\n\n\nUbuntu 12+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nDebian 6+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nRedHat\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nCentOS 6+\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nCoreOS\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nTinyLinux (boot2docker)\n\n\nYes\n\n\nYes\n\n\n\n\n\n\nOS X Yosemite+\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nWindows\n\n\nNo\n\n\nNo\n\n\n\n\n\n\n\n\nContainer Platform Support\n\n\nREX-Ray\n currently supports the following container platforms:\n\n\n\n\n\n\n\n\nPlatform\n\n\nUse\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nVolume Driver Plugin\n\n\n\n\n\n\nMesos\n\n\nVolume Driver Isolator module\n\n\n\n\n\n\nMesos + Docker\n\n\nVolume Driver Plugin\n\n\n\n\n\n\n\n\nGetting Started\n\n\nThis section will help you get REX-Ray up and running quickly. For more advanced\nconfigurations including\n\ncore properties\n and additional\nstorage providers use the \nUser Guide\n menu in the tool-bar.\n\n\nInstalling REX-Ray\n\n\nThe following command will download the most recent, stable build of REX-Ray\nand install it to \n/usr/bin/rexray\n on Linux systems. REX-Ray will be\nregistered as either a SystemD or SystemV service depending upon the OS.\n\n\n$ curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -\n\n\n\n\nConfiguring REX-Ray\n\n\nCreate a configuration file on the host at \n/etc/rexray\n in YAML format called\n\nconfig.yml\n (this file can be created with \nvi\n or transferred over via \nscp\n\n  or \nftp\n). Here is a simple example for using Amazon EC2:\n\n\nrexray:\n  storageDrivers:\n  - ec2\naws:\n  accessKey: MyAccessKey\n  secretKey: MySecretKey\n\n\n\n\nFrom here, REX-Ray can now be used as a command line tool. View the commands\navailable:\n\n\n$ rexray --help\n\n\n\n\nTo verify the configuration file is being accessed and the AWS Keys are being\nused, list volumes that can be accessed:\n\n\n$ rexray volume ls\n\n\n\n\nIf there is an error, use the \n-l debug\n flag and consult debugging instructions\n located under \nGetting Help\n. If nothing is returned using \nls\n\n, then everything is functioning as expected\n\n\nStart REX-Ray as a Service\n\n\nContainer platforms rely on REX-Ray to be running as a service to function\nproperly. For instance, Docker communicates to the REX-Ray Volume Driver through\na Unix socket file.\n\n\n$ rexray start\n\n\n\n\nREX-Ray with Docker\n\n\nDocker 1.10+ is recommended to use REX-Ray as a\n\nDocker Volume Driver Plugin\n.\n\n\nThe following example uses two Amazon EC2 Virtual Machines, \nEC2a\n and \nEC2b\n,\nthat reside within the same Availability Zone.\n\n\nFrom \nEC2a\n, create a new volume called \nhellopersistence\n. After the new volume\n is created, mount the volume to the host and container using the\n \n--volume-driver\n and \n-v\n flag in the \ndocker run\n command. Create a new file\ncalled \nmyfile\n using \ndocker exec\n that will be persisted throughout the\nexample. Lastly, stop and remove the container so it no longer exists:\n\n\n$ docker volume create --driver rexray --opt size=10 --name hellopersistence\n$ docker run -tid --volume-driver=rexray -v hellopersistence:/mystore \\\n  --name temp01 busybox\n$ docker exec temp01 touch /mystore/myfile\n$ docker exec temp01 ls /mystore\n$ docker rm -f temp01\n\n\n\n\nFrom \nEC2b\n, create a new container that mounts the pre-existing volume and\nverify \nmyfile\n that was originally created from host \nEC2a\n has persisted.\n\n\n$ docker run -tid --volume-driver=rexray -v hellopersistence:/mystore \\\n  --name temp01 busybox\n$ docker exec temp01 ls /mystore\n\n\n\n\nCongratulations, you have used \nREX-Ray\n to provide persistence for stateless\ncontainers!\n\n\nExamples using MongoDB, Postgres, and more with persistent storage can be found\nat \nApplication Examples\n.\n\n\nGetting Help\n\n\nHaving issues? No worries, let's figure it out together.\n\n\nDebug\n\n\nThe \ndebug\n flag can be appended to any command in order to get verbose output:\n\n\n$ rexray volume -l debug\n\n\n\n\nThe above command will list all of the volumes visible to \nREX-Ray\n with debug\nlogging enabled.\n\n\nGitHub and Slack\n\n\nAnd if you need a little extra help, please don't hesitate to use\n\nGitHub issues\n or join the active\nconversation on the\n\nEMC {code} Community Slack Team\n in\nthe #project-rexray channel", 
            "title": "Home"
        }, 
        {
            "location": "/#rex-ray", 
            "text": "Openly serious about storage   REX-Ray  delivers persistent storage access for container runtimes, such as\nDocker and Mesos, and provides an easy interface for enabling advanced storage\nfunctionality across common storage, virtualization and cloud platforms. For\nexample, here's how to list storage for a guest hosted on Amazon Web Services\n(AWS) with  REX-Ray :  $ export REXRAY_STORAGEDRIVERS=ec2\n$ export AWS_ACCESSKEY=access_key\n$ export AWS_SECRETKEY=secret_key\n$ rexray volume get\n\n- providername: ec2\n  instanceid: i-695bb6ab\n  volumeid: vol-dedbadc3\n  devicename: /dev/sda1\n  region: us-west-1\n  status: attached", 
            "title": "REX-Ray"
        }, 
        {
            "location": "/#overview", 
            "text": "REX-Ray  is an abstraction layer between storage endpoints and container\nplatforms. The administration and orchestration of various storage platforms\ncan all be performed using the same set of commands.", 
            "title": "Overview"
        }, 
        {
            "location": "/#storage-provider-support", 
            "text": "The following storage providers and platforms are supported by  REX-Ray .     Provider  Storage Platform(s)      Amazon EC2  EBS    Google Compute Engine  Disk    Open Stack  Cinder    Rackspace  Cinder    EMC  ScaleIO ,  XtremIO ,  VMAX ,  Isilon    Virtual Box  Virtual Media", 
            "title": "Storage Provider Support"
        }, 
        {
            "location": "/#operating-system-support", 
            "text": "The following operating systems (OS) are supported by  REX-Ray :     OS  Command Line  Service      Ubuntu 12+  Yes  Yes    Debian 6+  Yes  Yes    RedHat  Yes  Yes    CentOS 6+  Yes  Yes    CoreOS  Yes  Yes    TinyLinux (boot2docker)  Yes  Yes    OS X Yosemite+  Yes  No    Windows  No  No", 
            "title": "Operating System Support"
        }, 
        {
            "location": "/#container-platform-support", 
            "text": "REX-Ray  currently supports the following container platforms:     Platform  Use      Docker  Volume Driver Plugin    Mesos  Volume Driver Isolator module    Mesos + Docker  Volume Driver Plugin", 
            "title": "Container Platform Support"
        }, 
        {
            "location": "/#getting-started", 
            "text": "This section will help you get REX-Ray up and running quickly. For more advanced\nconfigurations including core properties  and additional\nstorage providers use the  User Guide  menu in the tool-bar.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#installing-rex-ray", 
            "text": "The following command will download the most recent, stable build of REX-Ray\nand install it to  /usr/bin/rexray  on Linux systems. REX-Ray will be\nregistered as either a SystemD or SystemV service depending upon the OS.  $ curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -", 
            "title": "Installing REX-Ray"
        }, 
        {
            "location": "/#configuring-rex-ray", 
            "text": "Create a configuration file on the host at  /etc/rexray  in YAML format called config.yml  (this file can be created with  vi  or transferred over via  scp \n  or  ftp ). Here is a simple example for using Amazon EC2:  rexray:\n  storageDrivers:\n  - ec2\naws:\n  accessKey: MyAccessKey\n  secretKey: MySecretKey  From here, REX-Ray can now be used as a command line tool. View the commands\navailable:  $ rexray --help  To verify the configuration file is being accessed and the AWS Keys are being\nused, list volumes that can be accessed:  $ rexray volume ls  If there is an error, use the  -l debug  flag and consult debugging instructions\n located under  Getting Help . If nothing is returned using  ls \n, then everything is functioning as expected", 
            "title": "Configuring REX-Ray"
        }, 
        {
            "location": "/#start-rex-ray-as-a-service", 
            "text": "Container platforms rely on REX-Ray to be running as a service to function\nproperly. For instance, Docker communicates to the REX-Ray Volume Driver through\na Unix socket file.  $ rexray start", 
            "title": "Start REX-Ray as a Service"
        }, 
        {
            "location": "/#rex-ray-with-docker", 
            "text": "Docker 1.10+ is recommended to use REX-Ray as a Docker Volume Driver Plugin .  The following example uses two Amazon EC2 Virtual Machines,  EC2a  and  EC2b ,\nthat reside within the same Availability Zone.  From  EC2a , create a new volume called  hellopersistence . After the new volume\n is created, mount the volume to the host and container using the\n  --volume-driver  and  -v  flag in the  docker run  command. Create a new file\ncalled  myfile  using  docker exec  that will be persisted throughout the\nexample. Lastly, stop and remove the container so it no longer exists:  $ docker volume create --driver rexray --opt size=10 --name hellopersistence\n$ docker run -tid --volume-driver=rexray -v hellopersistence:/mystore \\\n  --name temp01 busybox\n$ docker exec temp01 touch /mystore/myfile\n$ docker exec temp01 ls /mystore\n$ docker rm -f temp01  From  EC2b , create a new container that mounts the pre-existing volume and\nverify  myfile  that was originally created from host  EC2a  has persisted.  $ docker run -tid --volume-driver=rexray -v hellopersistence:/mystore \\\n  --name temp01 busybox\n$ docker exec temp01 ls /mystore  Congratulations, you have used  REX-Ray  to provide persistence for stateless\ncontainers!  Examples using MongoDB, Postgres, and more with persistent storage can be found\nat  Application Examples .", 
            "title": "REX-Ray with Docker"
        }, 
        {
            "location": "/#getting-help", 
            "text": "Having issues? No worries, let's figure it out together.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/#debug", 
            "text": "The  debug  flag can be appended to any command in order to get verbose output:  $ rexray volume -l debug  The above command will list all of the volumes visible to  REX-Ray  with debug\nlogging enabled.", 
            "title": "Debug"
        }, 
        {
            "location": "/#github-and-slack", 
            "text": "And if you need a little extra help, please don't hesitate to use GitHub issues  or join the active\nconversation on the EMC {code} Community Slack Team  in\nthe #project-rexray channel", 
            "title": "GitHub and Slack"
        }, 
        {
            "location": "/user-guide/installation/", 
            "text": "Installation\n\n\nGetting the bits, bit by bit\n\n\n\n\nOverview\n\n\nThere are several different methods available for installing \nREX-Ray\n. It\nis written in Go, so there are typically no dependencies that must be installed\nalongside its single binary file. The manual methods can be extremely simple\nthrough tools like \ncurl\n. You also have the opportunity to perform install\nsteps individually. Following the manual installs, \nconfiguration\n\nmust take place.\n\n\nWe also provide some great automation examples using tools like \nAnsible\n and\n\nPuppet\n. These approaches will perform the whole process including configuring\nthe REX-Ray for you.\n\n\nManual Installs\n\n\nManual installations are in contrast to batch, automated installations.\n\n\nMake sure that before installing REX-Ray that you have uninstalled any previous\nversions. A \nrexray uninstall\n can assist with this where appropriate.\n\n\nFollowing an installation and configuration, you can use REX-Ray interactively\nthrough commands like \nrexray volume\n. Noticeably different from this is having\nREX-Ray integrate with Container Engines such as Docker. This requires that\nyou run \nrexray start\n or relevant service start command like\n\nsystemctl start rexray\n.\n\n\nInstall via curl\n\n\nThe following command will download the most recent, stable build of \nREX-Ray\n\nand install it to \n/usr/bin/rexray\n or \n/opt/bin/rexray\n. On Linux systems\n\nREX-Ray\n will also be registered as either a SystemD or SystemV service.\n\n\nThere is an optional flag to choose which version to install. Notice how we\nspecify \nstable\n, see the additional version names below that are also valid.\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s stable\n\n\n\n\nInstall a pre-built binary\n\n\nThere are a handful of necessary manual steps to properly install REX-Ray\nfrom pre-built binaries.\n\n\n\n\n\n\nDownload the proper binary. There are also pre-built binaries available for\nthe various release types.\n\n\n\n\n\n\n\n\nVersion\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUnstable\n\n\nThe most up-to-date, bleeding-edge, and often unstable REX-Ray binaries.\n\n\n\n\n\n\nStaged\n\n\nThe most up-to-date, release candidate REX-Ray binaries.\n\n\n\n\n\n\nStable\n\n\nThe most up-to-date, stable REX-Ray binaries.\n\n\n\n\n\n\n\n\n\n\n\n\nUncompress and move the binary to the proper location. Preferably \n/usr/bin\n\nshould be where REX-Ray is moved, but this path is not required.\n\n\n\n\nInstall as a service with \nrexray install\n. This will register itself\nwith SystemD or SystemV for proper initialization.\n\n\n\n\nBuild and install from source\n\n\nREX-Ray\n is also fairly simple to build from source, especially if you have\n\nDocker\n installed:\n\n\nSRC=$(mktemp -d 2\n /dev/null || mktemp -d -t rexray 2\n /dev/null) \n cd $SRC \n docker run --rm -it -e GO15VENDOREXPERIMENT=1 -v $SRC:/usr/src/rexray -w /usr/src/rexray golang:1.5.1 bash -c \ngit clone https://github.com/emccode/rexray.git -b master . \n make build-all\u201d\n\n\n\n\nIf you'd prefer to not use \nDocker\n to build \nREX-Ray\n then all you need is Go 1.5:\n\n\n# clone the rexray repo\ngit clone https://github.com/emccode/rexray.git\n\n# change directories into the freshly-cloned repo\ncd rexray\n\n# build rexray\nmake build-all\n\n\n\n\nAfter either of the above methods for building \nREX-Ray\n there should be a \n.bin\n directory in the current directory, and inside \n.bin\n will be binaries for Linux-i386, Linux-x86-64,\nand Darwin-x86-64.\n\n\n[0]akutz@poppy:tmp.SJxsykQwp7$ ls .bin/*/rexray\n-rwxr-xr-x. 1 root 14M Sep 17 10:36 .bin/Darwin-x86_64/rexray*\n-rwxr-xr-x. 1 root 12M Sep 17 10:36 .bin/Linux-i386/rexray*\n-rwxr-xr-x. 1 root 14M Sep 17 10:36 .bin/Linux-x86_64/rexray*\n\n\n\n\nAutomated Installs\n\n\nBecause REX-Ray is simple to install using the \ncurl\n script, installation\nusing configuration management tools is relatively easy as well. However,\nthere are a few areas that may prove to be tricky, such as writing the\nconfiguration file.\n\n\nThis section provides examples of automated installations using common\nconfiguration management and orchestration tools.\n\n\nAnsible\n\n\nWith Ansible, installing the latest REX-Ray binaries can be accomplished by\nincluding the \ncodenrhoden.rexray\n role from Ansible Galaxy.  The role accepts\nall the necessary variables to properly fill out your \nconfig.yml\n file.\n\n\nInstall the role from Galaxy:\n\n\nansible-galaxy install emccode.rexray\n\n\n\n\nExample playbook for installing REX-Ray on GCE Docker hosts:\n\n\n- hosts: gce_docker_hosts\n  roles:\n  - { role: emccode.rexray,\n      rexray_service: true,\n      rexray_storage_drivers: [gce],\n      rexray_gce_keyfile: \n/opt/gce_keyfile\n }\n\n\n\n\nRun the playbook:\n\n\nansible-playbook -i \ninventory\n playbook.yml\n\n\n\n\nAWS CloudFormation\n\n\nWith CloudFormation, the installation of the latest Docker and REX-Ray binaries\ncan be passed to the orchestrator using the 'UserData' property in a\nCloudFormation template. While the payload could also be provided as raw user\ndata via the AWS GUI, it would not sustain scalable automation.\n\n\nProperties\n: {\n  \nUserData\n: {\n    \nFn::Base64\n: {\n      \nFn::Join\n: [\n, [\n        \n#!/bin/bash -xe\\n\n,\n        \napt-get update\\n\n,\n        \napt-get -y install python-setuptools\\n\n,\n        \neasy_install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\\n\n,\n        \nln -s /usr/local/lib/python2.7/dist-packages/aws_cfn_bootstrap-1.4-py2.7.egg/init/ubuntu/cfn-hup /etc/init.d/cfn-hup\\n\n,\n        \nchmod +x /etc/init.d/cfn-hup\\n\n,\n        \nupdate-rc.d cfn-hup defaults\\n \n,\n        \nservice cfn-hup start\\n\n,\n        \n/usr/local/bin/cfn-init --stack \n, {\n          \nRef\n: \nAWS::StackName\n\n        }, \n --resource RexrayInstance \n, \n --configsets InstallAndRun --region \n, {\n          \nRef\n: \nAWS::Region\n\n        }, \n\\n\n,\n\n        \n# Install the latest Docker..\\n\n,\n        \n/usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com/\\n\n,\n        \nchmod +x /tmp/install-docker.sh\\n\n,\n        \n/tmp/install-docker.sh\\n\n,\n\n        \n# add the ubuntu user to the docker group..\\n\n,\n        \n/usr/sbin/usermod -G docker ubuntu\\n\n,\n\n        \n# Install the latest REX-ray\\n\n,\n        \n/usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\\n\n,\n        \nchmod +x /tmp/install-rexray.sh\\n\n,\n        \n/tmp/install-rexray.sh\\n\n,\n        \nchgrp docker /etc/rexray/config.yml\\n\n,\n        \nreboot\\n\n\n      ]]\n    }\n  }\n}\n\n\n\n\nDocker Machine (VirtualBox)\n\n\nSSH can be used to remotely deploy REX-Ray to a Docker Machine. While the\nfollowing example used VirtualBox as the underlying storage platform, the\nprovided \nconfig.yml\n file \ncould\n be modified to use any of the supported\ndrivers.\n\n\n\n\n\n\nSSH into the Docker machine and install REX-Ray.\n\n\n$ docker-machine ssh testing1 \\\n\"curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -\"\n\n\n\n\n\n\n\nInstall the udev extras package. This step is only required for versions of\n   boot2docker older than 1.10.\n\n\n$ docker-machine ssh testing1 \\\n\"wget http://tinycorelinux.net/6.x/x86_64/tcz/udev-extra.tcz \\\n\n tce-load -i udev-extra.tcz \n sudo udevadm trigger\"\n\n\n\n\n\n\n\nCreate a basic REX-Ray configuration file inside the Docker machine.\n\n\nNote\n: It is recommended to replace the \nvolumePath\n parameter with the\nlocal path VirtualBox uses to store its virtual media disk files.\n\n\n$ docker-machine ssh testing1 \\\n    \"sudo tee -a /etc/rexray/config.yml \n EOF\n    rexray:\n      storageDrivers:\n      - virtualbox\n      volume:\n        mount:\n          preempt: false\n    virtualbox:\n      endpoint: http://10.0.2.2:18083\n      tls: false\n      volumePath: /Users/YourUser/VirtualBox Volumes\n      controllerName: SATA\n    \"\n\n\n\n\n\n\n\nFinally, start the \nREX-Ray\n service inside the Docker machine.\n\n\n$ docker-machine ssh testing1 \"sudo rexray start\"\n\n\n\n\n\n\n\nOpenStack Heat\n\n\nUsing OpenStack Heat, in the HOT template format (yaml):\n\n\nresources:\n  my_server:\n    type: OS::Nova::Server\n    properties:\n      user_data_format: RAW\n      user_data:\n        str_replace:\n          template: |\n            #!/bin/bash -v\n            /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com\n            chmod +x /tmp/install-docker.sh\n            /tmp/install-docker.sh\n            /usr/sbin/usermod -G docker ubuntu\n            /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\n            chmod +x /tmp/install-rexray.sh\n            /tmp/install-rexray.sh\n            chgrp docker /etc/rexray/config.yml\n          params:\n            dummy: \n\n\n\n\n\nVagrant\n\n\nUsing Vagrant is a great option to deploy pre-configured \nREX-Ray\n nodes,\nincluding Docker, using the VirtualBox driver. All volume requests are handled\nusing VirtualBox's Virtual Media.\n\n\nA Vagrant environment and instructions using it are provided\n\nhere\n.", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/installation/#installation", 
            "text": "Getting the bits, bit by bit", 
            "title": "Installation"
        }, 
        {
            "location": "/user-guide/installation/#overview", 
            "text": "There are several different methods available for installing  REX-Ray . It\nis written in Go, so there are typically no dependencies that must be installed\nalongside its single binary file. The manual methods can be extremely simple\nthrough tools like  curl . You also have the opportunity to perform install\nsteps individually. Following the manual installs,  configuration \nmust take place.  We also provide some great automation examples using tools like  Ansible  and Puppet . These approaches will perform the whole process including configuring\nthe REX-Ray for you.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/installation/#manual-installs", 
            "text": "Manual installations are in contrast to batch, automated installations.  Make sure that before installing REX-Ray that you have uninstalled any previous\nversions. A  rexray uninstall  can assist with this where appropriate.  Following an installation and configuration, you can use REX-Ray interactively\nthrough commands like  rexray volume . Noticeably different from this is having\nREX-Ray integrate with Container Engines such as Docker. This requires that\nyou run  rexray start  or relevant service start command like systemctl start rexray .", 
            "title": "Manual Installs"
        }, 
        {
            "location": "/user-guide/installation/#install-via-curl", 
            "text": "The following command will download the most recent, stable build of  REX-Ray \nand install it to  /usr/bin/rexray  or  /opt/bin/rexray . On Linux systems REX-Ray  will also be registered as either a SystemD or SystemV service.  There is an optional flag to choose which version to install. Notice how we\nspecify  stable , see the additional version names below that are also valid.  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s stable", 
            "title": "Install via curl"
        }, 
        {
            "location": "/user-guide/installation/#install-a-pre-built-binary", 
            "text": "There are a handful of necessary manual steps to properly install REX-Ray\nfrom pre-built binaries.    Download the proper binary. There are also pre-built binaries available for\nthe various release types.     Version  Description      Unstable  The most up-to-date, bleeding-edge, and often unstable REX-Ray binaries.    Staged  The most up-to-date, release candidate REX-Ray binaries.    Stable  The most up-to-date, stable REX-Ray binaries.       Uncompress and move the binary to the proper location. Preferably  /usr/bin \nshould be where REX-Ray is moved, but this path is not required.   Install as a service with  rexray install . This will register itself\nwith SystemD or SystemV for proper initialization.", 
            "title": "Install a pre-built binary"
        }, 
        {
            "location": "/user-guide/installation/#build-and-install-from-source", 
            "text": "REX-Ray  is also fairly simple to build from source, especially if you have Docker  installed:  SRC=$(mktemp -d 2  /dev/null || mktemp -d -t rexray 2  /dev/null)   cd $SRC   docker run --rm -it -e GO15VENDOREXPERIMENT=1 -v $SRC:/usr/src/rexray -w /usr/src/rexray golang:1.5.1 bash -c  git clone https://github.com/emccode/rexray.git -b master .   make build-all\u201d  If you'd prefer to not use  Docker  to build  REX-Ray  then all you need is Go 1.5:  # clone the rexray repo\ngit clone https://github.com/emccode/rexray.git\n\n# change directories into the freshly-cloned repo\ncd rexray\n\n# build rexray\nmake build-all  After either of the above methods for building  REX-Ray  there should be a  .bin  directory in the current directory, and inside  .bin  will be binaries for Linux-i386, Linux-x86-64,\nand Darwin-x86-64.  [0]akutz@poppy:tmp.SJxsykQwp7$ ls .bin/*/rexray\n-rwxr-xr-x. 1 root 14M Sep 17 10:36 .bin/Darwin-x86_64/rexray*\n-rwxr-xr-x. 1 root 12M Sep 17 10:36 .bin/Linux-i386/rexray*\n-rwxr-xr-x. 1 root 14M Sep 17 10:36 .bin/Linux-x86_64/rexray*", 
            "title": "Build and install from source"
        }, 
        {
            "location": "/user-guide/installation/#automated-installs", 
            "text": "Because REX-Ray is simple to install using the  curl  script, installation\nusing configuration management tools is relatively easy as well. However,\nthere are a few areas that may prove to be tricky, such as writing the\nconfiguration file.  This section provides examples of automated installations using common\nconfiguration management and orchestration tools.", 
            "title": "Automated Installs"
        }, 
        {
            "location": "/user-guide/installation/#ansible", 
            "text": "With Ansible, installing the latest REX-Ray binaries can be accomplished by\nincluding the  codenrhoden.rexray  role from Ansible Galaxy.  The role accepts\nall the necessary variables to properly fill out your  config.yml  file.  Install the role from Galaxy:  ansible-galaxy install emccode.rexray  Example playbook for installing REX-Ray on GCE Docker hosts:  - hosts: gce_docker_hosts\n  roles:\n  - { role: emccode.rexray,\n      rexray_service: true,\n      rexray_storage_drivers: [gce],\n      rexray_gce_keyfile:  /opt/gce_keyfile  }  Run the playbook:  ansible-playbook -i  inventory  playbook.yml", 
            "title": "Ansible"
        }, 
        {
            "location": "/user-guide/installation/#aws-cloudformation", 
            "text": "With CloudFormation, the installation of the latest Docker and REX-Ray binaries\ncan be passed to the orchestrator using the 'UserData' property in a\nCloudFormation template. While the payload could also be provided as raw user\ndata via the AWS GUI, it would not sustain scalable automation.  Properties : {\n   UserData : {\n     Fn::Base64 : {\n       Fn::Join : [ , [\n         #!/bin/bash -xe\\n ,\n         apt-get update\\n ,\n         apt-get -y install python-setuptools\\n ,\n         easy_install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\\n ,\n         ln -s /usr/local/lib/python2.7/dist-packages/aws_cfn_bootstrap-1.4-py2.7.egg/init/ubuntu/cfn-hup /etc/init.d/cfn-hup\\n ,\n         chmod +x /etc/init.d/cfn-hup\\n ,\n         update-rc.d cfn-hup defaults\\n  ,\n         service cfn-hup start\\n ,\n         /usr/local/bin/cfn-init --stack  , {\n           Ref :  AWS::StackName \n        },   --resource RexrayInstance  ,   --configsets InstallAndRun --region  , {\n           Ref :  AWS::Region \n        },  \\n ,\n\n         # Install the latest Docker..\\n ,\n         /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com/\\n ,\n         chmod +x /tmp/install-docker.sh\\n ,\n         /tmp/install-docker.sh\\n ,\n\n         # add the ubuntu user to the docker group..\\n ,\n         /usr/sbin/usermod -G docker ubuntu\\n ,\n\n         # Install the latest REX-ray\\n ,\n         /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\\n ,\n         chmod +x /tmp/install-rexray.sh\\n ,\n         /tmp/install-rexray.sh\\n ,\n         chgrp docker /etc/rexray/config.yml\\n ,\n         reboot\\n \n      ]]\n    }\n  }\n}", 
            "title": "AWS CloudFormation"
        }, 
        {
            "location": "/user-guide/installation/#docker-machine-virtualbox", 
            "text": "SSH can be used to remotely deploy REX-Ray to a Docker Machine. While the\nfollowing example used VirtualBox as the underlying storage platform, the\nprovided  config.yml  file  could  be modified to use any of the supported\ndrivers.    SSH into the Docker machine and install REX-Ray.  $ docker-machine ssh testing1 \\\n\"curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -\"    Install the udev extras package. This step is only required for versions of\n   boot2docker older than 1.10.  $ docker-machine ssh testing1 \\\n\"wget http://tinycorelinux.net/6.x/x86_64/tcz/udev-extra.tcz \\  tce-load -i udev-extra.tcz   sudo udevadm trigger\"    Create a basic REX-Ray configuration file inside the Docker machine.  Note : It is recommended to replace the  volumePath  parameter with the\nlocal path VirtualBox uses to store its virtual media disk files.  $ docker-machine ssh testing1 \\\n    \"sudo tee -a /etc/rexray/config.yml   EOF\n    rexray:\n      storageDrivers:\n      - virtualbox\n      volume:\n        mount:\n          preempt: false\n    virtualbox:\n      endpoint: http://10.0.2.2:18083\n      tls: false\n      volumePath: /Users/YourUser/VirtualBox Volumes\n      controllerName: SATA\n    \"    Finally, start the  REX-Ray  service inside the Docker machine.  $ docker-machine ssh testing1 \"sudo rexray start\"", 
            "title": "Docker Machine (VirtualBox)"
        }, 
        {
            "location": "/user-guide/installation/#openstack-heat", 
            "text": "Using OpenStack Heat, in the HOT template format (yaml):  resources:\n  my_server:\n    type: OS::Nova::Server\n    properties:\n      user_data_format: RAW\n      user_data:\n        str_replace:\n          template: |\n            #!/bin/bash -v\n            /usr/bin/curl -o /tmp/install-docker.sh https://get.docker.com\n            chmod +x /tmp/install-docker.sh\n            /tmp/install-docker.sh\n            /usr/sbin/usermod -G docker ubuntu\n            /usr/bin/curl -ssL -o /tmp/install-rexray.sh https://dl.bintray.com/emccode/rexray/install\n            chmod +x /tmp/install-rexray.sh\n            /tmp/install-rexray.sh\n            chgrp docker /etc/rexray/config.yml\n          params:\n            dummy:", 
            "title": "OpenStack Heat"
        }, 
        {
            "location": "/user-guide/installation/#vagrant", 
            "text": "Using Vagrant is a great option to deploy pre-configured  REX-Ray  nodes,\nincluding Docker, using the VirtualBox driver. All volume requests are handled\nusing VirtualBox's Virtual Media.  A Vagrant environment and instructions using it are provided here .", 
            "title": "Vagrant"
        }, 
        {
            "location": "/user-guide/config/", 
            "text": "Configuring REX-Ray\n\n\nTweak this, turn that, peek behind the curtain...\n\n\n\n\nOverview\n\n\nThis page reviews how to configure \nREX-Ray\n to suit any environment, beginning\nwith the the most common use cases, exploring recommended guidelines, and\nfinally, delving into the details of more advanced settings.\n\n\nBasic Configuration\n\n\nThis section outlines the most common configuration scenarios encountered by\n\nREX-Ray\n's users.\n\n\nA typical configuration occurs by creating the file \n/etc/rexray/config.yml\n.\nThis is the global settings file for \nREX-Ray\n, and is used whether the\nprogram is used as a command line interface (CLI) application or started as a\nbackground service. Please see the \nadvanced section\n\nfor more information and options regarding configuration.\n\n\nExample sans Modules\n\n\nThe following example is a YAML configuration for the VirtualBox driver.\n\n\nrexray:\n  logLevel: warn\n  storageDrivers:\n  - virtualbox\ndocker:\n  size: 1\nvirtualbox:\n  endpoint:       http://10.0.2.2:18083\n  tls:            false\n  volumePath:     $HOME/Repos/vagrant/rexray/Volumes\n  controllerName: SATA\n\n\n\n\nSettings occur in three primary areas:\n\n\n\n\nrexray\n\n\ndocker\n\n\nvirtualbox\n\n\n\n\nThe \nrexray\n section contains all properties specific to \nREX-Ray\n. The\nYAML property path \nrexray.storageDrivers\n lists the names of the storage\ndrivers loaded by \nREX-Ray\n. In this case, only the \nvirtualbox\n storage\ndriver is loaded. All of the \nrexray\n properties are\n\ndocumented\n below.\n\n\nThe \ndocker\n section defines properties specific to Docker. The property\n\ndocker.size\n defines in gigabytes the default size for a new Docker volume.\nThe complete list of properties for the \ndocker\n section are described on the\n\nSchedulers page\n.\n\n\nFinally, the \nvirtualbox\n section configures the VirtualBox driver loaded by\n\nREX-Ray\n, as indicated via the \nrexray.storageDrivers\n property). The\nStorage Drivers page has information about the configuration details of\n\neach driver\n, including\n\nVirtualBox\n.\n\n\nExample with Modules\n\n\nModules enable a single \nREX-Ray\n instance to present multiple personalities or\nvolume endpoints, serving hosts that require access to multiple storage\nplatforms.\n\n\nDefining Modules\n\n\nThe following example demonstrates a basic configuration that presents two\nmodules using the VirtualBox driver.\n\n\nrexray:\n  logLevel: warn\n  storageDrivers:\n  - virtualbox\n  modules:\n    default-docker:\n      type: docker\n      desc: \nThe default docker module.\n\n      host: \nunix:///run/docker/plugins/vb1.sock\n\n      docker:\n        size: 1\n      virtualbox:\n        endpoint:      http://10.0.2.2:18083\n        tls:           false\n        volumePath:    \n$HOME/Repos/vagrant/rexray/Volumes\n\n        controllerName: SATA\n    vb2-module:\n      type: docker\n      desc: \nThe second docker module.\n\n      host: \nunix:///run/docker/plugins/vb2.sock\n\n      docker:\n        size: 1\n      virtualbox:\n        endpoint:      http://10.0.2.2:18083\n        tls:           false\n        volumePath:    \n$HOME/Repos/vagrant/rexray/Volumes\n\n        controllerName: SATA\n\n\n\n\nLike the previous example that did not use modules, this example begins by\ndefining the root section \nrexray\n. Unlike the previous example, the \ndocker\n\nand  \nvirtualbox\n sections are no longer at the root. Instead the section\n\nrexray.modules\n is defined. The \nmodules\n key in the \nrexray\n section is\nwhere all modules are configured. Each key that is a child of \nmodules\n\nrepresents the name of a module.\n\n\nThe above example defines two modules:\n\n\n\n\n\n\ndefault-module\n\n\nThis is a special module, and it's always defined, even if not explicitly\nlisted. In the previous example without modules, the \ndocker\n and\n\nvirtualbox\n sections at the root actually informed the configuration of\nthe implicit \ndefault-docker\n module. In this example the explicit\ndeclaration of the \ndefault-docker\n module enables several of its\nproperties to be overridden and given desired values. The Advanced\nConfiguration section has more information on\n\nDefault Modules\n.\n\n\n\n\n\n\nvb2-module\n\n\nThis is a new, custom module configured almost identically to the\n\ndefault-module\n with the exception of a unique host address as defined\nby the module's \nhost\n key.\n\n\n\n\n\n\nNotice that both modules share many of the same properties and values. In fact,\nwhen defining both modules, the top-level \ndocker\n and \nvirtualbox\n sections\nwere simply copied into each module as sub-sections. This is perfectly valid\nas any configuration path that begins from the root of the \nREX-Ray\n\nconfiguration file can be duplicated beginning as a child of a module\ndefinition. This allows global settings to be overridden just for a specific\nmodules.\n\n\nAs noted, each module shares identical values with the exception of the module's\nname and host. The host is the address used by Docker to communicate with\n\nREX-Ray\n. The base name of the socket file specified in the address can be\nused with \ndocker --volume-driver=\n. With the current example the value of the\n\n--volume-driver\n parameter would be either \nvb1\n of \nvb2\n.\n\n\nModules and Inherited Properties\n\n\nThere is also another way to write the previous example while reducing the\nnumber of repeated, identical properties shared by two modules.\n\n\nrexray:\n  logLevel: warn\n  storageDrivers:\n  - virtualbox\n  modules:\n    default-docker:\n      host: \nunix:///run/docker/plugins/vb1.sock\n\n      docker:\n        size: 1\n    vb2:\n      type: docker\nvirtualbox:\n  endpoint:      http://10.0.2.2:18083\n  tls:           false\n  volumePath:    \n$HOME/Repos/vagrant/rexray/Volumes\n\n  controllerName: SATA\n\n\n\n\nThe above example may look strikingly different than the previous one, but it's\nactually the same with just a few tweaks.\n\n\nWhile there are still two modules defined, the second one has been renamed from\n\nvb2-module\n to \nvb2\n. The change is a more succinct way to represent the same\nintent, and it also provides a nice side-effect. If the \nhost\n key is omitted\nfrom a Docker module, the value for the \nhost\n key is automatically generated\nusing the module's name. Therefore since there is no \nhost\n key for the \nvb2\n\nmodule, the value will be \nunix:///run/docker/plugins/vb2.sock\n.\n\n\nAdditionally, \nvirtualbox\n sections from each module definition have been\nremoved and now only a single, global \nvirtualbox\n section is present at the\nroot. When accessing properties, a module will first attempt to access a\nproperty defined in the context of the module, but if that fails the property\nlookup will resolve against globally defined keys as well.\n\n\nFinally, please also note that the \ndocker\n section has \nnot\n been promoted\nback to a global property set, and is in fact still located in the context of\nthe \ndefault-docker\n module. This means that create volume requests sent to the\n\ndefault-docker\n module will result in 1GB volumes by default whereas create\nvolume requests handled by the \nvb2\n module will result in 16GB volumes (since\n16GB is the default value for the \ndocker.size\n property).\n\n\nLogging\n\n\nThe \n-l|--logLevel\n option or \nrexray.logLevel\n configuration key can be set\nto any of the following values to increase or decrease the verbosity of the\ninformation logged to the console or the \nREX-Ray\n log file (defaults to\n\n/var/log/rexray/rexray.log\n).\n\n\n\n\npanic\n\n\nfatal\n\n\nerror\n\n\nwarn\n\n\ninfo\n\n\ndebug\n\n\n\n\nTroubleshooting\n\n\nThe command \nrexray env\n can be used to print out the runtime interpretation\nof the environment, including configured properties, in order to help diagnose\nconfiguration issues.\n\n\n$ rexray env | grep DEFAULT-DOCKER\nREXRAY_MODULES_DEFAULT-DOCKER_TYPE=docker\nREXRAY_MODULES_DEFAULT-DOCKER_DISABLED=false\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_CONTROLLERNAME=SATA\nREXRAY_MODULES_DEFAULT-DOCKER_DESC=The default docker module.\nREXRAY_MODULES_DEFAULT-DOCKER_HOST=unix:///run/docker/plugins/vb1.sock\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_ENDPOINT=http://10.0.2.2:18083\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_VOLUMEPATH=$HOME/Repos/vagrant/rexray/Volumes\nREXRAY_MODULES_DEFAULT-DOCKER_DOCKER_SIZE=1\nREXRAY_MODULES_DEFAULT-DOCKER_SPEC=/etc/docker/plugins/rexray.spec\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_TLS=false\n\n\n\n\nAdvanced Configuration\n\n\nThe following sections detail every last aspect of how \nREX-Ray\n works and can\nbe configured.\n\n\nData Directories\n\n\nThe first time \nREX-Ray\n is executed it will create several directories if\nthey do not already exist:\n\n\n\n\n/etc/rexray\n\n\n/var/log/rexray\n\n\n/var/run/rexray\n\n\n/var/lib/rexray\n\n\n\n\nThe above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable \nREXRAY_HOME\n.\n\n\nREXRAY_HOME\n can be used to define a custom home directory for \nREX-Ray\n.\nThis directory is irrespective of the actual \nREX-Ray\n binary. Instead, the\ndirectory specified in \nREXRAY_HOME\n is the root directory where the \nREX-Ray\n\nbinary expects all of the program's data directories to be located.\n\n\nFor example, the following command sets a custom value for \nREXRAY_HOME\n and\nthen gets a volume list:\n\n\nenv REXRAY_HOME=/tmp/rexray rexray volume\n\n\n\n\nThe above command would produce a list of volumes and create the following\ndirectories in the process:\n\n\n\n\n/tmp/rexray/etc/rexray\n\n\n/tmp/rexray/var/log/rexray\n\n\n/tmp/rexray/var/run/rexray\n\n\n/tmp/rexray/var/lib/rexray\n\n\n\n\nThe entire configuration section will refer to the global configuration file as\na file located inside of \n/etc/rexray\n, but it should be noted that if\n\nREXRAY_HOME\n is set the location of the global configuration file can be\nchanged.\n\n\nConfiguration Methods\n\n\nThere are three ways to configure \nREX-Ray\n:\n\n\n\n\nCommand line options\n\n\nEnvironment variables\n\n\nConfiguration files\n\n\n\n\nThe order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.\n\n\nConfiguration Files\n\n\nThere are two \nREX-Ray\n configuration files - global and user:\n\n\n\n\n/etc/rexray/config.yml\n\n\n$HOME/.rexray/config.yml\n\n\n\n\nPlease note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts \nREX-Ray\n. And\nif \nREX-Ray\n is being started as a service, then \nsudo\n is likely being used,\nwhich means that \n$HOME/.rexray/config.yml\n won't point to \nyour\n home\ndirectory, but rather \n/root/.rexray/config.yml\n.\n\n\nThe next section has an example configuration with the default configuration.\n\n\nConfiguration Properties\n\n\nThe section \nConfiguration Methods\n mentions there are\nthree ways to configure REX-Ray: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.\n\n\nThese are the global configuration properties with their default values as\nrepresented in a a YAML configuration file:\n\n\nrexray:\n    logLevel: warn\n    osDrivers:\n    - linux\n    storageDrivers:\n    volumeDrivers:\n    - docker\n\n\n\n\nThe property \nrexray.logLevel\n is a string and the properties\n\nrexray.osDrivers\n, \nrexray.storageDrivers\n, and \nrexray.volumeDrivers\n are all\narrays of strings. These values can also be set via environment variables or the\ncommand line, but to do so requires knowing the names of the environment\nvariables or CLI flags to use. Luckily those are very easy to figure out just\nby knowing the property names.\n\n\nAll properties that might appear in the \nREX-Ray\n configuration file\nfall under some type of heading. For example, take the default configuration\nabove.\n\n\nThe rule for environment variables is as follows:\n\n\n\n\nEach nested level becomes a part of the environment variable name followed\n    by an underscore \n_\n except for the terminating part.\n\n\nThe entire environment variable name is uppercase.\n\n\n\n\nNested properties follow these rules for CLI flags:\n\n\n\n\nThe root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.\n\n\nThe remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.\n\n\nAll levels are then concatenated together.\n\n\nSee the verbose help for exact global flags using \nrexray --help -v\n\n    as they may be chopped to minimize verbosity.\n\n\n\n\nThe following table illustrates the transformations:\n\n\n\n\n\n\n\n\nProperty Name\n\n\nEnvironment Variable\n\n\nCLI Flag\n\n\n\n\n\n\n\n\n\n\nrexray.logLevel\n\n\nREXRAY_LOGLEVEL\n\n\n--logLevel\n\n\n\n\n\n\nrexray.osDrivers\n\n\nREXRAY_OSDRIVERS\n\n\n--osDrivers\n\n\n\n\n\n\nrexray.storageDrivers\n\n\nREXRAY_STORAGEDRIVERS\n\n\n--storageDrivers\n\n\n\n\n\n\nrexray.volumeDrivers\n\n\nREXRAY_VOLUMEDRIVERS\n\n\n--volumeDrivers\n\n\n\n\n\n\n\n\nAnother example is a possible configuration of the Amazon Web Services (AWS)\nElastic Compute Cloud (EC2) storage driver:\n\n\naws:\n    accessKey: MyAccessKey\n    secretKey: MySecretKey\n    region:    USNW\n\n\n\n\n\n\n\n\n\n\nProperty Name\n\n\nEnvironment Variable\n\n\nCLI Flag\n\n\n\n\n\n\n\n\n\n\naws.accessKey\n\n\nAWS_ACCESSKEY\n\n\n--awsAccessKey\n\n\n\n\n\n\naws.secretKey\n\n\nAWS_SECRETKEY\n\n\n--awsSecretKey\n\n\n\n\n\n\naws.region\n\n\nAWS_REGION\n\n\n--awsRegion\n\n\n\n\n\n\n\n\nString Arrays\n\n\nPlease note that properties that are represented as arrays in a configuration\nfile, such as the \nrexray.osDrivers\n, \nrexray.storageDrivers\n, and\n\nrexray.volumeDrivers\n above, are not arrays, but multi-valued strings where a\nspace acts as a delimiter. This is because the Viper project\n\ndoes not bind\n Go StringSlices\n(string arrays) correctly to \nPFlags\n.\n\n\nFor example, this is how one would specify the storage drivers \nec2\n and\n\nxtremio\n in a configuration file:\n\n\nrexray:\n    storageDrivers:\n    - ec2\n    - xtremio\n\n\n\n\nHowever, to specify the same values in an environment variable,\n\nREXRAY_STORAGEDRIVERS=\"ec2 xtremio\"\n, and as a CLI flag,\n\n--storageDrivers=\"ec2 xtremio\"\n.\n\n\nLogging Configuration\n\n\nThe \nREX-Ray\n log level determines the level of verbosity emitted by the\ninternal logger. The default level is \nwarn\n, but there are three other levels\nas well:\n\n\n\n\n\n\n\n\nLog Level\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nerror\n\n\nLog only errors\n\n\n\n\n\n\nwarn\n\n\nLog errors and anything out of place\n\n\n\n\n\n\ninfo\n\n\nLog errors, warnings, and workflow messages\n\n\n\n\n\n\ndebug\n\n\nLog everything\n\n\n\n\n\n\n\n\nFor example, the following two commands may look slightly different, but they\nare functionally the same, both printing a list of volumes using the \ndebug\n\nlog level:\n\n\nUse the \ndebug\n log level - Example 1\n\n\nrexray volume get -l debug\n\n\n\n\nUse the \ndebug\n log level - Example 2\n\n\nenv REXRAY_LOGLEVEL=debug rexray volume get\n\n\n\n\nDriver Configuration\n\n\nThere are three types of drivers:\n\n\n\n\nOS Drivers\n\n\nStorage Drivers\n\n\nVolume Drivers\n\n\n\n\nOS Drivers\n\n\nOperating system (OS) drivers enable \nREX-Ray\n to manage storage on\nthe underlying OS. Currently the following OS drivers are supported:\n\n\n\n\n\n\n\n\nDriver\n\n\nDriver Name\n\n\n\n\n\n\n\n\n\n\nLinux\n\n\nlinux\n\n\n\n\n\n\n\n\nThe OS driver \nlinux\n is automatically activated when \nREX-Ray\n is running on\nthe Linux OS.\n\n\nStorage Drivers\n\n\nStorage drivers enable \nREX-Ray\n to communicate with direct-attached or remote\nstorage systems. Currently the following storage drivers are supported:\n\n\n\n\n\n\n\n\nDriver\n\n\nDriver Name\n\n\n\n\n\n\n\n\n\n\nAmazon EC2\n\n\nec2\n\n\n\n\n\n\nGoogle Compute Engine\n\n\ngce\n\n\n\n\n\n\nIsilon\n\n\nisilon\n\n\n\n\n\n\nOpenStack\n\n\nopenstack\n\n\n\n\n\n\nRackspace\n\n\nrackspace\n\n\n\n\n\n\nScaleIO\n\n\nscaleio\n\n\n\n\n\n\nVirtualBox\n\n\nvirtualbox\n\n\n\n\n\n\nVMAX\n\n\nvmax\n\n\n\n\n\n\nXtremIO\n\n\nxtremio\n\n\n\n\n\n\n\n\nThe \nrexray.storageDrivers\n property can be used to activate storage drivers..\n\n\nVolume Drivers\n\n\nVolume drivers enable \nREX-Ray\n to manage volumes for consumers of the storage,\nsuch as \nDocker\n or \nMesos\n. Currently the following volume drivers are\nsupported:\n\n\n\n\n\n\n\n\nDriver\n\n\nDriver Name\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\ndocker\n\n\n\n\n\n\n\n\nThe volume driver \ndocker\n is automatically activated.\n\n\nModule Configuration\n\n\nThis section reviews exposing multiple, differently configured endpoints by\nusing modules.\n\n\nDefault Modules\n\n\nIf not explicitly specified in a configuration source, \nREX-Ray\n always\nconsiders the following, default modules:\n\n\nrexray:\n    modules:\n        default-admin:\n            type:     admin\n            desc:     The default admin module.\n            host:     tcp://127.0.0.1:7979\n            disabled: false\n        default-docker:\n            type:     docker\n            desc:     The default docker module.\n            host:     unix:///run/docker/plugins/rexray.sock\n            spec:     /etc/docker/plugins/rexray.spec\n            disabled: false\n\n\n\n\nThe first module, \ndefault-admin\n, is used by the CLI to communicate with the\nREX-Ray service API. For security reasons the \ndefault-admin\n module is bound\nto the loopback IP address by default.\n\n\nThe second default module, \ndefault-docker\n, exposes \nREX-Ray\n as a Docker\nVolume Plug-in via the specified sock and spec files.\n\n\nAdditional Modules\n\n\nIt's also possible to create additional modules via the configuration file:\n\n\nrexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type: docker\n            desc: A second docker module.\n            host: unix:///run/docker/plugins/isilon2.sock\n            spec: /etc/docker/plugins/isilon2.spec\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24\n\n\n\n\nThe above example defines three modules:\n\n\n\n\ndefault-admin\n\n\ndefault-docker\n\n\nisilon2\n\n\n\n\nThe first two, default modules are not included in the configuration file as\nthey are implicit. The only reason to define them explicitly is to override\ntheir properties, a feature discussed in the next section.\n\n\nIgnoring the \ndefault-admin\n module, the \ndefault-docker\n and \nisilon2\n modules\nare both Docker modules as indicated by a module's \ntype\n property. Just like\nthe \ndefault-docker\n module, the custom module \nisilon2\n is configured to use\nthe default isilon settings from the root key, \nisilon\n. Therefore the modules\n\ndefault-docker\n and \nisilon2\n are configured exactly the same except for they\nare exposed via different sock and spec files.\n\n\nInferred Properties\n\n\nThe following example is nearly identical to the previous one except this\nexample is missing the \nhost\n, \nspec\n, \ndesc\n, and \ndisabled\n properties for\nthe \nisilon2\n module:\n\n\nrexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type: docker\n            desc: A second docker module.\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24\n\n\n\n\nA module is not required to provide a description (\ndesc\n), and \ndisabled\n will\nalways default to \nfalse\n unless explicitly specified as \ntrue\n. Docker modules\n(\ntype: docker\n) will also infer the values of the \nhost\n and \nspec\n\nproperties if they are not explicitly provided. Because the name of the\nmodule above is \nisilon2\n and the \nhost\n and \nspec\n properties are not defined,\nthose values will be automatically set to\n\nunix:///run/docker/plugins/isilon2.sock\n and \n/etc/docker/plugins/isilon2.spec\n\nrespectively.\n\n\nOverriding Defaults\n\n\nIt is also possible to override a default module's configuration. What if it's\ndetermined the \ndefault-admin\n module should be accessible externally and the\n\ndefault-docker\n module should use a different sock file? Simply override those\nkeys and only those keys:\n\n\nrexray:\n    storageDrivers:\n    - isilon\n    modules:\n        default-admin:\n            host: tcp://:7979\n        default-docker:\n            host: unix:///run/docker/plugins/isilon1.sock\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24\n\n\n\n\nOverriding Inherited Properties\n\n\nIn all of the module configuration examples so far there has been a root key\nnamed \nisilon\n that provides the settings for the storage driver used by the\nmodules. Thanks to scoped configuration support and inherited properties, it's\nalso quite simple to provide adjustments to a default configuration at the\nmodule level. For example, imagine the \nisilon2\n module should load a driver\nthat points to a different \nvolumePath\n?\n\n\nrexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type:       docker\n            isilon:\n                volumePath: /rexray/isilon2\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24\n\n\n\n\nThe above example will load two Docker modules, the \ndefault-docker\n module and\nthe \nisilon2\n module. The \ndefault-docker\n module's \nisilon.volumePath\n will be\nset to \n/rexray/default\n whereas the \nisilon2\n module's \nisilon.volumePath\n is\noverridden and set to \n/rexray/isilon2\n.\n\n\nAny key path structure can be duplicated under the module's name, and the value\nat the terminus of that path will be used in place of any inherited value.\nAnother example is overriding the type of storage driver used by the \nisilon2\n\nmodule. There may be a case where the \nisilon2\n module needs to use an\nenhanced version of the \nisilon\n storage driver but still use the same\nconfiguration:\n\n\nrexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type:       docker\n            rexray:\n                storageDrivers:\n                - isilonEnhanced\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24\n\n\n\n\nThe above example recreates the key path structure \nrexray.storageDrivers\n\nbeneath the key path structure \nrexray.modules.isilon2\n. Whenever any query is\nmade for \nrexray.storageDrivers\n inside the \nisilon2\n module, the value\n\n[]string{\"isilonEnhanced\"}\n is returned instead of \n[]string{\"isilon\"}\n.\n\n\nDisabling Modules\n\n\nBoth default and custom modules can be disabled by setting the key \ndisabled\n to\ntrue inside a module definition:\n\n\nrexray:\n    storageDrivers:\n    - isilon\n    modules:\n        default-docker:\n            disabled: true\n        isilon2:\n            type:     docker\n        isilon3:\n            type:     docker\n            disabled: true\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24\n\n\n\n\nThe above example disables the \ndefault-docker\n and \nisilon3\n modules such that\n\nisilon2\n is the only Docker module loaded.\n\n\nVolume Configuration\n\n\nThis section describes various global configuration options related to\noperations such as mounting and unmounting volumes.\n\n\nPreemption\n\n\nThere is a capability to preemptively detach any existing attachments to other\ninstances before attempting a mount.  This will enable use cases for\navailability where another instance must be able to take control of a volume\nwithout the current owner instance being involved.  The operation is considered\nequivalent to a power off of the existing instance for the device.\n\n\nExample configuration file follows:\n\n\nrexray:\n  storageDrivers:\n  - openstack\n  volume:\n    mount:\n      preempt: true\nopenStack:\n  authUrl: https://authUrl:35357/v2.0/\n  username: username\n  password: password\n  tenantName: tenantName\n  regionName: regionName\n\n\n\n\n\n\n\n\n\n\nDriver\n\n\nSupported\n\n\n\n\n\n\n\n\n\n\nEC2\n\n\nYes, no Ubuntu support\n\n\n\n\n\n\nIsilon\n\n\nNot yet\n\n\n\n\n\n\nOpenStack\n\n\nWith Cinder v2\n\n\n\n\n\n\nScaleIO\n\n\nYes\n\n\n\n\n\n\nRackspace\n\n\nNo\n\n\n\n\n\n\nVirtualBox\n\n\nYes\n\n\n\n\n\n\nVMAX\n\n\nNot yet\n\n\n\n\n\n\nXtremIO\n\n\nYes\n\n\n\n\n\n\n\n\nIgnore Used Count\n\n\nBy default accounting takes place during operations that are performed\non \nMount\n, \nUnmount\n, and other operations.  This only has impact when running\nas a service through the HTTP/JSON interface since the counts are persisted\nin memory.  The purpose of respecting the \nUsed Count\n is to ensure that a\nvolume is not unmounted until the unmount requests have equaled the mount\nrequests.  \n\n\nIn the \nDocker\n use case if there are multiple containers sharing a volume\non the same host, the the volume will not be unmounted until the last container\nis stopped.  \n\n\nThe following setting should only be used if you wish to \ndisable\n this\nfunctionality.  This would make sense if the accounting is being done from\nhigher layers and all unmount operations should proceed without control.\n\n\nrexray:\n  volume:\n    unmount:\n      ignoreUsedCount: true\n\n\n\n\nCurrently a reset of the service will cause the counts to be reset.  This\nwill cause issues if \nmultiple containers\n are sharing a volume.  If you are\nsharing volumes, it is recommended that you reset the service along with the\naccompanying container runtime (if this setting is false) to ensure they are\nsynchronized.  \n\n\nVolume Path Disable Cache (0.3.2)\n\n\nIn order to minimize the impact to return \nPath\n requests, a caching\ncapability has been introduced by default. A \nList\n request will cause the\nreturned volumes and paths to be evaluated and those with active mounts are\nrecorded. Subsequent \nPath\n requests for volumes that have no recorded mounts\nwill not result in active path lookups. Once the mount counter is initialized or\na \nList\n operation occurs where a mount is recorded, the volume will be looked\nup for future \nPath\n operations.\n\n\nrexray:\n  volume:\n    path:\n      disableCache: true\n\n\n\n\nVolume Root Path (0.3.1)\n\n\nWhen volumes are mounted there can be an additional path that is specified to\nbe created and passed as the valid mount point.  This is required for certain\napplications that do not want to place data from the root of a mount point.\n\nThe default is the \n/data\n path.  If a value is set by \nlinux.volume.rootPath\n,\nthen the default will be overwritten.\n\n\nIf upgrading to 0.3.1 then you can either set this to an empty value, or move\nthe internal directory in your existing volumes to \n/data\n.\n\n\nrexray:\nlinux:\n  volume:\n    rootPath: /data\n\n\n\n\nVolume FileMode (0.3.1)\n\n\nThe permissions of the \nlinux.volume.rootPath\n can be set to default values.  At\neach mount, the permissions will be written based on this value.  The default\nis to include the \n0700\n mode.\n\n\nrexray:\nlinux:\n  volume:\n    fileMode: 0700", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/config/#configuring-rex-ray", 
            "text": "Tweak this, turn that, peek behind the curtain...", 
            "title": "Configuring REX-Ray"
        }, 
        {
            "location": "/user-guide/config/#overview", 
            "text": "This page reviews how to configure  REX-Ray  to suit any environment, beginning\nwith the the most common use cases, exploring recommended guidelines, and\nfinally, delving into the details of more advanced settings.", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/config/#basic-configuration", 
            "text": "This section outlines the most common configuration scenarios encountered by REX-Ray 's users.  A typical configuration occurs by creating the file  /etc/rexray/config.yml .\nThis is the global settings file for  REX-Ray , and is used whether the\nprogram is used as a command line interface (CLI) application or started as a\nbackground service. Please see the  advanced section \nfor more information and options regarding configuration.", 
            "title": "Basic Configuration"
        }, 
        {
            "location": "/user-guide/config/#example-sans-modules", 
            "text": "The following example is a YAML configuration for the VirtualBox driver.  rexray:\n  logLevel: warn\n  storageDrivers:\n  - virtualbox\ndocker:\n  size: 1\nvirtualbox:\n  endpoint:       http://10.0.2.2:18083\n  tls:            false\n  volumePath:     $HOME/Repos/vagrant/rexray/Volumes\n  controllerName: SATA  Settings occur in three primary areas:   rexray  docker  virtualbox   The  rexray  section contains all properties specific to  REX-Ray . The\nYAML property path  rexray.storageDrivers  lists the names of the storage\ndrivers loaded by  REX-Ray . In this case, only the  virtualbox  storage\ndriver is loaded. All of the  rexray  properties are documented  below.  The  docker  section defines properties specific to Docker. The property docker.size  defines in gigabytes the default size for a new Docker volume.\nThe complete list of properties for the  docker  section are described on the Schedulers page .  Finally, the  virtualbox  section configures the VirtualBox driver loaded by REX-Ray , as indicated via the  rexray.storageDrivers  property). The\nStorage Drivers page has information about the configuration details of each driver , including VirtualBox .", 
            "title": "Example sans Modules"
        }, 
        {
            "location": "/user-guide/config/#example-with-modules", 
            "text": "Modules enable a single  REX-Ray  instance to present multiple personalities or\nvolume endpoints, serving hosts that require access to multiple storage\nplatforms.", 
            "title": "Example with Modules"
        }, 
        {
            "location": "/user-guide/config/#defining-modules", 
            "text": "The following example demonstrates a basic configuration that presents two\nmodules using the VirtualBox driver.  rexray:\n  logLevel: warn\n  storageDrivers:\n  - virtualbox\n  modules:\n    default-docker:\n      type: docker\n      desc:  The default docker module. \n      host:  unix:///run/docker/plugins/vb1.sock \n      docker:\n        size: 1\n      virtualbox:\n        endpoint:      http://10.0.2.2:18083\n        tls:           false\n        volumePath:     $HOME/Repos/vagrant/rexray/Volumes \n        controllerName: SATA\n    vb2-module:\n      type: docker\n      desc:  The second docker module. \n      host:  unix:///run/docker/plugins/vb2.sock \n      docker:\n        size: 1\n      virtualbox:\n        endpoint:      http://10.0.2.2:18083\n        tls:           false\n        volumePath:     $HOME/Repos/vagrant/rexray/Volumes \n        controllerName: SATA  Like the previous example that did not use modules, this example begins by\ndefining the root section  rexray . Unlike the previous example, the  docker \nand   virtualbox  sections are no longer at the root. Instead the section rexray.modules  is defined. The  modules  key in the  rexray  section is\nwhere all modules are configured. Each key that is a child of  modules \nrepresents the name of a module.  The above example defines two modules:    default-module  This is a special module, and it's always defined, even if not explicitly\nlisted. In the previous example without modules, the  docker  and virtualbox  sections at the root actually informed the configuration of\nthe implicit  default-docker  module. In this example the explicit\ndeclaration of the  default-docker  module enables several of its\nproperties to be overridden and given desired values. The Advanced\nConfiguration section has more information on Default Modules .    vb2-module  This is a new, custom module configured almost identically to the default-module  with the exception of a unique host address as defined\nby the module's  host  key.    Notice that both modules share many of the same properties and values. In fact,\nwhen defining both modules, the top-level  docker  and  virtualbox  sections\nwere simply copied into each module as sub-sections. This is perfectly valid\nas any configuration path that begins from the root of the  REX-Ray \nconfiguration file can be duplicated beginning as a child of a module\ndefinition. This allows global settings to be overridden just for a specific\nmodules.  As noted, each module shares identical values with the exception of the module's\nname and host. The host is the address used by Docker to communicate with REX-Ray . The base name of the socket file specified in the address can be\nused with  docker --volume-driver= . With the current example the value of the --volume-driver  parameter would be either  vb1  of  vb2 .", 
            "title": "Defining Modules"
        }, 
        {
            "location": "/user-guide/config/#modules-and-inherited-properties", 
            "text": "There is also another way to write the previous example while reducing the\nnumber of repeated, identical properties shared by two modules.  rexray:\n  logLevel: warn\n  storageDrivers:\n  - virtualbox\n  modules:\n    default-docker:\n      host:  unix:///run/docker/plugins/vb1.sock \n      docker:\n        size: 1\n    vb2:\n      type: docker\nvirtualbox:\n  endpoint:      http://10.0.2.2:18083\n  tls:           false\n  volumePath:     $HOME/Repos/vagrant/rexray/Volumes \n  controllerName: SATA  The above example may look strikingly different than the previous one, but it's\nactually the same with just a few tweaks.  While there are still two modules defined, the second one has been renamed from vb2-module  to  vb2 . The change is a more succinct way to represent the same\nintent, and it also provides a nice side-effect. If the  host  key is omitted\nfrom a Docker module, the value for the  host  key is automatically generated\nusing the module's name. Therefore since there is no  host  key for the  vb2 \nmodule, the value will be  unix:///run/docker/plugins/vb2.sock .  Additionally,  virtualbox  sections from each module definition have been\nremoved and now only a single, global  virtualbox  section is present at the\nroot. When accessing properties, a module will first attempt to access a\nproperty defined in the context of the module, but if that fails the property\nlookup will resolve against globally defined keys as well.  Finally, please also note that the  docker  section has  not  been promoted\nback to a global property set, and is in fact still located in the context of\nthe  default-docker  module. This means that create volume requests sent to the default-docker  module will result in 1GB volumes by default whereas create\nvolume requests handled by the  vb2  module will result in 16GB volumes (since\n16GB is the default value for the  docker.size  property).", 
            "title": "Modules and Inherited Properties"
        }, 
        {
            "location": "/user-guide/config/#logging", 
            "text": "The  -l|--logLevel  option or  rexray.logLevel  configuration key can be set\nto any of the following values to increase or decrease the verbosity of the\ninformation logged to the console or the  REX-Ray  log file (defaults to /var/log/rexray/rexray.log ).   panic  fatal  error  warn  info  debug", 
            "title": "Logging"
        }, 
        {
            "location": "/user-guide/config/#troubleshooting", 
            "text": "The command  rexray env  can be used to print out the runtime interpretation\nof the environment, including configured properties, in order to help diagnose\nconfiguration issues.  $ rexray env | grep DEFAULT-DOCKER\nREXRAY_MODULES_DEFAULT-DOCKER_TYPE=docker\nREXRAY_MODULES_DEFAULT-DOCKER_DISABLED=false\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_CONTROLLERNAME=SATA\nREXRAY_MODULES_DEFAULT-DOCKER_DESC=The default docker module.\nREXRAY_MODULES_DEFAULT-DOCKER_HOST=unix:///run/docker/plugins/vb1.sock\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_ENDPOINT=http://10.0.2.2:18083\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_VOLUMEPATH=$HOME/Repos/vagrant/rexray/Volumes\nREXRAY_MODULES_DEFAULT-DOCKER_DOCKER_SIZE=1\nREXRAY_MODULES_DEFAULT-DOCKER_SPEC=/etc/docker/plugins/rexray.spec\nREXRAY_MODULES_DEFAULT-DOCKER_VIRTUALBOX_TLS=false", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/config/#advanced-configuration", 
            "text": "The following sections detail every last aspect of how  REX-Ray  works and can\nbe configured.", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/user-guide/config/#data-directories", 
            "text": "The first time  REX-Ray  is executed it will create several directories if\nthey do not already exist:   /etc/rexray  /var/log/rexray  /var/run/rexray  /var/lib/rexray   The above directories will contain configuration files, logs, PID files, and\nmounted volumes. However, the location of these directories can also be\ninfluenced with the environment variable  REXRAY_HOME .  REXRAY_HOME  can be used to define a custom home directory for  REX-Ray .\nThis directory is irrespective of the actual  REX-Ray  binary. Instead, the\ndirectory specified in  REXRAY_HOME  is the root directory where the  REX-Ray \nbinary expects all of the program's data directories to be located.  For example, the following command sets a custom value for  REXRAY_HOME  and\nthen gets a volume list:  env REXRAY_HOME=/tmp/rexray rexray volume  The above command would produce a list of volumes and create the following\ndirectories in the process:   /tmp/rexray/etc/rexray  /tmp/rexray/var/log/rexray  /tmp/rexray/var/run/rexray  /tmp/rexray/var/lib/rexray   The entire configuration section will refer to the global configuration file as\na file located inside of  /etc/rexray , but it should be noted that if REXRAY_HOME  is set the location of the global configuration file can be\nchanged.", 
            "title": "Data Directories"
        }, 
        {
            "location": "/user-guide/config/#configuration-methods", 
            "text": "There are three ways to configure  REX-Ray :   Command line options  Environment variables  Configuration files   The order of the items above is also the order of precedence when considering\noptions set in multiple locations that may override one another. Values set\nvia CLI flags have the highest order of precedence, followed by values set by\nenvironment variables, followed, finally, by values set in configuration files.", 
            "title": "Configuration Methods"
        }, 
        {
            "location": "/user-guide/config/#configuration-files", 
            "text": "There are two  REX-Ray  configuration files - global and user:   /etc/rexray/config.yml  $HOME/.rexray/config.yml   Please note that while the user configuration file is located inside the user's\nhome directory, this is the directory of the user that starts  REX-Ray . And\nif  REX-Ray  is being started as a service, then  sudo  is likely being used,\nwhich means that  $HOME/.rexray/config.yml  won't point to  your  home\ndirectory, but rather  /root/.rexray/config.yml .  The next section has an example configuration with the default configuration.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/user-guide/config/#configuration-properties", 
            "text": "The section  Configuration Methods  mentions there are\nthree ways to configure REX-Ray: config files, environment variables, and the\ncommand line. However, this section will illuminate the relationship between the\nnames of the configuration file properties, environment variables, and CLI\nflags.  These are the global configuration properties with their default values as\nrepresented in a a YAML configuration file:  rexray:\n    logLevel: warn\n    osDrivers:\n    - linux\n    storageDrivers:\n    volumeDrivers:\n    - docker  The property  rexray.logLevel  is a string and the properties rexray.osDrivers ,  rexray.storageDrivers , and  rexray.volumeDrivers  are all\narrays of strings. These values can also be set via environment variables or the\ncommand line, but to do so requires knowing the names of the environment\nvariables or CLI flags to use. Luckily those are very easy to figure out just\nby knowing the property names.  All properties that might appear in the  REX-Ray  configuration file\nfall under some type of heading. For example, take the default configuration\nabove.  The rule for environment variables is as follows:   Each nested level becomes a part of the environment variable name followed\n    by an underscore  _  except for the terminating part.  The entire environment variable name is uppercase.   Nested properties follow these rules for CLI flags:   The root level's first character is lower-cased with the rest of the root\n    level's text left unaltered.  The remaining levels' first characters are all upper-cased with the the\n    remaining text of that level left unaltered.  All levels are then concatenated together.  See the verbose help for exact global flags using  rexray --help -v \n    as they may be chopped to minimize verbosity.   The following table illustrates the transformations:     Property Name  Environment Variable  CLI Flag      rexray.logLevel  REXRAY_LOGLEVEL  --logLevel    rexray.osDrivers  REXRAY_OSDRIVERS  --osDrivers    rexray.storageDrivers  REXRAY_STORAGEDRIVERS  --storageDrivers    rexray.volumeDrivers  REXRAY_VOLUMEDRIVERS  --volumeDrivers     Another example is a possible configuration of the Amazon Web Services (AWS)\nElastic Compute Cloud (EC2) storage driver:  aws:\n    accessKey: MyAccessKey\n    secretKey: MySecretKey\n    region:    USNW     Property Name  Environment Variable  CLI Flag      aws.accessKey  AWS_ACCESSKEY  --awsAccessKey    aws.secretKey  AWS_SECRETKEY  --awsSecretKey    aws.region  AWS_REGION  --awsRegion", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/user-guide/config/#string-arrays", 
            "text": "Please note that properties that are represented as arrays in a configuration\nfile, such as the  rexray.osDrivers ,  rexray.storageDrivers , and rexray.volumeDrivers  above, are not arrays, but multi-valued strings where a\nspace acts as a delimiter. This is because the Viper project does not bind  Go StringSlices\n(string arrays) correctly to  PFlags .  For example, this is how one would specify the storage drivers  ec2  and xtremio  in a configuration file:  rexray:\n    storageDrivers:\n    - ec2\n    - xtremio  However, to specify the same values in an environment variable, REXRAY_STORAGEDRIVERS=\"ec2 xtremio\" , and as a CLI flag, --storageDrivers=\"ec2 xtremio\" .", 
            "title": "String Arrays"
        }, 
        {
            "location": "/user-guide/config/#logging-configuration", 
            "text": "The  REX-Ray  log level determines the level of verbosity emitted by the\ninternal logger. The default level is  warn , but there are three other levels\nas well:     Log Level  Description      error  Log only errors    warn  Log errors and anything out of place    info  Log errors, warnings, and workflow messages    debug  Log everything     For example, the following two commands may look slightly different, but they\nare functionally the same, both printing a list of volumes using the  debug \nlog level:  Use the  debug  log level - Example 1  rexray volume get -l debug  Use the  debug  log level - Example 2  env REXRAY_LOGLEVEL=debug rexray volume get", 
            "title": "Logging Configuration"
        }, 
        {
            "location": "/user-guide/config/#driver-configuration", 
            "text": "There are three types of drivers:   OS Drivers  Storage Drivers  Volume Drivers", 
            "title": "Driver Configuration"
        }, 
        {
            "location": "/user-guide/config/#os-drivers", 
            "text": "Operating system (OS) drivers enable  REX-Ray  to manage storage on\nthe underlying OS. Currently the following OS drivers are supported:     Driver  Driver Name      Linux  linux     The OS driver  linux  is automatically activated when  REX-Ray  is running on\nthe Linux OS.", 
            "title": "OS Drivers"
        }, 
        {
            "location": "/user-guide/config/#storage-drivers", 
            "text": "Storage drivers enable  REX-Ray  to communicate with direct-attached or remote\nstorage systems. Currently the following storage drivers are supported:     Driver  Driver Name      Amazon EC2  ec2    Google Compute Engine  gce    Isilon  isilon    OpenStack  openstack    Rackspace  rackspace    ScaleIO  scaleio    VirtualBox  virtualbox    VMAX  vmax    XtremIO  xtremio     The  rexray.storageDrivers  property can be used to activate storage drivers..", 
            "title": "Storage Drivers"
        }, 
        {
            "location": "/user-guide/config/#volume-drivers", 
            "text": "Volume drivers enable  REX-Ray  to manage volumes for consumers of the storage,\nsuch as  Docker  or  Mesos . Currently the following volume drivers are\nsupported:     Driver  Driver Name      Docker  docker     The volume driver  docker  is automatically activated.", 
            "title": "Volume Drivers"
        }, 
        {
            "location": "/user-guide/config/#module-configuration", 
            "text": "This section reviews exposing multiple, differently configured endpoints by\nusing modules.", 
            "title": "Module Configuration"
        }, 
        {
            "location": "/user-guide/config/#default-modules", 
            "text": "If not explicitly specified in a configuration source,  REX-Ray  always\nconsiders the following, default modules:  rexray:\n    modules:\n        default-admin:\n            type:     admin\n            desc:     The default admin module.\n            host:     tcp://127.0.0.1:7979\n            disabled: false\n        default-docker:\n            type:     docker\n            desc:     The default docker module.\n            host:     unix:///run/docker/plugins/rexray.sock\n            spec:     /etc/docker/plugins/rexray.spec\n            disabled: false  The first module,  default-admin , is used by the CLI to communicate with the\nREX-Ray service API. For security reasons the  default-admin  module is bound\nto the loopback IP address by default.  The second default module,  default-docker , exposes  REX-Ray  as a Docker\nVolume Plug-in via the specified sock and spec files.", 
            "title": "Default Modules"
        }, 
        {
            "location": "/user-guide/config/#additional-modules", 
            "text": "It's also possible to create additional modules via the configuration file:  rexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type: docker\n            desc: A second docker module.\n            host: unix:///run/docker/plugins/isilon2.sock\n            spec: /etc/docker/plugins/isilon2.spec\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24  The above example defines three modules:   default-admin  default-docker  isilon2   The first two, default modules are not included in the configuration file as\nthey are implicit. The only reason to define them explicitly is to override\ntheir properties, a feature discussed in the next section.  Ignoring the  default-admin  module, the  default-docker  and  isilon2  modules\nare both Docker modules as indicated by a module's  type  property. Just like\nthe  default-docker  module, the custom module  isilon2  is configured to use\nthe default isilon settings from the root key,  isilon . Therefore the modules default-docker  and  isilon2  are configured exactly the same except for they\nare exposed via different sock and spec files.", 
            "title": "Additional Modules"
        }, 
        {
            "location": "/user-guide/config/#inferred-properties", 
            "text": "The following example is nearly identical to the previous one except this\nexample is missing the  host ,  spec ,  desc , and  disabled  properties for\nthe  isilon2  module:  rexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type: docker\n            desc: A second docker module.\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24  A module is not required to provide a description ( desc ), and  disabled  will\nalways default to  false  unless explicitly specified as  true . Docker modules\n( type: docker ) will also infer the values of the  host  and  spec \nproperties if they are not explicitly provided. Because the name of the\nmodule above is  isilon2  and the  host  and  spec  properties are not defined,\nthose values will be automatically set to unix:///run/docker/plugins/isilon2.sock  and  /etc/docker/plugins/isilon2.spec \nrespectively.", 
            "title": "Inferred Properties"
        }, 
        {
            "location": "/user-guide/config/#overriding-defaults", 
            "text": "It is also possible to override a default module's configuration. What if it's\ndetermined the  default-admin  module should be accessible externally and the default-docker  module should use a different sock file? Simply override those\nkeys and only those keys:  rexray:\n    storageDrivers:\n    - isilon\n    modules:\n        default-admin:\n            host: tcp://:7979\n        default-docker:\n            host: unix:///run/docker/plugins/isilon1.sock\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24", 
            "title": "Overriding Defaults"
        }, 
        {
            "location": "/user-guide/config/#overriding-inherited-properties", 
            "text": "In all of the module configuration examples so far there has been a root key\nnamed  isilon  that provides the settings for the storage driver used by the\nmodules. Thanks to scoped configuration support and inherited properties, it's\nalso quite simple to provide adjustments to a default configuration at the\nmodule level. For example, imagine the  isilon2  module should load a driver\nthat points to a different  volumePath ?  rexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type:       docker\n            isilon:\n                volumePath: /rexray/isilon2\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24  The above example will load two Docker modules, the  default-docker  module and\nthe  isilon2  module. The  default-docker  module's  isilon.volumePath  will be\nset to  /rexray/default  whereas the  isilon2  module's  isilon.volumePath  is\noverridden and set to  /rexray/isilon2 .  Any key path structure can be duplicated under the module's name, and the value\nat the terminus of that path will be used in place of any inherited value.\nAnother example is overriding the type of storage driver used by the  isilon2 \nmodule. There may be a case where the  isilon2  module needs to use an\nenhanced version of the  isilon  storage driver but still use the same\nconfiguration:  rexray:\n    storageDrivers:\n    - isilon\n    modules:\n        isilon2:\n            type:       docker\n            rexray:\n                storageDrivers:\n                - isilonEnhanced\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24  The above example recreates the key path structure  rexray.storageDrivers \nbeneath the key path structure  rexray.modules.isilon2 . Whenever any query is\nmade for  rexray.storageDrivers  inside the  isilon2  module, the value []string{\"isilonEnhanced\"}  is returned instead of  []string{\"isilon\"} .", 
            "title": "Overriding Inherited Properties"
        }, 
        {
            "location": "/user-guide/config/#disabling-modules", 
            "text": "Both default and custom modules can be disabled by setting the key  disabled  to\ntrue inside a module definition:  rexray:\n    storageDrivers:\n    - isilon\n    modules:\n        default-docker:\n            disabled: true\n        isilon2:\n            type:     docker\n        isilon3:\n            type:     docker\n            disabled: true\nisilon:\n    endpoint:   https://172.17.177.230:8080\n    insecure:   true\n    username:   root\n    password:   P@ssword1!\n    volumePath: /rexray/default\n    nfsHost:    172.17.177.230\n    quotas:     true\n    dataSubnet: 172.17.177.0/24  The above example disables the  default-docker  and  isilon3  modules such that isilon2  is the only Docker module loaded.", 
            "title": "Disabling Modules"
        }, 
        {
            "location": "/user-guide/config/#volume-configuration", 
            "text": "This section describes various global configuration options related to\noperations such as mounting and unmounting volumes.", 
            "title": "Volume Configuration"
        }, 
        {
            "location": "/user-guide/config/#preemption", 
            "text": "There is a capability to preemptively detach any existing attachments to other\ninstances before attempting a mount.  This will enable use cases for\navailability where another instance must be able to take control of a volume\nwithout the current owner instance being involved.  The operation is considered\nequivalent to a power off of the existing instance for the device.  Example configuration file follows:  rexray:\n  storageDrivers:\n  - openstack\n  volume:\n    mount:\n      preempt: true\nopenStack:\n  authUrl: https://authUrl:35357/v2.0/\n  username: username\n  password: password\n  tenantName: tenantName\n  regionName: regionName     Driver  Supported      EC2  Yes, no Ubuntu support    Isilon  Not yet    OpenStack  With Cinder v2    ScaleIO  Yes    Rackspace  No    VirtualBox  Yes    VMAX  Not yet    XtremIO  Yes", 
            "title": "Preemption"
        }, 
        {
            "location": "/user-guide/config/#ignore-used-count", 
            "text": "By default accounting takes place during operations that are performed\non  Mount ,  Unmount , and other operations.  This only has impact when running\nas a service through the HTTP/JSON interface since the counts are persisted\nin memory.  The purpose of respecting the  Used Count  is to ensure that a\nvolume is not unmounted until the unmount requests have equaled the mount\nrequests.    In the  Docker  use case if there are multiple containers sharing a volume\non the same host, the the volume will not be unmounted until the last container\nis stopped.    The following setting should only be used if you wish to  disable  this\nfunctionality.  This would make sense if the accounting is being done from\nhigher layers and all unmount operations should proceed without control.  rexray:\n  volume:\n    unmount:\n      ignoreUsedCount: true  Currently a reset of the service will cause the counts to be reset.  This\nwill cause issues if  multiple containers  are sharing a volume.  If you are\nsharing volumes, it is recommended that you reset the service along with the\naccompanying container runtime (if this setting is false) to ensure they are\nsynchronized.", 
            "title": "Ignore Used Count"
        }, 
        {
            "location": "/user-guide/config/#volume-path-disable-cache-032", 
            "text": "In order to minimize the impact to return  Path  requests, a caching\ncapability has been introduced by default. A  List  request will cause the\nreturned volumes and paths to be evaluated and those with active mounts are\nrecorded. Subsequent  Path  requests for volumes that have no recorded mounts\nwill not result in active path lookups. Once the mount counter is initialized or\na  List  operation occurs where a mount is recorded, the volume will be looked\nup for future  Path  operations.  rexray:\n  volume:\n    path:\n      disableCache: true", 
            "title": "Volume Path Disable Cache (0.3.2)"
        }, 
        {
            "location": "/user-guide/config/#volume-root-path-031", 
            "text": "When volumes are mounted there can be an additional path that is specified to\nbe created and passed as the valid mount point.  This is required for certain\napplications that do not want to place data from the root of a mount point. \nThe default is the  /data  path.  If a value is set by  linux.volume.rootPath ,\nthen the default will be overwritten.  If upgrading to 0.3.1 then you can either set this to an empty value, or move\nthe internal directory in your existing volumes to  /data .  rexray:\nlinux:\n  volume:\n    rootPath: /data", 
            "title": "Volume Root Path (0.3.1)"
        }, 
        {
            "location": "/user-guide/config/#volume-filemode-031", 
            "text": "The permissions of the  linux.volume.rootPath  can be set to default values.  At\neach mount, the permissions will be written based on this value.  The default\nis to include the  0700  mode.  rexray:\nlinux:\n  volume:\n    fileMode: 0700", 
            "title": "Volume FileMode (0.3.1)"
        }, 
        {
            "location": "/user-guide/storage-providers/", 
            "text": "Storage Providers\n\n\nConnecting storage and platforms...\n\n\n\n\nOverview\n\n\nThis page reviews the storage providers and platforms supported by \nREX-Ray\n.\n\n\nAmazon EC2\n\n\nThe Amazon EC2 driver registers a storage driver named \nec2\n with the \nREX-Ray\n\ndriver manager and is used to connect and manage storage on EC2 instances. The\nEC2 driver is made possible by the\n\ngoamz project\n.\n\n\nConfiguration\n\n\nThe following is an example configuration of the AWS EC2 driver.\n\n\naws:\n    accessKey: MyAccessKey\n    secretKey: MySecretKey\n    region:    USNW\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nActivating the Driver\n\n\nTo activate the EC2 driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nec2\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with OpenStack.\n\n\nrexray:\n  storageDrivers:\n  - ec2\naws:\n    accessKey: MyAccessKey\n    secretKey: MySecretKey\n\n\n\n\nGoogle Compute Engine\n\n\nThe Google Compute Engine (GCE) registers a storage driver named \ngce\n with the\n\nREX-Ray\n driver manager and is used to connect and manage GCE storage.\n\n\nPrerequisites\n\n\nIn order to leverage the GCE driver, REX-Ray must be located on the\nrunning GCE instance that you wish to receive storage.  There must also\nbe a \njson key\n file for the credentials that can be retrieved from the \nAPI\nportal\n.\n\n\nConfiguration\n\n\nThe following is an example configuration of the GCE driver.\n\n\ngce:\n  keyfile: path_to_json_key\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nActivating the Driver\n\n\nTo activate the GCE driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \ngce\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with GCE.\n\n\nrexray:\n  storageDrivers:\n  - gce\ngce:\n  keyfile: /certdir/cert.json\n\n\n\n\nConfigurable Items\n\n\nThe following items are configurable specific to this driver.\n- \nvolumeTypes\n\n\nIsilon\n\n\nThe Isilon driver registers a storage driver named \nisilon\n with the \nREX-Ray\n\ndriver manager and is used to connect and manage Isilon NAS storage.  The\ndriver creates logical volumes in directories on the Isilon cluster.  Volumes\nare exported via NFS and restricted to a single client at a time.  Quotas can\nalso be used to ensure that a volume directory doesn't exceed a specified size.\n\n\nConfiguration\n\n\nThe following is an example configuration of the Isilon driver.\n\n\nisilon:\n  endpoint: https://endpoint:8080\n  insecure: true\n  username: username\n  group: groupname\n  password: password\n  volumePath: /rexray\n  nfsHost: nfsHost\n  dataSubnet: subnet\n  quotas: true\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nExtra Parameters\n\n\nThe following items are configurable specific to this driver.\n\n\n\n\nvolumePath\n represents the location under \n/ifs/volumes\n to allow volumes to\n   be created and removed.\n\n\nnfsHost\n is the configurable host used when mounting exports\n\n\ndataSubnet\n is the subnet the REX-Ray driver is running on\n\n\n\n\nOptional Parameters\n\n\nThe following items are not required, but available to this driver.\n\n\n\n\ninsecure\n defaults to \nfalse\n.\n\n\ngroup\n defaults to the group of the user specified in the configuration.\n\n   Only use this option if you need volumes to be created with a different\n   group.\n\n\nvolumePath\n defaults to \"\".  This will have all new volumes created directly\n   under \n/ifs/volumes\n.\n\n\nquotas\n defaults to \nfalse\n.  Set to \ntrue\n if you have a SmartQuotas\n   license enabled.\n\n\n\n\nActivating the Driver\n\n\nTo activate the Isilon driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nisilon\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with Isilon.\n\n\nrexray:\n  storageDrivers:\n  - isilon\nisilon:\n  endpoint: https://endpoint:8080\n  insecure: true\n  username: username\n  password: password\n  volumePath: /rexray\n  nfsHost: nfsHost\n  dataSubnet: subnet\n  quotas: true\n\n\n\n\nInstructions\n\n\nIt is expected that the \nvolumePath\n exists already within the Isilon system.\nThis example would reflect a directory create under \n/ifs/volumes/rexray\n for\ncreated volumes.  It is not necessary to export this volume.  The \ndataSubnet\n\nparameter is required so the Isilon driver can restrict access to attached\nvolumes to the host that REX-Ray is running on.\n\n\nIf \nquotas\n are enabled, a SmartQuotas license must also be enabled on the\nIsilon cluster for the capacity size functionality of REX-Ray to work.\n\n\nA SnapshotIQ license must be enabled on the Isilon cluster for the snapshot\nfunctionality of \nREX-Ray\n to work.\n\n\nCaveats\n\n\nThe Isilon driver is not without its caveats:\n\n\n\n\nThe \n--volumeType\n flag is ignored\n\n\nThe account used to access the Isilon cluster must be in a role with the\n  following privileges:\n\n\nNamespace Access (ISI_PRIV_NS_IFS_ACCESS)\n\n\nPlatform API (ISI_PRIV_LOGIN_PAPI)\n\n\nNFS (ISI_PRIV_NFS)\n\n\nRestore (ISI_PRIV_IFS_RESTORE)\n\n\nQuota (ISI_PRIV_QUOTA)          (if \nquotas\n are enabled)\n\n\nSnapshot (ISI_PRIV_SNAPSHOT)    (if snapshots are used)\n\n\n\n\n\n\n\n\nOpenStack\n\n\nThe OpenStack driver registers a storage driver named \nopenstack\n with the\n\nREX-Ray\n driver manager and is used to connect and manage storage on OpenStack\ninstances.\n\n\nConfiguration\n\n\nThe following is an example configuration of the OpenStack driver.\n\n\nopenstack:\n    authURL:              https://domain.com/openstack\n    userID:               0\n    userName:             admin\n    password:             mypassword\n    tenantID:             0\n    tenantName:           customer\n    domainID:             0\n    domainName:           corp\n    regionName:           USNW\n    availabilityZoneName: Gold\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nActivating the Driver\n\n\nTo activate the OpenStack driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nopenstack\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with OpenStack.\n\n\nrexray:\n  storageDrivers:\n  - openstack\nopenstack:\n  authUrl: https://keystoneHost:35357/v2.0/\n  username: username\n  password: password\n  tenantName: tenantName\n  regionName: regionName\n\n\n\n\nRackspace\n\n\nThe Rackspace driver registers a storage driver named \nrackspace\n with the\n\nREX-Ray\n driver manager and is used to connect and manage storage on Rackspace\ninstances.\n\n\nConfiguration\n\n\nThe following is an example configuration of the Rackspace driver.\n\n\nrackspace:\n    authURL:    https://domain.com/rackspace\n    userID:     0\n    userName:   admin\n    password:   mypassword\n    tenantID:   0\n    tenantName: customer\n    domainID:   0\n    domainName: corp\n\n\n\n\nActivating the Driver\n\n\nTo activate the Rackspace driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nrackspace\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with Rackspace.\n\n\nrexray:\n  storageDrivers:\n  - rackspace\nrackspace:\n  authUrl: https://keystoneHost:35357/v2.0/\n  username: username\n  password: password\n  tenantName: tenantName\n  regionName: regionName\n\n\n\n\nScaleIO\n\n\nThe ScaleIO driver registers a storage driver named \nscaleio\n with the \nREX-Ray\n\ndriver manager and is used to connect and manage ScaleIO storage.  The ScaleIO\n\nREST Gateway\n is required for the driver to function.\n\n\nConfiguration\n\n\nThe following is an example with all possible fields configured.  For a running\nexample see the \nExamples\n section.\n\n\nscaleio:\n    endpoint:             https://host_ip/api\n    insecure:             false\n    useCerts:             true\n    userName:             admin\n    password:             mypassword\n    systemID:             0\n    systemName:           sysv\n    protectionDomainID:   0\n    protectionDomainName: corp\n    storagePoolID:        0\n    storagePoolName:      gold\n    thinOrThick:          ThinProvisioned\n\n\n\n\nConfiguration Notes\n\n\n\n\ninsecure\n should be set to \ntrue\n if you have not loaded the SSL\ncertificates on the host.  A successful wget or curl should be possible without\nSSL errors to the API \nendpoint\n in this case.\n\n\nuseCerts\n should only be set if you want to leverage the internal SSL\ncertificates.  This would be useful if you are deploying the REX-Ray binary\non a host that does not have any certificates installed.\n\n\nsystemID\n takes priority over \nsystemName\n.\n\n\nprotectionDomainID\n takes priority over \nprotectionDomainName\n.\n\n\nstoragePoolID\n takes priority over \nstoragePoolName\n.\n\n\nthinkOrThick\n determines whether to provision as the default\n\nThinProvisioned\n, or \nThickProvisioned\n.\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nRuntime Behavior\n\n\nThe \nstorageType\n field that is configured per volume is considered the\nScaleIO Storage Pool.  This can be configured by default with the \nstoragePool\n\nsetting.  It is important that you create unique names for your Storage Pools\non the same ScaleIO platform.  Otherwise, when specifying \nstorageType\n it\nmay choose at random which \nprotectionDomain\n the pool comes from.\n\n\nThe \navailabilityZone\n field represents the ScaleIO Protection Domain.\n\n\nConfiguring the Gateway\n\n\n\n\nInstall the \nEMC-ScaleIO-gateway\n package.\n\n\nEdit the \n/opt/emc/scaleio/gateway/webapps/ROOT/WEB-INF/classes/gatewayUser.properties\n\nfile and append the proper MDM IP addresses to the following \nmdm.ip.addresses=\n\nparameter.\n\n\nBy default the password is the same as your administrative MDM password.\n\n\nStart the gateway \nservice scaleio-gateway start\n.\n\n\nWith 1.32 we have noticed a restart of the gateway may be necessary as well\nafter an initial install with \nservice scaleio-gateway restart\n. \n\n\n\n\nActivating the Driver\n\n\nTo activate the ScaleIO driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nscaleio\n as the driver name.\n\n\nTroubleshooting\n\n\nEnsure that you are able to open a TCP connection to the gateway with the\naddress that you will be supplying below in the \ngateway_ip\n parameter.  For\nexample \ntelnet gateway_ip 443\n should open a successful connection.  Removing\nthe \nEMC-ScaleIO-gateway\n package and reinstalling can force re-creation of\nself-signed certs which may help resolve gateway problems.  Also try restarting\nthe gateway with \nservice scaleio-gateway restart\n.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with ScaleIO.\n\n\nrexray:\n  storageDrivers:\n  - scaleio\nscaleio:\n  endpoint: https://gateway_ip/api\n  insecure: true\n  userName: username\n  password: password\n  systemName: tenantName\n  protectionDomainName: protectionDomainName\n  storagePoolName: storagePoolName\n\n\n\n\nVirtualBox\n\n\nThe VirtualBox driver registers a storage driver named \nvirtualbox\n with the\n\nREX-Ray\n driver manager and is used by VirtualBox VM's to to connect and\nmanage volumes provided by Virtual Box.\n\n\nPrerequisites\n\n\nIn order to leverage the \nvirtualbox\n driver, REX-Ray must be located on each\nVM that you wish to be able to consume external volumes.  The driver\nleverages the \nvboxwebserv\n HTTP SOAP API which is a process that must be\nstarted from the VirtualBox \nhost\n (ie OS X) using \nvboxwebsrv -H 0.0.0.0 -v\n or\nadditionally with \n-b\n for running in the background.  This allows the VMs\nrunning \nREX-Ray\n to remotely make calls to the underlying VirtualBox\napplication.  A test for connectivity can be done with\n\ntelnet virtualboxip 18083\n from the VM.  The \nvirtualboxip\n is what you\nwould put in the \nendpoint\n value.\n\n\nIt is optional to leverage authentication.  The HTTP SOAP API can have\nauthentication disabled by running\n\nVBoxManage setproperty websrvauthlibrary null\n.\n\n\nHot-Plugging is required, which limits the usefulness of this driver to \nSATA\n\nonly.  Ensure that your VM has \npre-created\n this controller and it is\nnamed \nSATA\n.  Otherwise the \ncontrollerName\n field must be populated\nwith the name of the controller you wish to use.  The port count must be set\nmanually as it cannot be increased when the VMs are on.  A count of \n30\n\nis sugggested.\n\n\nVirtualBox 5.0.10+ must be used.\n\n\nConfiguration\n\n\nThe following is an example configuration of the VirtualBox driver.\n\nThe \nlocalMachineNameOrId\n parameter is for development use where you force\nREX-Ray to use a specific VM identity.  Choose a \nvolumePath\n to store the\nvolume files or virtual disks.  This path should be created ahead of time.\n\n\nvirtualbox:\n  endpoint: http://virtualboxhost:18083\n  userName: optional\n  password: optional\n  tls: false\n  volumePath: /Users/your_user/VirtualBox Volumes\n  controllerName: name\n  localMachineNameOrId: forDevelopmentUse\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nActivating the Driver\n\n\nTo activate the VirtualBox driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nvirtualbox\n as the driver name.\n\n\nExamples\n\n\nBelow is a working \nconfig.yml\n file that works with VirtualBox.\n\n\nrexray:\n  storageDrivers:\n  - virtualbox\nvirtualbox:\n  endpoint: http://virtualBoxIP:18083\n  volumePath: /Users/your_user/VirtualBox Volumes\n\n\n\n\nCaveats\n\n\n\n\nSnapshot and create volume from volume functionality is not\n  available yet with this driver.\n\n\nThe driver supports VirtualBox 5.0.10+\n\n\n\n\nVMAX\n\n\nThe VMAX driver registers a storage driver named \nvmax\n with the \nREX-Ray\n\ndriver manager and is used to connect and manage VMAX block storage.\n\n\nThis driver will in the future be used in many scenarios including HBA and\niSCSI.  Right now, the driver is functioning in the \nvmh\n mode.  This means\nthe Volume attachment and detachment is occurring as RDM's to a VM where\n\nREX-Ray\n is running.  This use case enables you to address volumes for\ncontainers as 1st class VMAX volumes while being capable of taking advantage\nof mobility options such as \nvSphere vMotion\n.\n\n\nConfiguration\n\n\nThe following is an example configuration of the Isilon driver.\n\n\nvmax:\n  smisHost: smisHost\n  smisPort: smisPort\n  insecure: true\n  username: admin\n  password: password\n  sid: '000000000000'\n  volumePrefix: \nrexray_\n\n  storageGroup: storageGroup\n  mode: vmh\n  vmh:\n    host: vcenter_or_vm_host\n    username: username\n    password: password\n    insecure: true\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nExtra Parameters\n\n\nThe following items are configurable specific to this driver.\n\n\n\n\nsid\n is your array ID, ensure you enclose it in quotes to preserve it as a\n   string.\n\n\nvolumePrefix\n is an (optional) field that limits visibility and usage of\n   volumes to those only with this prefix, ex. \nrexray_\n.\n\n\nstorageGroup\n is the fields that determines which masking view to use\n   when adding and removing volumes.\n\n\n\n\nActivating the Driver\n\n\nTo activate the VMAX driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nvmax\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with Isilon.\n\n\nrexray:\n  storageDrivers:\n  - vmax\n  logLevel: debug\nvmax:\n  smisHost: smisHost\n  smisPort: smisPort\n  insecure: true\n  username: admin\n  password: password\n  sid: '000000000000'\n  volumePrefix: \nrexray_\n\n  storageGroup: storageGroup\n  mode: vmh\n  vmh:\n    host: vcenter_or_vm_host\n    username: username\n    password: password\n    insecure: true\n\n\n\n\nInstructions\n\n\nFor the \nvmh\n mode, ensure that you have pre-created the storage group as\ndefined by \nstorageGroup\n.  The underlying ESX hosts in the cluster where the VM\ncan move should already be in a masking view that has access to the VMAX ports\nand should be properly logged in.\n\n\nCaveats\n\n\n\n\nThis driver currently ignores the \n--volumeType\n flag.\n\n\nPre-emption is not currently supported.  A mount to an alternate VM will be\n  denied.\n\n\nA max of \n56\n devices is supported per VM.\n\n\n\n\nXtremIO\n\n\nThe XtremIO registers a storage driver named \nxtremio\n with the \nREX-Ray\n\ndriver manager and is used to connect and manage XtremIO storage.\n\n\nConfiguration\n\n\nThe following is an example configuration of the XtremIO driver.\n\n\nxtremio:\n    endpoint:         https://domain.com/xtremio\n    userName:         admin\n    password:         mypassword\n    insecure:         false\n    deviceMapper:     false\n    multipath:        true\n    remoteManagement: false\n\n\n\n\nFor information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are\n\ntransformed\n.\n\n\nActivating the Driver\n\n\nTo activate the XtremIO driver please follow the instructions for\n\nactivating storage drivers\n,\nusing \nxtremio\n as the driver name.\n\n\nExamples\n\n\nBelow is a full \nconfig.yml\n file that works with XtremIO.\n\n\nrexray:\n  storageDrivers:\n  - xtremio\nxtremio:\n  endpoint: endpoint\n  insecure: true\n  username: admin\n  password: password\n  multipath: true\n\n\n\n\nPrerequisites\n\n\nTo use the XtremIO driver, an iSCSI connection between the host and XtremIO\nmust already be established.\n\n\nInstall\n\n\nThe driver currently is built for iSCSI operations with XtremIO.  It is expected\nthat connectivity between the host and XIO has been established.  The following\npackages can be used for this.  \nopen-scsi\n provides the iSCSI connectivity.\n\nmultipath-tools\n enables multi-path behavior and relates to the \nmultipath\n\nflag if installed.\n\n\n\n\napt-get install open-iscsi\n\n\napt-get install multipath-tools\n\n\niscsiadm -m discovery -t st -p 192.168.1.61\n\n\niscsiadm -m node -l\n\n\n\n\nInitiator Group\n\n\nOnce a login has occurred, then you should be able to create a initiator\ngroup for this iSCSI IQN.  You can leverage default naming for the initiator\nand group.", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#storage-providers", 
            "text": "Connecting storage and platforms...", 
            "title": "Storage Providers"
        }, 
        {
            "location": "/user-guide/storage-providers/#overview", 
            "text": "This page reviews the storage providers and platforms supported by  REX-Ray .", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/storage-providers/#amazon-ec2", 
            "text": "The Amazon EC2 driver registers a storage driver named  ec2  with the  REX-Ray \ndriver manager and is used to connect and manage storage on EC2 instances. The\nEC2 driver is made possible by the goamz project .", 
            "title": "Amazon EC2"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration", 
            "text": "The following is an example configuration of the AWS EC2 driver.  aws:\n    accessKey: MyAccessKey\n    secretKey: MySecretKey\n    region:    USNW  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver", 
            "text": "To activate the EC2 driver please follow the instructions for activating storage drivers ,\nusing  ec2  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples", 
            "text": "Below is a full  config.yml  file that works with OpenStack.  rexray:\n  storageDrivers:\n  - ec2\naws:\n    accessKey: MyAccessKey\n    secretKey: MySecretKey", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#google-compute-engine", 
            "text": "The Google Compute Engine (GCE) registers a storage driver named  gce  with the REX-Ray  driver manager and is used to connect and manage GCE storage.", 
            "title": "Google Compute Engine"
        }, 
        {
            "location": "/user-guide/storage-providers/#prerequisites", 
            "text": "In order to leverage the GCE driver, REX-Ray must be located on the\nrunning GCE instance that you wish to receive storage.  There must also\nbe a  json key  file for the credentials that can be retrieved from the  API\nportal .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_1", 
            "text": "The following is an example configuration of the GCE driver.  gce:\n  keyfile: path_to_json_key  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_1", 
            "text": "To activate the GCE driver please follow the instructions for activating storage drivers ,\nusing  gce  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_1", 
            "text": "Below is a full  config.yml  file that works with GCE.  rexray:\n  storageDrivers:\n  - gce\ngce:\n  keyfile: /certdir/cert.json", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#configurable-items", 
            "text": "The following items are configurable specific to this driver.\n-  volumeTypes", 
            "title": "Configurable Items"
        }, 
        {
            "location": "/user-guide/storage-providers/#isilon", 
            "text": "The Isilon driver registers a storage driver named  isilon  with the  REX-Ray \ndriver manager and is used to connect and manage Isilon NAS storage.  The\ndriver creates logical volumes in directories on the Isilon cluster.  Volumes\nare exported via NFS and restricted to a single client at a time.  Quotas can\nalso be used to ensure that a volume directory doesn't exceed a specified size.", 
            "title": "Isilon"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_2", 
            "text": "The following is an example configuration of the Isilon driver.  isilon:\n  endpoint: https://endpoint:8080\n  insecure: true\n  username: username\n  group: groupname\n  password: password\n  volumePath: /rexray\n  nfsHost: nfsHost\n  dataSubnet: subnet\n  quotas: true  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#extra-parameters", 
            "text": "The following items are configurable specific to this driver.   volumePath  represents the location under  /ifs/volumes  to allow volumes to\n   be created and removed.  nfsHost  is the configurable host used when mounting exports  dataSubnet  is the subnet the REX-Ray driver is running on", 
            "title": "Extra Parameters"
        }, 
        {
            "location": "/user-guide/storage-providers/#optional-parameters", 
            "text": "The following items are not required, but available to this driver.   insecure  defaults to  false .  group  defaults to the group of the user specified in the configuration. \n   Only use this option if you need volumes to be created with a different\n   group.  volumePath  defaults to \"\".  This will have all new volumes created directly\n   under  /ifs/volumes .  quotas  defaults to  false .  Set to  true  if you have a SmartQuotas\n   license enabled.", 
            "title": "Optional Parameters"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_2", 
            "text": "To activate the Isilon driver please follow the instructions for activating storage drivers ,\nusing  isilon  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_2", 
            "text": "Below is a full  config.yml  file that works with Isilon.  rexray:\n  storageDrivers:\n  - isilon\nisilon:\n  endpoint: https://endpoint:8080\n  insecure: true\n  username: username\n  password: password\n  volumePath: /rexray\n  nfsHost: nfsHost\n  dataSubnet: subnet\n  quotas: true", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#instructions", 
            "text": "It is expected that the  volumePath  exists already within the Isilon system.\nThis example would reflect a directory create under  /ifs/volumes/rexray  for\ncreated volumes.  It is not necessary to export this volume.  The  dataSubnet \nparameter is required so the Isilon driver can restrict access to attached\nvolumes to the host that REX-Ray is running on.  If  quotas  are enabled, a SmartQuotas license must also be enabled on the\nIsilon cluster for the capacity size functionality of REX-Ray to work.  A SnapshotIQ license must be enabled on the Isilon cluster for the snapshot\nfunctionality of  REX-Ray  to work.", 
            "title": "Instructions"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats", 
            "text": "The Isilon driver is not without its caveats:   The  --volumeType  flag is ignored  The account used to access the Isilon cluster must be in a role with the\n  following privileges:  Namespace Access (ISI_PRIV_NS_IFS_ACCESS)  Platform API (ISI_PRIV_LOGIN_PAPI)  NFS (ISI_PRIV_NFS)  Restore (ISI_PRIV_IFS_RESTORE)  Quota (ISI_PRIV_QUOTA)          (if  quotas  are enabled)  Snapshot (ISI_PRIV_SNAPSHOT)    (if snapshots are used)", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/storage-providers/#openstack", 
            "text": "The OpenStack driver registers a storage driver named  openstack  with the REX-Ray  driver manager and is used to connect and manage storage on OpenStack\ninstances.", 
            "title": "OpenStack"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_3", 
            "text": "The following is an example configuration of the OpenStack driver.  openstack:\n    authURL:              https://domain.com/openstack\n    userID:               0\n    userName:             admin\n    password:             mypassword\n    tenantID:             0\n    tenantName:           customer\n    domainID:             0\n    domainName:           corp\n    regionName:           USNW\n    availabilityZoneName: Gold  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_3", 
            "text": "To activate the OpenStack driver please follow the instructions for activating storage drivers ,\nusing  openstack  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_3", 
            "text": "Below is a full  config.yml  file that works with OpenStack.  rexray:\n  storageDrivers:\n  - openstack\nopenstack:\n  authUrl: https://keystoneHost:35357/v2.0/\n  username: username\n  password: password\n  tenantName: tenantName\n  regionName: regionName", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#rackspace", 
            "text": "The Rackspace driver registers a storage driver named  rackspace  with the REX-Ray  driver manager and is used to connect and manage storage on Rackspace\ninstances.", 
            "title": "Rackspace"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_4", 
            "text": "The following is an example configuration of the Rackspace driver.  rackspace:\n    authURL:    https://domain.com/rackspace\n    userID:     0\n    userName:   admin\n    password:   mypassword\n    tenantID:   0\n    tenantName: customer\n    domainID:   0\n    domainName: corp", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_4", 
            "text": "To activate the Rackspace driver please follow the instructions for activating storage drivers ,\nusing  rackspace  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_4", 
            "text": "Below is a full  config.yml  file that works with Rackspace.  rexray:\n  storageDrivers:\n  - rackspace\nrackspace:\n  authUrl: https://keystoneHost:35357/v2.0/\n  username: username\n  password: password\n  tenantName: tenantName\n  regionName: regionName", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#scaleio", 
            "text": "The ScaleIO driver registers a storage driver named  scaleio  with the  REX-Ray \ndriver manager and is used to connect and manage ScaleIO storage.  The ScaleIO REST Gateway  is required for the driver to function.", 
            "title": "ScaleIO"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_5", 
            "text": "The following is an example with all possible fields configured.  For a running\nexample see the  Examples  section.  scaleio:\n    endpoint:             https://host_ip/api\n    insecure:             false\n    useCerts:             true\n    userName:             admin\n    password:             mypassword\n    systemID:             0\n    systemName:           sysv\n    protectionDomainID:   0\n    protectionDomainName: corp\n    storagePoolID:        0\n    storagePoolName:      gold\n    thinOrThick:          ThinProvisioned", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration-notes", 
            "text": "insecure  should be set to  true  if you have not loaded the SSL\ncertificates on the host.  A successful wget or curl should be possible without\nSSL errors to the API  endpoint  in this case.  useCerts  should only be set if you want to leverage the internal SSL\ncertificates.  This would be useful if you are deploying the REX-Ray binary\non a host that does not have any certificates installed.  systemID  takes priority over  systemName .  protectionDomainID  takes priority over  protectionDomainName .  storagePoolID  takes priority over  storagePoolName .  thinkOrThick  determines whether to provision as the default ThinProvisioned , or  ThickProvisioned .   For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration Notes"
        }, 
        {
            "location": "/user-guide/storage-providers/#runtime-behavior", 
            "text": "The  storageType  field that is configured per volume is considered the\nScaleIO Storage Pool.  This can be configured by default with the  storagePool \nsetting.  It is important that you create unique names for your Storage Pools\non the same ScaleIO platform.  Otherwise, when specifying  storageType  it\nmay choose at random which  protectionDomain  the pool comes from.  The  availabilityZone  field represents the ScaleIO Protection Domain.", 
            "title": "Runtime Behavior"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuring-the-gateway", 
            "text": "Install the  EMC-ScaleIO-gateway  package.  Edit the  /opt/emc/scaleio/gateway/webapps/ROOT/WEB-INF/classes/gatewayUser.properties \nfile and append the proper MDM IP addresses to the following  mdm.ip.addresses= \nparameter.  By default the password is the same as your administrative MDM password.  Start the gateway  service scaleio-gateway start .  With 1.32 we have noticed a restart of the gateway may be necessary as well\nafter an initial install with  service scaleio-gateway restart .", 
            "title": "Configuring the Gateway"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_5", 
            "text": "To activate the ScaleIO driver please follow the instructions for activating storage drivers ,\nusing  scaleio  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#troubleshooting", 
            "text": "Ensure that you are able to open a TCP connection to the gateway with the\naddress that you will be supplying below in the  gateway_ip  parameter.  For\nexample  telnet gateway_ip 443  should open a successful connection.  Removing\nthe  EMC-ScaleIO-gateway  package and reinstalling can force re-creation of\nself-signed certs which may help resolve gateway problems.  Also try restarting\nthe gateway with  service scaleio-gateway restart .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_5", 
            "text": "Below is a full  config.yml  file that works with ScaleIO.  rexray:\n  storageDrivers:\n  - scaleio\nscaleio:\n  endpoint: https://gateway_ip/api\n  insecure: true\n  userName: username\n  password: password\n  systemName: tenantName\n  protectionDomainName: protectionDomainName\n  storagePoolName: storagePoolName", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#virtualbox", 
            "text": "The VirtualBox driver registers a storage driver named  virtualbox  with the REX-Ray  driver manager and is used by VirtualBox VM's to to connect and\nmanage volumes provided by Virtual Box.", 
            "title": "VirtualBox"
        }, 
        {
            "location": "/user-guide/storage-providers/#prerequisites_1", 
            "text": "In order to leverage the  virtualbox  driver, REX-Ray must be located on each\nVM that you wish to be able to consume external volumes.  The driver\nleverages the  vboxwebserv  HTTP SOAP API which is a process that must be\nstarted from the VirtualBox  host  (ie OS X) using  vboxwebsrv -H 0.0.0.0 -v  or\nadditionally with  -b  for running in the background.  This allows the VMs\nrunning  REX-Ray  to remotely make calls to the underlying VirtualBox\napplication.  A test for connectivity can be done with telnet virtualboxip 18083  from the VM.  The  virtualboxip  is what you\nwould put in the  endpoint  value.  It is optional to leverage authentication.  The HTTP SOAP API can have\nauthentication disabled by running VBoxManage setproperty websrvauthlibrary null .  Hot-Plugging is required, which limits the usefulness of this driver to  SATA \nonly.  Ensure that your VM has  pre-created  this controller and it is\nnamed  SATA .  Otherwise the  controllerName  field must be populated\nwith the name of the controller you wish to use.  The port count must be set\nmanually as it cannot be increased when the VMs are on.  A count of  30 \nis sugggested.  VirtualBox 5.0.10+ must be used.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_6", 
            "text": "The following is an example configuration of the VirtualBox driver. \nThe  localMachineNameOrId  parameter is for development use where you force\nREX-Ray to use a specific VM identity.  Choose a  volumePath  to store the\nvolume files or virtual disks.  This path should be created ahead of time.  virtualbox:\n  endpoint: http://virtualboxhost:18083\n  userName: optional\n  password: optional\n  tls: false\n  volumePath: /Users/your_user/VirtualBox Volumes\n  controllerName: name\n  localMachineNameOrId: forDevelopmentUse  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_6", 
            "text": "To activate the VirtualBox driver please follow the instructions for activating storage drivers ,\nusing  virtualbox  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_6", 
            "text": "Below is a working  config.yml  file that works with VirtualBox.  rexray:\n  storageDrivers:\n  - virtualbox\nvirtualbox:\n  endpoint: http://virtualBoxIP:18083\n  volumePath: /Users/your_user/VirtualBox Volumes", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats_1", 
            "text": "Snapshot and create volume from volume functionality is not\n  available yet with this driver.  The driver supports VirtualBox 5.0.10+", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/storage-providers/#vmax", 
            "text": "The VMAX driver registers a storage driver named  vmax  with the  REX-Ray \ndriver manager and is used to connect and manage VMAX block storage.  This driver will in the future be used in many scenarios including HBA and\niSCSI.  Right now, the driver is functioning in the  vmh  mode.  This means\nthe Volume attachment and detachment is occurring as RDM's to a VM where REX-Ray  is running.  This use case enables you to address volumes for\ncontainers as 1st class VMAX volumes while being capable of taking advantage\nof mobility options such as  vSphere vMotion .", 
            "title": "VMAX"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_7", 
            "text": "The following is an example configuration of the Isilon driver.  vmax:\n  smisHost: smisHost\n  smisPort: smisPort\n  insecure: true\n  username: admin\n  password: password\n  sid: '000000000000'\n  volumePrefix:  rexray_ \n  storageGroup: storageGroup\n  mode: vmh\n  vmh:\n    host: vcenter_or_vm_host\n    username: username\n    password: password\n    insecure: true  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#extra-parameters_1", 
            "text": "The following items are configurable specific to this driver.   sid  is your array ID, ensure you enclose it in quotes to preserve it as a\n   string.  volumePrefix  is an (optional) field that limits visibility and usage of\n   volumes to those only with this prefix, ex.  rexray_ .  storageGroup  is the fields that determines which masking view to use\n   when adding and removing volumes.", 
            "title": "Extra Parameters"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_7", 
            "text": "To activate the VMAX driver please follow the instructions for activating storage drivers ,\nusing  vmax  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_7", 
            "text": "Below is a full  config.yml  file that works with Isilon.  rexray:\n  storageDrivers:\n  - vmax\n  logLevel: debug\nvmax:\n  smisHost: smisHost\n  smisPort: smisPort\n  insecure: true\n  username: admin\n  password: password\n  sid: '000000000000'\n  volumePrefix:  rexray_ \n  storageGroup: storageGroup\n  mode: vmh\n  vmh:\n    host: vcenter_or_vm_host\n    username: username\n    password: password\n    insecure: true", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#instructions_1", 
            "text": "For the  vmh  mode, ensure that you have pre-created the storage group as\ndefined by  storageGroup .  The underlying ESX hosts in the cluster where the VM\ncan move should already be in a masking view that has access to the VMAX ports\nand should be properly logged in.", 
            "title": "Instructions"
        }, 
        {
            "location": "/user-guide/storage-providers/#caveats_2", 
            "text": "This driver currently ignores the  --volumeType  flag.  Pre-emption is not currently supported.  A mount to an alternate VM will be\n  denied.  A max of  56  devices is supported per VM.", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/storage-providers/#xtremio", 
            "text": "The XtremIO registers a storage driver named  xtremio  with the  REX-Ray \ndriver manager and is used to connect and manage XtremIO storage.", 
            "title": "XtremIO"
        }, 
        {
            "location": "/user-guide/storage-providers/#configuration_8", 
            "text": "The following is an example configuration of the XtremIO driver.  xtremio:\n    endpoint:         https://domain.com/xtremio\n    userName:         admin\n    password:         mypassword\n    insecure:         false\n    deviceMapper:     false\n    multipath:        true\n    remoteManagement: false  For information on the equivalent environment variable and CLI flag names\nplease see the section on how non top-level configuration properties are transformed .", 
            "title": "Configuration"
        }, 
        {
            "location": "/user-guide/storage-providers/#activating-the-driver_8", 
            "text": "To activate the XtremIO driver please follow the instructions for activating storage drivers ,\nusing  xtremio  as the driver name.", 
            "title": "Activating the Driver"
        }, 
        {
            "location": "/user-guide/storage-providers/#examples_8", 
            "text": "Below is a full  config.yml  file that works with XtremIO.  rexray:\n  storageDrivers:\n  - xtremio\nxtremio:\n  endpoint: endpoint\n  insecure: true\n  username: admin\n  password: password\n  multipath: true", 
            "title": "Examples"
        }, 
        {
            "location": "/user-guide/storage-providers/#prerequisites_2", 
            "text": "To use the XtremIO driver, an iSCSI connection between the host and XtremIO\nmust already be established.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/user-guide/storage-providers/#install", 
            "text": "The driver currently is built for iSCSI operations with XtremIO.  It is expected\nthat connectivity between the host and XIO has been established.  The following\npackages can be used for this.   open-scsi  provides the iSCSI connectivity. multipath-tools  enables multi-path behavior and relates to the  multipath \nflag if installed.   apt-get install open-iscsi  apt-get install multipath-tools  iscsiadm -m discovery -t st -p 192.168.1.61  iscsiadm -m node -l", 
            "title": "Install"
        }, 
        {
            "location": "/user-guide/storage-providers/#initiator-group", 
            "text": "Once a login has occurred, then you should be able to create a initiator\ngroup for this iSCSI IQN.  You can leverage default naming for the initiator\nand group.", 
            "title": "Initiator Group"
        }, 
        {
            "location": "/user-guide/schedulers/", 
            "text": "Schedulers\n\n\nScheduling storage one resource at a time...\n\n\n\n\nOverview\n\n\nThis page reviews the scheduling systems supported by \nREX-Ray\n.\n\n\nDocker\n\n\nREX-Ray\n has a \nDocker Volume Driver\n which is compatible with 1.7+.\n\n\nIt is suggested that you are running \nDocker 1.10.2+\n with \nREX-Ray\n especially\nif you are sharing volumes between containers, or you want interactive\nvolume commands through \ndocker volume\n.\n\n\nExample Configuration\n\n\nBelow is an example \nconfig.yml\n that can be used.  The \nvolume.mount.preempt\n\nis an optional parameter here which enables any host to take control of a\nvolume irrespective of whether other hosts are using the volume.  If this is\nset to \nfalse\n then plugins should ensure \nsafety\n first by locking the\nvolume from to the current owner host. We also specify \ndocker.size\n which will\ncreate all new volumes at the specified size in GB.\n\n\nrexray:\n  storageDrivers:\n  - virtualbox\n  volume:\n    mount:\n      preempt: true\ndocker:\n  size: 1\nvirtualbox:\n  endpoint: http://yourlaptop:18083\n  volumePath: /Users/youruser/VirtualBox Volumes\n  controllerName: SATA\n\n\n\n\nExtra Global Parameters\n\n\nThese are all valid parameters that can be configured for the service.\n\n\n\n\n\n\n\n\nparameter\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ndocker.size\n\n\nSize in GB\n\n\n\n\n\n\ndocker.iops\n\n\nIOPS\n\n\n\n\n\n\ndocker.volumeType\n\n\nType of Volume or Storage Pool\n\n\n\n\n\n\ndocker.fsType\n\n\nType of filesystem for new volumes (ext4/xfs)\n\n\n\n\n\n\ndocker.availabilityZone\n\n\nExtensible parameter per storage driver\n\n\n\n\n\n\nlinux.volume.rootPath\n\n\nThe path within the volume to private mount (/data)\n\n\n\n\n\n\nrexray.volume.mount.preempt\n\n\nForcefully take control of volumes when requested\n\n\n\n\n\n\n\n\nStarting Volume Driver\n\n\nREX-Ray must be running as a service to serve requests from Docker. This can be\ndone by running \nrexray start\n.  Make sure you restart REX-Ray if you make\nconfiguration changes.\n\n\n$ sudo rexray start\nStarting REX-Ray...SUCESS!\n\n  The REX-Ray daemon is now running at PID 18141. To\n  shutdown the daemon execute the following command:\n\n    sudo rexray stop\n\n\n\nFollowing this you can now leverage volumes with Docker.\n\n\nCreating and Using Volumes\n\n\nThere are two ways to interact with volumes. You can use the \ndocker run\n\ncommand in combination with \n--volume-driver\n for new volumes, or\nspecify \n-v volumeName\n by itself for existing volumes. The \n--volumes-from\n\nwill also work when sharing existing volumes with a new container.\n\n\nThe \ndocker volume\n sub-command\nenables complete management to create, remove, and list existing volumes. All\nvolumes are returned from the underlying storage platform.\n\n\n\n\n\n\nRun containers with volumes (1.7+)\n\n\ndocker run -ti --volume-driver=rexray -v test:/test busybox\n\n\n\n\n\n\n\nCreate volume with options (1.8+)\n\n\ndocker volume create --driver=rexray --opt=size=5 --name=test\n\n\n\n\n\n\n\nExtra Volume Create Options\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nsize\n\n\nSize in GB\n\n\n\n\n\n\nIOPS\n\n\nIOPS\n\n\n\n\n\n\nvolumeType\n\n\nType of Volume or Storage Pool\n\n\n\n\n\n\nvolumeName\n\n\nCreate from an existing volume name\n\n\n\n\n\n\nvolumeID\n\n\nCreat from an existing volume ID\n\n\n\n\n\n\nsnapshotName\n\n\nCreate from an existing snapshot name\n\n\n\n\n\n\nsnapshotID\n\n\nCreate from an existing snapshot ID\n\n\n\n\n\n\n\n\nCaveats\n\n\nIf you restart the REX-Ray instance while volumes \nare shared between\nDocker containers\n then problems may arise when stopping one of the containers\nsharing the volume.  It is suggested that you avoid stopping these containers\nat this point until all containers sharing the volumes can be stopped.  This\nwill enable the unmount process to proceed cleanly.\n\n\nMesos\n\n\nIn Mesos the frameworks are responsible for receiving requests from\nconsumers and then proceeding to schedule and manage tasks.  Some frameworks\nare open to run any workload for sustained periods of time (ie. Marathon), and\nothers are use case specific (ie. Cassandra).  Further than this, frameworks can\nreceive requests from other platforms or schedulers instead of consumers such as\nCloud Foundry, Kubernetes, and Swarm.\n\n\nOnce frameworks decide to accept resource offers from Mesos, tasks are launched\nto support workloads.  These tasks eventually make it down to Mesos agents\nto spin up containers.  \n\n\nREX-Ray\n provides the ability for any agent receiving a task to request\nstorage be orchestrated for that task.  \n\n\nThere are two primary methods that \nREX-Ray\n functions with Mesos.  It is up to\nthe framework to determine which is most appropriate.  Mesos (0.26) has two\ncontainerizer options for tasks, \nDocker\n and \nMesos\n.\n\n\nDocker Containerizer with Marathon\n\n\nIf the framework uses the Docker containerizer, it is required that both\n\nDocker\n and \nREX-Ray\n are configured ahead of time and working.  It is best to\nrefer to the \nDocker\n page for more\ninformation.  Once this is configured across all appropriate agents, the\nfollowing is an example of using Marathon to start an application with external\nvolumes.\n\n\n{\n    \nid\n: \nnginx\n,\n    \ncontainer\n: {\n        \ndocker\n: {\n            \nimage\n: \nmillion12/nginx\n,\n            \nnetwork\n: \nBRIDGE\n,\n            \nportMappings\n: [{\n                \ncontainerPort\n: 80,\n                \nhostPort\n: 0,\n                \nprotocol\n: \ntcp\n\n            }],\n            \nparameters\n: [{\n                \nkey\n: \nvolume-driver\n,\n                \nvalue\n: \nrexray\n\n            }, {\n                \nkey\n: \nvolume\n,\n                \nvalue\n: \nnginx-data:/data/www\n\n            }]\n        }\n    },\n    \ncpus\n: 0.2,\n    \nmem\n: 32.0,\n    \ninstances\n: 1\n}\n\n\n\n\nMesos Containerizer with Marathon\n\n\nMesos 0.23+\n includes modules that enables extensibility for different\nportions the architecture.  The \ndvdcli\n and\n\nmesos-module-dvdi\n projects are\nrequired for this method to enable external volume support with the native\ncontainerizer.\n\n\nThe following is a similar example to the one above.  But here we are specifying\nto use the the native containerizer and requesting volumes through The \nenv\n\nsection.\n\n\n{\n  \nid\n: \nhello-play\n,\n  \ncmd\n: \nwhile [ true ] ; do touch /var/lib/rexray/volumes/test12345/hello ; sleep 5 ; done\n,\n  \nmem\n: 32,\n  \ncpus\n: 0.1,\n  \ninstances\n: 1,\n  \nenv\n: {\n    \nDVDI_VOLUME_NAME\n: \ntest12345\n,\n    \nDVDI_VOLUME_DRIVER\n: \nrexray\n,\n    \nDVDI_VOLUME_OPTS\n: \nsize=5,iops=150,volumetype=io1,newfstype=xfs,overwritefs=true\n\n  }\n}\n\n\n\n\nThis example also comes along with a couple of important settings for the\nnative method.  This is a \nconfig.yml\n file that can be used.  In this case we\nare showing a \nvirtualbox\n driver configuration, but you can use anything here.\n\nWe suggest two optional options for the \nmesos-module-dvdi\n.  Setting the\n\nvolume.mount.preempt\n flag ensures any host can preempt control of a volume\nfrom other hosts.  Refer to the \nUser-Guide\n for\nmore information on preempt.  The \nvolume.unmount.ignoreusedcount\n ensures that\n\nmesos-module-dvdi\n is authoritative when it comes to deciding when to unmount\nvolumes.\n\n\nrexray:\n  storageDrivers:\n  - virtualbox\n  volume:\n    mount:\n      preempt: true\n    unmount:\n      ignoreusedcount: true\nvirtualbox:\n  endpoint: http://yourlaptop:18083\n  volumePath: /Users/youruser/VirtualBox Volumes\n  controllerName: SATA", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#schedulers", 
            "text": "Scheduling storage one resource at a time...", 
            "title": "Schedulers"
        }, 
        {
            "location": "/user-guide/schedulers/#overview", 
            "text": "This page reviews the scheduling systems supported by  REX-Ray .", 
            "title": "Overview"
        }, 
        {
            "location": "/user-guide/schedulers/#docker", 
            "text": "REX-Ray  has a  Docker Volume Driver  which is compatible with 1.7+.  It is suggested that you are running  Docker 1.10.2+  with  REX-Ray  especially\nif you are sharing volumes between containers, or you want interactive\nvolume commands through  docker volume .", 
            "title": "Docker"
        }, 
        {
            "location": "/user-guide/schedulers/#example-configuration", 
            "text": "Below is an example  config.yml  that can be used.  The  volume.mount.preempt \nis an optional parameter here which enables any host to take control of a\nvolume irrespective of whether other hosts are using the volume.  If this is\nset to  false  then plugins should ensure  safety  first by locking the\nvolume from to the current owner host. We also specify  docker.size  which will\ncreate all new volumes at the specified size in GB.  rexray:\n  storageDrivers:\n  - virtualbox\n  volume:\n    mount:\n      preempt: true\ndocker:\n  size: 1\nvirtualbox:\n  endpoint: http://yourlaptop:18083\n  volumePath: /Users/youruser/VirtualBox Volumes\n  controllerName: SATA", 
            "title": "Example Configuration"
        }, 
        {
            "location": "/user-guide/schedulers/#extra-global-parameters", 
            "text": "These are all valid parameters that can be configured for the service.     parameter  description      docker.size  Size in GB    docker.iops  IOPS    docker.volumeType  Type of Volume or Storage Pool    docker.fsType  Type of filesystem for new volumes (ext4/xfs)    docker.availabilityZone  Extensible parameter per storage driver    linux.volume.rootPath  The path within the volume to private mount (/data)    rexray.volume.mount.preempt  Forcefully take control of volumes when requested", 
            "title": "Extra Global Parameters"
        }, 
        {
            "location": "/user-guide/schedulers/#starting-volume-driver", 
            "text": "REX-Ray must be running as a service to serve requests from Docker. This can be\ndone by running  rexray start .  Make sure you restart REX-Ray if you make\nconfiguration changes.  $ sudo rexray start\nStarting REX-Ray...SUCESS!\n\n  The REX-Ray daemon is now running at PID 18141. To\n  shutdown the daemon execute the following command:\n\n    sudo rexray stop  Following this you can now leverage volumes with Docker.", 
            "title": "Starting Volume Driver"
        }, 
        {
            "location": "/user-guide/schedulers/#creating-and-using-volumes", 
            "text": "There are two ways to interact with volumes. You can use the  docker run \ncommand in combination with  --volume-driver  for new volumes, or\nspecify  -v volumeName  by itself for existing volumes. The  --volumes-from \nwill also work when sharing existing volumes with a new container.  The  docker volume  sub-command\nenables complete management to create, remove, and list existing volumes. All\nvolumes are returned from the underlying storage platform.    Run containers with volumes (1.7+)  docker run -ti --volume-driver=rexray -v test:/test busybox    Create volume with options (1.8+)  docker volume create --driver=rexray --opt=size=5 --name=test", 
            "title": "Creating and Using Volumes"
        }, 
        {
            "location": "/user-guide/schedulers/#extra-volume-create-options", 
            "text": "option  description      size  Size in GB    IOPS  IOPS    volumeType  Type of Volume or Storage Pool    volumeName  Create from an existing volume name    volumeID  Creat from an existing volume ID    snapshotName  Create from an existing snapshot name    snapshotID  Create from an existing snapshot ID", 
            "title": "Extra Volume Create Options"
        }, 
        {
            "location": "/user-guide/schedulers/#caveats", 
            "text": "If you restart the REX-Ray instance while volumes  are shared between\nDocker containers  then problems may arise when stopping one of the containers\nsharing the volume.  It is suggested that you avoid stopping these containers\nat this point until all containers sharing the volumes can be stopped.  This\nwill enable the unmount process to proceed cleanly.", 
            "title": "Caveats"
        }, 
        {
            "location": "/user-guide/schedulers/#mesos", 
            "text": "In Mesos the frameworks are responsible for receiving requests from\nconsumers and then proceeding to schedule and manage tasks.  Some frameworks\nare open to run any workload for sustained periods of time (ie. Marathon), and\nothers are use case specific (ie. Cassandra).  Further than this, frameworks can\nreceive requests from other platforms or schedulers instead of consumers such as\nCloud Foundry, Kubernetes, and Swarm.  Once frameworks decide to accept resource offers from Mesos, tasks are launched\nto support workloads.  These tasks eventually make it down to Mesos agents\nto spin up containers.    REX-Ray  provides the ability for any agent receiving a task to request\nstorage be orchestrated for that task.    There are two primary methods that  REX-Ray  functions with Mesos.  It is up to\nthe framework to determine which is most appropriate.  Mesos (0.26) has two\ncontainerizer options for tasks,  Docker  and  Mesos .", 
            "title": "Mesos"
        }, 
        {
            "location": "/user-guide/schedulers/#docker-containerizer-with-marathon", 
            "text": "If the framework uses the Docker containerizer, it is required that both Docker  and  REX-Ray  are configured ahead of time and working.  It is best to\nrefer to the  Docker  page for more\ninformation.  Once this is configured across all appropriate agents, the\nfollowing is an example of using Marathon to start an application with external\nvolumes.  {\n     id :  nginx ,\n     container : {\n         docker : {\n             image :  million12/nginx ,\n             network :  BRIDGE ,\n             portMappings : [{\n                 containerPort : 80,\n                 hostPort : 0,\n                 protocol :  tcp \n            }],\n             parameters : [{\n                 key :  volume-driver ,\n                 value :  rexray \n            }, {\n                 key :  volume ,\n                 value :  nginx-data:/data/www \n            }]\n        }\n    },\n     cpus : 0.2,\n     mem : 32.0,\n     instances : 1\n}", 
            "title": "Docker Containerizer with Marathon"
        }, 
        {
            "location": "/user-guide/schedulers/#mesos-containerizer-with-marathon", 
            "text": "Mesos 0.23+  includes modules that enables extensibility for different\nportions the architecture.  The  dvdcli  and mesos-module-dvdi  projects are\nrequired for this method to enable external volume support with the native\ncontainerizer.  The following is a similar example to the one above.  But here we are specifying\nto use the the native containerizer and requesting volumes through The  env \nsection.  {\n   id :  hello-play ,\n   cmd :  while [ true ] ; do touch /var/lib/rexray/volumes/test12345/hello ; sleep 5 ; done ,\n   mem : 32,\n   cpus : 0.1,\n   instances : 1,\n   env : {\n     DVDI_VOLUME_NAME :  test12345 ,\n     DVDI_VOLUME_DRIVER :  rexray ,\n     DVDI_VOLUME_OPTS :  size=5,iops=150,volumetype=io1,newfstype=xfs,overwritefs=true \n  }\n}  This example also comes along with a couple of important settings for the\nnative method.  This is a  config.yml  file that can be used.  In this case we\nare showing a  virtualbox  driver configuration, but you can use anything here. \nWe suggest two optional options for the  mesos-module-dvdi .  Setting the volume.mount.preempt  flag ensures any host can preempt control of a volume\nfrom other hosts.  Refer to the  User-Guide  for\nmore information on preempt.  The  volume.unmount.ignoreusedcount  ensures that mesos-module-dvdi  is authoritative when it comes to deciding when to unmount\nvolumes.  rexray:\n  storageDrivers:\n  - virtualbox\n  volume:\n    mount:\n      preempt: true\n    unmount:\n      ignoreusedcount: true\nvirtualbox:\n  endpoint: http://yourlaptop:18083\n  volumePath: /Users/youruser/VirtualBox Volumes\n  controllerName: SATA", 
            "title": "Mesos Containerizer with Marathon"
        }, 
        {
            "location": "/user-guide/application/", 
            "text": "Applications\n\n\nPersistence for applications in containers.\n\n\n\n\nGetting Started\n\n\nThis tutorial will serve as a generic guide for taking Docker images found on\n\nDocker Hub\n and utilizing persistent external storage\nvia \nREX-Ray\n.  This should provide guidance for certain applications, but also\ngenerically so you can add persistence properly to other applications.\n\n\nInstructions\n\n\nThe following are a set of instructions for investigating an existing container\nimage to determine how to properly apply persistence.\n\n\nThe first step is to determine which application you are looking to deploy, then\nproceed to its \nDocker Hub\n page.  In this example, we\nwill be using \nPostgreSQL on Docker Hub\n.  \n\n\nMost application vendors will post their Dockerfile on the main page for that\ngiven image. Many of them will also make them available by version minimally via\na download or via Github. Continuing with our PostgreSQL example, we will use\nthe \nDockerfile for version 9.3\n\nsince it happens to be the default version provided with Ubuntu 14.04.\n\n\nProperly written Dockerfiles will include the proper information that separates\npersistent information from the container image and deployed container.  This is\nvisible when the author of the \nDockerfile\n includes a \nVOLUME\n statement to\ndefine where stateful information should be held.\n\n\nOpen the \nDockerfile\n and do a search for \nVOLUME\n and take note of the\nvolumes that will be created for this image. Then we can use \nREX-Ray\n or\n\nDocker\n to create the external persistent volume for this image. In the\nPostgreSQL 9.3 example, there is a single volume in the Dockerfile:\n\n\nVOLUME /var/lib/postgresql/data\n\n\n\n\nThe single path or paths listed refer to the volumes that should be attached\nwhen running the container.  Following this you can create a volume and attach\nit to a container with the \n-v\n flag.\n\n\n\n\nPostgreSQL\n  \n\n\n\n\n$ docker volume create --driver=rexray --name=postgresql --opt=size=\nsizeInGB\n\n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres\n\n\n\n\nPopular Applications\n\n\nExternal persistent storage can be applied to any number of applications\nincluding but not limited the following examples.\n\n\n\n\nCassandra\n\n\n\n\nPostgreSQL\n\n\n$ docker volume create --driver=rexray --name=postgresql --opt=size=\nsizeInGB\n\n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres\n\n\n\n\n\n\n\nMariaDB\n\n\n\n\n\n\nMongoDB\n\n\n$ docker volume create --driver=rexray --name=mongodb --opt=size=\nsizeInGB\n\n$ docker run -d --volume-driver=rexray -v mongodb:/data/db mongo\n\n\n\n\n\n\n\nMySQL\n\n\n\n\nRedis\n$ docker volume create --driver=rexray --name=redis --opt=size=\nsizeInGB\n\n$ docker run -d --volume-driver=rexray -v redis:/data redis", 
            "title": "Applications"
        }, 
        {
            "location": "/user-guide/application/#applications", 
            "text": "Persistence for applications in containers.", 
            "title": "Applications"
        }, 
        {
            "location": "/user-guide/application/#getting-started", 
            "text": "This tutorial will serve as a generic guide for taking Docker images found on Docker Hub  and utilizing persistent external storage\nvia  REX-Ray .  This should provide guidance for certain applications, but also\ngenerically so you can add persistence properly to other applications.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/application/#instructions", 
            "text": "The following are a set of instructions for investigating an existing container\nimage to determine how to properly apply persistence.  The first step is to determine which application you are looking to deploy, then\nproceed to its  Docker Hub  page.  In this example, we\nwill be using  PostgreSQL on Docker Hub .    Most application vendors will post their Dockerfile on the main page for that\ngiven image. Many of them will also make them available by version minimally via\na download or via Github. Continuing with our PostgreSQL example, we will use\nthe  Dockerfile for version 9.3 \nsince it happens to be the default version provided with Ubuntu 14.04.  Properly written Dockerfiles will include the proper information that separates\npersistent information from the container image and deployed container.  This is\nvisible when the author of the  Dockerfile  includes a  VOLUME  statement to\ndefine where stateful information should be held.  Open the  Dockerfile  and do a search for  VOLUME  and take note of the\nvolumes that will be created for this image. Then we can use  REX-Ray  or Docker  to create the external persistent volume for this image. In the\nPostgreSQL 9.3 example, there is a single volume in the Dockerfile:  VOLUME /var/lib/postgresql/data  The single path or paths listed refer to the volumes that should be attached\nwhen running the container.  Following this you can create a volume and attach\nit to a container with the  -v  flag.   PostgreSQL      $ docker volume create --driver=rexray --name=postgresql --opt=size= sizeInGB \n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres", 
            "title": "Instructions"
        }, 
        {
            "location": "/user-guide/application/#popular-applications", 
            "text": "External persistent storage can be applied to any number of applications\nincluding but not limited the following examples.   Cassandra   PostgreSQL  $ docker volume create --driver=rexray --name=postgresql --opt=size= sizeInGB \n$ docker run -d -e POSTGRES_PASSWORD=mysecretpassword --volume-driver=rexray \\\n    -v data:/var/lib/postgresql/data postgres    MariaDB    MongoDB  $ docker volume create --driver=rexray --name=mongodb --opt=size= sizeInGB \n$ docker run -d --volume-driver=rexray -v mongodb:/data/db mongo    MySQL   Redis $ docker volume create --driver=rexray --name=redis --opt=size= sizeInGB \n$ docker run -d --volume-driver=rexray -v redis:/data redis", 
            "title": "Popular Applications"
        }, 
        {
            "location": "/dev-guide/project-guidelines/", 
            "text": "Project Guidelines\n\n\nThese are important.\n\n\n\n\nPeople contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.\n\n\nStyle \n Syntax\n\n\nAll source files should be processed by the following tools prior to being\ncommitted. Any errors or warnings produced by the tools should be corrected\nbefore the source is committed.\n\n\n\n\n\n\n\n\nTool\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngofmt\n\n\nA golang source formatting tool\n\n\n\n\n\n\ngolint\n\n\nA golang linter\n\n\n\n\n\n\ngovet\n\n\nA golang source optimization tool\n\n\n\n\n\n\ngocyclo\n\n\nA golang cyclomatic complexity detection tool. No function should have a score above 0.15\n\n\n\n\n\n\n\n\nIf \nAtom\n is your IDE of choice, install the\n\ngo-plus\n package, and it will execute all of\nthe tools above less gocyclo upon saving a file.\n\n\nIn lieu of using Atom as the IDE, the project's \nMakefile\n automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.\n\n\nAnother option is to use a client-side, pre-commit hook to ensure that the\nsources meet the required standards. For example, in the project's \n.git/hooks\n\ndirectory create a file called \npre-commit\n and mark it as executable. Then\npaste the following content inside the file:\n\n\n#!/bin/sh\nmake fmt 1\n /dev/null\n\n\n\n\nThe above script will execute prior to a Git commit operation, prior to even\nthe commit message dialog. The script will invoke the \nMakefile\n's \nfmt\n\ntarget, formatting the sources. If the command returns a non-zero exit code,\nthe commit operation will abort with the error.\n\n\nCode Coverage\n\n\nAll new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.\n\n\nThis project uses\n\nCoveralls\n for code coverage, and\nall pull requests are processed just as a build from \nmaster\n. If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.\n\n\nIt's also possible to test the project locally while outputting the code\ncoverage. On the command line, from the project's root directory, execute the\nfollowing:\n\n\n$ .build/test.sh\nok      github.com/emccode/rexray/rexray/cli    0.039s  coverage: 33.6% of statements\nok      github.com/emccode/rexray/test  0.080s  coverage: 94.0% of statements in github.com/emccode/rexray, github.com/emccode/rexray/core\n...\nok      github.com/emccode/rexray/util  0.024s  coverage: 100.0% of statements\n[0]akutz@pax:rexray$\n\n\n\n\nThe file \ntest.sh\n in the \n.build\n directory is the same script executed during\nthe project's \nautomated build system\n. The only\ndifference is when executed locally the results are not submitted to Coveralls.\nStill, using the \ntest.sh\n file one can easily determine if a package's coverage\nhas decreased and if additional testing is necessary.\n\n\nCommit Messages\n\n\nCommit messages should follow the guide \n5 Useful Tips For a Better Commit\nMessage\n.\nThe two primary rules to which to adhere are:\n\n\n\n\n\n\nCommit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.\n\n\n\n\n\n\nThe commit message's body should not have a width that exceeds 72\n     characters.\n\n\n\n\n\n\nFor example, the following commit has a very useful message that is succinct\nwithout losing utility.\n\n\ncommit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz \nsakutz@gmail.com\n\nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.\n\n\n\n\nPlease note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s\n -s\nAdded --format,-f option for CLI\n\n\n\n\nIt's also equally simple to print the commit's subject and body together:\n\n\n$ git show e80c696939a03f26cd180934ba642a729b0d2941 --format=\n%s%n%n%b\n -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.\n\n\n\n\nSubmitting Changes\n\n\nAll developers are required to follow the\n\nGitHub Flow model\n when\nproposing new features or even submitting fixes.\n\n\nPlease note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a \nfork\n of this project, not from within a\nbranch of this project itself.\n\n\nPull requests submitted to this project should adhere to the following\nguidelines:\n\n\n\n\n\n\nBranches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.\n\n\n\n\n\n\nUnless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#project-guidelines", 
            "text": "These are important.   People contributing code to this project must adhere to the following rules.\nThese standards are in place to keep code clean, consistent, and stable.", 
            "title": "Project Guidelines"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#style-syntax", 
            "text": "All source files should be processed by the following tools prior to being\ncommitted. Any errors or warnings produced by the tools should be corrected\nbefore the source is committed.     Tool  Description      gofmt  A golang source formatting tool    golint  A golang linter    govet  A golang source optimization tool    gocyclo  A golang cyclomatic complexity detection tool. No function should have a score above 0.15     If  Atom  is your IDE of choice, install the go-plus  package, and it will execute all of\nthe tools above less gocyclo upon saving a file.  In lieu of using Atom as the IDE, the project's  Makefile  automatically\nexecutes the above tools as part of the build process and will fail the build\nif problems are discovered.  Another option is to use a client-side, pre-commit hook to ensure that the\nsources meet the required standards. For example, in the project's  .git/hooks \ndirectory create a file called  pre-commit  and mark it as executable. Then\npaste the following content inside the file:  #!/bin/sh\nmake fmt 1  /dev/null  The above script will execute prior to a Git commit operation, prior to even\nthe commit message dialog. The script will invoke the  Makefile 's  fmt \ntarget, formatting the sources. If the command returns a non-zero exit code,\nthe commit operation will abort with the error.", 
            "title": "Style &amp; Syntax"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#code-coverage", 
            "text": "All new work submitted to the project should have associated tests where\napplicable. If there is ever a question of whether or not a test is applicable\nthen the answer is likely yes.  This project uses Coveralls  for code coverage, and\nall pull requests are processed just as a build from  master . If a pull request\ndecreases the project's code coverage, the pull request will be declined until\nsuch time that testing is added or enhanced to compensate.  It's also possible to test the project locally while outputting the code\ncoverage. On the command line, from the project's root directory, execute the\nfollowing:  $ .build/test.sh\nok      github.com/emccode/rexray/rexray/cli    0.039s  coverage: 33.6% of statements\nok      github.com/emccode/rexray/test  0.080s  coverage: 94.0% of statements in github.com/emccode/rexray, github.com/emccode/rexray/core\n...\nok      github.com/emccode/rexray/util  0.024s  coverage: 100.0% of statements\n[0]akutz@pax:rexray$  The file  test.sh  in the  .build  directory is the same script executed during\nthe project's  automated build system . The only\ndifference is when executed locally the results are not submitted to Coveralls.\nStill, using the  test.sh  file one can easily determine if a package's coverage\nhas decreased and if additional testing is necessary.", 
            "title": "Code Coverage"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#commit-messages", 
            "text": "Commit messages should follow the guide  5 Useful Tips For a Better Commit\nMessage .\nThe two primary rules to which to adhere are:    Commit message subjects should not exceed 50 characters in total and\n     should be followed by a blank line.    The commit message's body should not have a width that exceeds 72\n     characters.    For example, the following commit has a very useful message that is succinct\nwithout losing utility.  commit e80c696939a03f26cd180934ba642a729b0d2941\nAuthor: akutz  sakutz@gmail.com \nDate:   Tue Oct 20 23:47:36 2015 -0500\n\n    Added --format,-f option for CLI\n\n    This patch adds the flag '--format' or '-f' for the\n    following CLI commands:\n\n        * adapter instances\n        * device [get]\n        * snapshot [get]\n        * snapshot copy\n        * snapshot create\n        * volume [get]\n        * volume attach\n        * volume create\n        * volume map\n        * volume mount\n        * volume path\n\n    The user can specify either '--format=yml|yaml|json' or\n    '-f yml|yaml|json' in order to influence how the resulting,\n    structured data is marshaled prior to being emitted to the console.  Please note that the output above is the full output for viewing a commit.\nHowever, because the above message adheres to the commit message rules, it's\nquite easy to show just the commit's subject:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s  -s\nAdded --format,-f option for CLI  It's also equally simple to print the commit's subject and body together:  $ git show e80c696939a03f26cd180934ba642a729b0d2941 --format= %s%n%n%b  -s\nAdded --format,-f option for CLI\n\nThis patch adds the flag '--format' or '-f' for the\nfollowing CLI commands:\n\n    * adapter instances\n    * device [get]\n    * snapshot [get]\n    * snapshot copy\n    * snapshot create\n    * volume [get]\n    * volume attach\n    * volume create\n    * volume map\n    * volume mount\n    * volume path\n\nThe user can specify either '--format=yml|yaml|json' or\n'-f yml|yaml|json' in order to influence how the resulting,\nstructured data is marshaled prior to being emitted to the console.", 
            "title": "Commit Messages"
        }, 
        {
            "location": "/dev-guide/project-guidelines/#submitting-changes", 
            "text": "All developers are required to follow the GitHub Flow model  when\nproposing new features or even submitting fixes.  Please note that although not explicitly stated in the referenced GitHub Flow\nmodel, all work should occur on a  fork  of this project, not from within a\nbranch of this project itself.  Pull requests submitted to this project should adhere to the following\nguidelines:    Branches should be rebased off of the upstream master prior to being\n    opened as pull requests and again prior to merge. This is to ensure that\n    the build system accounts for any changes that may only be detected during\n    the build and test phase.    Unless granted an exception a pull request should contain only a single\n    commit. This is because features and patches should be atomic -- wholly\n    shippable items that are either included in a release, or not. Please\n    squash commits on a branch before opening a pull request. It is not a\n    deal-breaker otherwise, but please be prepared to add a comment or\n    explanation as to why you feel multiple commits are required.", 
            "title": "Submitting Changes"
        }, 
        {
            "location": "/dev-guide/build-reference/", 
            "text": "Build Reference\n\n\nHow to build REX-Ray\n\n\n\n\nBuild Requirements\n\n\nThis project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to\n\nbuild\n \nREX-Ray\n, not run it.\n\n\n\n\n\n\n\n\nRequirement\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nOperating System\n\n\nLinux, OS X\n\n\n\n\n\n\nGo\n\n\n=1.5\n\n\n\n\n\n\nGNU Make\n\n\n=3.80\n\n\n\n\n\n\n\n\nOS X ships with a very old version of GNU Make, and a package manager like\n\nHomebrew\n can be used to install the required version.\n\n\nCross-Compilation\n\n\nThis project's \nMakefile\n\nis configured by default to cross-compile for Linux x86 \n x86_64 as well as\nDarwin (OS X) x86_64. Therefore the build process will fail if the local Go\nenvironment is not set up for cross-compilation. Please take a minute to read\nthis \nblog post\n\nregarding cross-compilation with Go \n=1.5.\n\n\nBuild Binary\n\n\nBuilding from source is pretty simple as all steps, including fetching\ndependencies (as well as fetching the tool that fetches dependencies), are\nconfigured as part of the included \nMakefile\n.\n\n\nBasic Build\n\n\nThe most basic build can be achieved by simply typing \nmake\n from the project's\nroot directory. For what it's worth, executing \nmake\n sans arguments is the\nsame as executing \nmake install\n for this project's \nMakefile\n.\n\n\n$ make\nSemVer: 0.3.1-rc1+8+dirty\nRpmVer: 0.3.1+rc1+8+dirty\nBranch: feature/dev-guide\nCommit: ea59957557d01725ec147d5f71ce6a30aa8698e9\nFormed: Wed, 16 Dec 2015 14:58:33 CST\n\ntarget: deps\n  ...installing glide...SUCCESS!\n  ...downloading go dependencies...SUCCESS!\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: install\n  ...installing rexray Darwin-x86_64...SUCCESS!\n\nThe REX-Ray binary is 62MB and located at:\n\n  /Users/akutz/Projects/go/bin/rexray\n\n\n\n\nBinary Size\n\n\nPlease note that the extraordinary size of the binary is due to the Isilon\nstorage adapter's dependency on the\n\nVMware VMOMI library for Go\n. The types\ndefinition source in that package compiles to a 45MB archive. We're currently\nworking to figure out an alternative to this, even if it means creating VMware\nSOAP messages from scratch.\n\n\nBuild All\n\n\nIn order to build all versions of the binary type the following:\n\n\n$ make build-all\nSemVer: 0.3.1-rc1+8+dirty\nRpmVer: 0.3.1+rc1+8+dirty\nBranch: feature/dev-guide\nCommit: ea59957557d01725ec147d5f71ce6a30aa8698e9\nFormed: Wed, 16 Dec 2015 15:03:27 CST\n\ntarget: deps\n  ...installing glide...SUCCESS!\n  ...downloading go dependencies...SUCCESS!\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: build\n  ...building rexray Linux-i386...SUCCESS!\n\nThe REX-Ray binary is 53MB and located at:\n\n  .build/bin/Linux-i386/rexray\n\ntarget: build\n  ...building rexray Linux-x86_64...SUCCESS!\n\nThe REX-Ray binary is 62MB and located at:\n\n  .build/bin/Linux-x86_64/rexray\n\ntarget: build\n  ...building rexray Darwin-x86_64...SUCCESS!\n\nThe REX-Ray binary is 62MB and located at:\n\n  .build/bin/Darwin-x86_64/rexray\n\n\n\n\nBuild Packages\n\n\nThe \nMakefile\n also includes targets that assist in the creation of\ndistributable packages using the produced artifact.\n\n\nBuild Tarballs\n\n\nThe \nMakefile\n's \nbuild\n and \nbuild-all\n targets will not only build the binary\nin place, but it will also compress the binary as a tarball so it's ready for\ndeployment. For example, after the \nmake build-all\n above, this is the contents\nof the directory \n.build/deploy\n:\n\n\n$ ls .build/deploy\nDarwin-x86_64/\nLinux-i386/\nLinux-x86_64/\nlatest/\n\n\n\n\nLooking inside the directory \n.build/deploy/Linux-x86_64\n reveals:\n\n\n$ ls .build/deploy/Linux-x86_64/\nrexray-Linux-x86_64-0.3.1-rc1+8+dirty.tar.gz\n\n\n\n\nBuild RPMs\n\n\nThe \nMakefile\n's \nrpm-all\n target will package the binary as\narchitecture-specific RPMs when executed on a system that supports the RPM\ndevelopment environment:\n\n\n$ make rpm-all\ntarget: rpm\n  ...building rpm i386...SUCCESS!\n\nThe REX-Ray RPM is 6MB and located at:\n\n  .build/deploy/Linux-i386/rexray-0.3.1+rc1+8+dirty-1.i386.rpm\n\ntarget: rpm\n  ...building rpm x86_64...SUCCESS!\n\nThe REX-Ray RPM is 7MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray-0.3.1+rc1+8+dirty-1.x86_64.rpm\n\n\n\n\nBuild DEBs\n\n\nThe \nMakefile\n's \ndeb-all\n target will package the binary as\narchitecture-specific DEBs when executed on a system that supports the Alien\nRPM-to-DEB conversion tools:\n\n\n$ make deb-all\ntarget: deb\n  ...building deb x86_64...SUCCESS!\n\nThe REX-Ray DEB is 4MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray_0.3.1+rc1+8+dirty-1_amd64.deb\n\n\n\n\nVersion File\n\n\nThere is a file at the root of the project named \nVERSION\n. The file contains\na single line with the \ntarget\n version of the project in the file. The version\nfollows the format:\n\n\n(?\nmajor\n\\d+)\\.(?\nminor\n\\d+)\\.(?\npatch\n\\d+)(-rc\\d+)?\n\n\nFor example, during active development of version \n0.4.0\n the file would\ncontain the version \n0.4.0\n. When it's time to create \n0.4.0\n's first\nrelease candidate the version in the file will be changed to \n0.4.0-rc1\n. And\nwhen it's time to release \n0.4.0\n the version is changed back to \n0.4.0\n.\n\n\nPlease note that we've discussed making the actively developed version the\ntargeted version with a \n-dev\n suffix, but trying this resulted in confusion\nfor the RPM and DEB package managers when using \nunstable\n releases.\n\n\nSo what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the \nVERSION\n file in fact has two purposes:\n\n\n\n\n\n\nFirst and foremost updating the \nVERSION\n file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of \nmaster\n would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the \nVERSION\n file is much cleaner.\n\n\n\n\n\n\nThe contents of the \nVERSION\n file are also used during the build process\n     as a means of overriding the output of a \ngit describe\n. This enables the\n     semantic version injected into the produced binary to be created using\n     the \ntargeted\n version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-reference", 
            "text": "How to build REX-Ray", 
            "title": "Build Reference"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-requirements", 
            "text": "This project has very few build requirements, but there are still one or two\nitems of which to be aware. Also, please note that this are the requirements to build   REX-Ray , not run it.     Requirement  Version      Operating System  Linux, OS X    Go  =1.5    GNU Make  =3.80     OS X ships with a very old version of GNU Make, and a package manager like Homebrew  can be used to install the required version.", 
            "title": "Build Requirements"
        }, 
        {
            "location": "/dev-guide/build-reference/#cross-compilation", 
            "text": "This project's  Makefile \nis configured by default to cross-compile for Linux x86   x86_64 as well as\nDarwin (OS X) x86_64. Therefore the build process will fail if the local Go\nenvironment is not set up for cross-compilation. Please take a minute to read\nthis  blog post \nregarding cross-compilation with Go  =1.5.", 
            "title": "Cross-Compilation"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-binary", 
            "text": "Building from source is pretty simple as all steps, including fetching\ndependencies (as well as fetching the tool that fetches dependencies), are\nconfigured as part of the included  Makefile .", 
            "title": "Build Binary"
        }, 
        {
            "location": "/dev-guide/build-reference/#basic-build", 
            "text": "The most basic build can be achieved by simply typing  make  from the project's\nroot directory. For what it's worth, executing  make  sans arguments is the\nsame as executing  make install  for this project's  Makefile .  $ make\nSemVer: 0.3.1-rc1+8+dirty\nRpmVer: 0.3.1+rc1+8+dirty\nBranch: feature/dev-guide\nCommit: ea59957557d01725ec147d5f71ce6a30aa8698e9\nFormed: Wed, 16 Dec 2015 14:58:33 CST\n\ntarget: deps\n  ...installing glide...SUCCESS!\n  ...downloading go dependencies...SUCCESS!\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: install\n  ...installing rexray Darwin-x86_64...SUCCESS!\n\nThe REX-Ray binary is 62MB and located at:\n\n  /Users/akutz/Projects/go/bin/rexray", 
            "title": "Basic Build"
        }, 
        {
            "location": "/dev-guide/build-reference/#binary-size", 
            "text": "Please note that the extraordinary size of the binary is due to the Isilon\nstorage adapter's dependency on the VMware VMOMI library for Go . The types\ndefinition source in that package compiles to a 45MB archive. We're currently\nworking to figure out an alternative to this, even if it means creating VMware\nSOAP messages from scratch.", 
            "title": "Binary Size"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-all", 
            "text": "In order to build all versions of the binary type the following:  $ make build-all\nSemVer: 0.3.1-rc1+8+dirty\nRpmVer: 0.3.1+rc1+8+dirty\nBranch: feature/dev-guide\nCommit: ea59957557d01725ec147d5f71ce6a30aa8698e9\nFormed: Wed, 16 Dec 2015 15:03:27 CST\n\ntarget: deps\n  ...installing glide...SUCCESS!\n  ...downloading go dependencies...SUCCESS!\ntarget: fmt\n  ...formatting rexray...SUCCESS!\ntarget: build\n  ...building rexray Linux-i386...SUCCESS!\n\nThe REX-Ray binary is 53MB and located at:\n\n  .build/bin/Linux-i386/rexray\n\ntarget: build\n  ...building rexray Linux-x86_64...SUCCESS!\n\nThe REX-Ray binary is 62MB and located at:\n\n  .build/bin/Linux-x86_64/rexray\n\ntarget: build\n  ...building rexray Darwin-x86_64...SUCCESS!\n\nThe REX-Ray binary is 62MB and located at:\n\n  .build/bin/Darwin-x86_64/rexray", 
            "title": "Build All"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-packages", 
            "text": "The  Makefile  also includes targets that assist in the creation of\ndistributable packages using the produced artifact.", 
            "title": "Build Packages"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-tarballs", 
            "text": "The  Makefile 's  build  and  build-all  targets will not only build the binary\nin place, but it will also compress the binary as a tarball so it's ready for\ndeployment. For example, after the  make build-all  above, this is the contents\nof the directory  .build/deploy :  $ ls .build/deploy\nDarwin-x86_64/\nLinux-i386/\nLinux-x86_64/\nlatest/  Looking inside the directory  .build/deploy/Linux-x86_64  reveals:  $ ls .build/deploy/Linux-x86_64/\nrexray-Linux-x86_64-0.3.1-rc1+8+dirty.tar.gz", 
            "title": "Build Tarballs"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-rpms", 
            "text": "The  Makefile 's  rpm-all  target will package the binary as\narchitecture-specific RPMs when executed on a system that supports the RPM\ndevelopment environment:  $ make rpm-all\ntarget: rpm\n  ...building rpm i386...SUCCESS!\n\nThe REX-Ray RPM is 6MB and located at:\n\n  .build/deploy/Linux-i386/rexray-0.3.1+rc1+8+dirty-1.i386.rpm\n\ntarget: rpm\n  ...building rpm x86_64...SUCCESS!\n\nThe REX-Ray RPM is 7MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray-0.3.1+rc1+8+dirty-1.x86_64.rpm", 
            "title": "Build RPMs"
        }, 
        {
            "location": "/dev-guide/build-reference/#build-debs", 
            "text": "The  Makefile 's  deb-all  target will package the binary as\narchitecture-specific DEBs when executed on a system that supports the Alien\nRPM-to-DEB conversion tools:  $ make deb-all\ntarget: deb\n  ...building deb x86_64...SUCCESS!\n\nThe REX-Ray DEB is 4MB and located at:\n\n  .build/deploy/Linux-x86_64/rexray_0.3.1+rc1+8+dirty-1_amd64.deb", 
            "title": "Build DEBs"
        }, 
        {
            "location": "/dev-guide/build-reference/#version-file", 
            "text": "There is a file at the root of the project named  VERSION . The file contains\na single line with the  target  version of the project in the file. The version\nfollows the format:  (? major \\d+)\\.(? minor \\d+)\\.(? patch \\d+)(-rc\\d+)?  For example, during active development of version  0.4.0  the file would\ncontain the version  0.4.0 . When it's time to create  0.4.0 's first\nrelease candidate the version in the file will be changed to  0.4.0-rc1 . And\nwhen it's time to release  0.4.0  the version is changed back to  0.4.0 .  Please note that we've discussed making the actively developed version the\ntargeted version with a  -dev  suffix, but trying this resulted in confusion\nfor the RPM and DEB package managers when using  unstable  releases.  So what's the point of the file if it's basically duplicating the utility of a\ntag? Well, the  VERSION  file in fact has two purposes:    First and foremost updating the  VERSION  file with the same value as that\n     of the tag used to create a release provides a single, contextual reason to\n     push a commit and tag. Otherwise some random commit off of  master  would\n     be tagged as a release candidate or release. Always using the commit that\n     is related to updating the  VERSION  file is much cleaner.    The contents of the  VERSION  file are also used during the build process\n     as a means of overriding the output of a  git describe . This enables the\n     semantic version injected into the produced binary to be created using\n     the  targeted  version of the next release and not just the value of the\n     last, tagged commit.", 
            "title": "Version File"
        }, 
        {
            "location": "/dev-guide/release-process/", 
            "text": "Release Process\n\n\nHow to release REX-Ray\n\n\n\n\nProject Stages\n\n\nThis project has three parallels stages of release:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nunstable\n\n\nThe tip or HEAD of the \nmaster\n branch is referred to as \nunstable\n\n\n\n\n\n\nstaged\n\n\nA commit tagged with the suffix \n-rc\\d+\n such as \nv0.3.1-rc2\n is a \nstaged\n release. These are release candidates.\n\n\n\n\n\n\nstable\n\n\nA commit tagged with a version sans \n-rc\\d+\n suffix such as \nv0.3.1\n is a \nstable\n release.\n\n\n\n\n\n\n\n\nThere are no steps necessary to create an \nunstable\n release as that happens\nautomatically whenever an untagged commit is pushed to \nmaster\n. However, the\nfollowing workflow should be used when tagging a \nstaged\n release candidate\nor \nstable\n release.\n\n\n\n\nReview outstanding issues \n pull requests\n\n\nPrepare release notes\n\n\nUpdate the version file\n\n\nCommit \n pull request\n\n\nTag the release\n\n\nUpdate the version file (again)\n\n\n\n\nReview Issues \n Pull Requests\n\n\nThe first step to a release is to review the outstanding\n\nissues\n and\n\npull requests\n that are tagged for\nthe release in question.\n\n\nIf there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.\n\n\nIt is \nhighly\n recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of \nmaster\n. Remember, while\nGitHub will update a pull request as in conflict if a change to \nmaster\n\nresults in a merge conflict with the pull request, GitHub will \nnot\n force a\nnew build to spawn unless the pull request is actually updated.\n\n\nAt the very minimum a pull request's build should be re-executed prior to the\npull request being merged if \nmaster\n has changed since the pull request was\nopened.\n\n\nPrepare Release Notes\n\n\nUpdate the release notes at \n.docs/about/release-notes.md\n. This file is\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.\n\n\nThe most recent, \nstable\n version of the release notes are always available\nonline at\n\nREX-Ray's documentation site\n.\n\n\nUpdate Version File\n\n\nThe \nVERSION\n file exists at the root of the project and should be updated to\nreflect the value of the intended release.\n\n\nFor example, if creating the first release candidate for version 0.3.1, the\ncontents of the \nVERSION\n file should be a single line \n0.3.1-rc1\n followed by\na newline character:\n\n\n$ cat VERSION\n0.3.1-rc1\n\n\n\n\nIf releasing version 0.3.1 proper then the contents of the \nVERSION\n file\nshould be \n0.3.1\n followed by a newline character:\n\n\n$ cat VERSION\n0.3.1\n\n\n\n\nCommit \n Pull Request\n\n\nOnce all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.\n\n\nPlease make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.\n\n\nA release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:\n\n\nRelease (Candidate) v0.3.1-rc1\n\nThis patch bumps the version to v0.3.1-rc1.\n\n\n\n\nIf the commit message is longer it should simply reflect the same information\nfrom the release notes.\n\n\nOnce committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.\n\n\nTag the Release\n\n\nOnce the pull request marking the \nstaged\n or \nstable\n release has been merged\ninto \nupstream\n's \nmaster\n it's time to tag the release.\n\n\nTag Format\n\n\nThe release tag should follow a prescribed format depending upon the release\ntype:\n\n\n\n\n\n\n\n\nRelease Type\n\n\nTag Format\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nstaged\n\n\nvMAJOR.MINOR.PATCH-rc[0-9]\n\n\nv0.3.1-rc1\n\n\n\n\n\n\nstable\n\n\nvMAJOR.MINOR-PATCH\n\n\nv0.3.1\n\n\n\n\n\n\n\n\nTag Methods\n\n\nThere are two ways to tag a release:\n\n\n\n\nGitHub Releases\n\n\nCommand Line\n\n\n\n\nCommand Line\n\n\nIf tagging a release via the command line be sure to fetch the latest changes\nfrom \nupstream\n's \nmaster\n and either merge them into your local copy of\n\nmaster\n or reset the local copy to reflect \nupstream\n prior to creating\nany tags.\n\n\nThe following combination of commands can be used to create a tag for\n0.3.1 Release Candidate 1:\n\n\ngit fetch upstream \n \\\n  git checkout master \n \\\n  git reset --hard upstream/master \n \\\n  git tag -a -m v0.3.1-rc1 v0.3.1-rc1\n\n\n\n\nThe above example combines a few operations:\n\n\n\n\nThe first command fetches the \nupstream\n changes\n\n\nThe local \nmaster\n branch is checked out\n\n\nThe local \nmaster\n branch is hard reset to \nupstream/master\n\n\nAn annotated tag is created on \nmaster\n for \nv0.3.1-rc1\n, or 0.3.1 Release\n     Candidate 1, with a tag message of \nv0.3.1-rc1\n.\n\n\n\n\nPlease note that the third step will erase any changes that exist only in the\nlocal \nmaster\n branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.\n\n\nThe above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a \nstaged\n or\n\nstable\n release. For \nstable\n releases the project's documentation will also be\nupdated.\n\n\nOnce positive everything looks good simply execute the following command to\npush the tag to the \nupstream\n repository:\n\n\ngit push upstream v0.3.1-rc1\n\n\n\n\nUpdate Version File (Again)\n\n\nAfter a release is tagged there is one final step involving the \nVERSION\n file.\nThe contents of the file should be updated to reflect the next, targeted release\nso that the produced artifacts reflect the targeted version value and not a\nvalue based on the last, tagged commit.\n\n\nFollowing the above examples where version \nv0.3.1-rc1\n was just staged, the\n\nVERSION\n file should be updated to indicate that 0.3.1 Release Candidate 2\n(\n0.3.1-rc2\n) is the next, targeted release:\n\n\n$ cat VERSION\n0.3.1-rc2\n\n\n\n\nCommit the change to the \nVERSION\n file with a commit message similar to the\nfollowing:\n\n\nBumped active dev version to v0.3.1-rc2\n\nThis patch bumps the active dev version to v0.3.1-rc2.\n\n\n\n\nOnce the \nVERSION\n file change is committed, push the change and open a pull \nrequest to merge into the project.", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#release-process", 
            "text": "How to release REX-Ray", 
            "title": "Release Process"
        }, 
        {
            "location": "/dev-guide/release-process/#project-stages", 
            "text": "This project has three parallels stages of release:     Name  Description      unstable  The tip or HEAD of the  master  branch is referred to as  unstable    staged  A commit tagged with the suffix  -rc\\d+  such as  v0.3.1-rc2  is a  staged  release. These are release candidates.    stable  A commit tagged with a version sans  -rc\\d+  suffix such as  v0.3.1  is a  stable  release.     There are no steps necessary to create an  unstable  release as that happens\nautomatically whenever an untagged commit is pushed to  master . However, the\nfollowing workflow should be used when tagging a  staged  release candidate\nor  stable  release.   Review outstanding issues   pull requests  Prepare release notes  Update the version file  Commit   pull request  Tag the release  Update the version file (again)", 
            "title": "Project Stages"
        }, 
        {
            "location": "/dev-guide/release-process/#review-issues-pull-requests", 
            "text": "The first step to a release is to review the outstanding issues  and pull requests  that are tagged for\nthe release in question.  If there are outstanding issues requiring changes or pending pull requests to\nbe merged, handle those prior to tagging any commit as a release candidate or\nrelease.  It is  highly  recommended that pull requests be merged synchronously after\nrebasing each subsequent one off of the new tip of  master . Remember, while\nGitHub will update a pull request as in conflict if a change to  master \nresults in a merge conflict with the pull request, GitHub will  not  force a\nnew build to spawn unless the pull request is actually updated.  At the very minimum a pull request's build should be re-executed prior to the\npull request being merged if  master  has changed since the pull request was\nopened.", 
            "title": "Review Issues &amp; Pull Requests"
        }, 
        {
            "location": "/dev-guide/release-process/#prepare-release-notes", 
            "text": "Update the release notes at  .docs/about/release-notes.md . This file is\nproject's authoritative changelog and should reflect new features, fixes, and\nany significant changes.  The most recent,  stable  version of the release notes are always available\nonline at REX-Ray's documentation site .", 
            "title": "Prepare Release Notes"
        }, 
        {
            "location": "/dev-guide/release-process/#update-version-file", 
            "text": "The  VERSION  file exists at the root of the project and should be updated to\nreflect the value of the intended release.  For example, if creating the first release candidate for version 0.3.1, the\ncontents of the  VERSION  file should be a single line  0.3.1-rc1  followed by\na newline character:  $ cat VERSION\n0.3.1-rc1  If releasing version 0.3.1 proper then the contents of the  VERSION  file\nshould be  0.3.1  followed by a newline character:  $ cat VERSION\n0.3.1", 
            "title": "Update Version File"
        }, 
        {
            "location": "/dev-guide/release-process/#commit-pull-request", 
            "text": "Once all outstanding issues and pull requests are handled, the release notes\nand version are updated, it's time to create a commit.  Please make sure that the changes to the release notes and version files are\na part of the same commit. This makes identifying the aspects of a release,\nstaged or otherwise, far easier for future developers.  A release's commit message can either be a reflection of the release notes or\nsomething simple. Either way the commit message should have the following\nsubject format and first line in its body:  Release (Candidate) v0.3.1-rc1\n\nThis patch bumps the version to v0.3.1-rc1.  If the commit message is longer it should simply reflect the same information\nfrom the release notes.  Once committed push the change to a fork and open a pull request. Even though\nthis commit marks a staged or official release, the pull request system is still\nused to assure that the build completes successfully and there are no unforeseen\nerrors.", 
            "title": "Commit &amp; Pull Request"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-the-release", 
            "text": "Once the pull request marking the  staged  or  stable  release has been merged\ninto  upstream 's  master  it's time to tag the release.", 
            "title": "Tag the Release"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-format", 
            "text": "The release tag should follow a prescribed format depending upon the release\ntype:     Release Type  Tag Format  Example      staged  vMAJOR.MINOR.PATCH-rc[0-9]  v0.3.1-rc1    stable  vMAJOR.MINOR-PATCH  v0.3.1", 
            "title": "Tag Format"
        }, 
        {
            "location": "/dev-guide/release-process/#tag-methods", 
            "text": "There are two ways to tag a release:   GitHub Releases  Command Line", 
            "title": "Tag Methods"
        }, 
        {
            "location": "/dev-guide/release-process/#command-line", 
            "text": "If tagging a release via the command line be sure to fetch the latest changes\nfrom  upstream 's  master  and either merge them into your local copy of master  or reset the local copy to reflect  upstream  prior to creating\nany tags.  The following combination of commands can be used to create a tag for\n0.3.1 Release Candidate 1:  git fetch upstream   \\\n  git checkout master   \\\n  git reset --hard upstream/master   \\\n  git tag -a -m v0.3.1-rc1 v0.3.1-rc1  The above example combines a few operations:   The first command fetches the  upstream  changes  The local  master  branch is checked out  The local  master  branch is hard reset to  upstream/master  An annotated tag is created on  master  for  v0.3.1-rc1 , or 0.3.1 Release\n     Candidate 1, with a tag message of  v0.3.1-rc1 .   Please note that the third step will erase any changes that exist only in the\nlocal  master  branch that do not also exist in the remote, upstream copy.\nHowever, if the two branches are not equal this method should not be used to\ncreate a tag anyway.  The above steps do not actually push the tag upstream. This is to allow for one\nfinal review of all the changes before doing so since the appearance of a new,\nannotated tag in the repository will cause the project's build system to\nautomatically kick off a build that will result in the release of a  staged  or stable  release. For  stable  releases the project's documentation will also be\nupdated.  Once positive everything looks good simply execute the following command to\npush the tag to the  upstream  repository:  git push upstream v0.3.1-rc1", 
            "title": "Command Line"
        }, 
        {
            "location": "/dev-guide/release-process/#update-version-file-again", 
            "text": "After a release is tagged there is one final step involving the  VERSION  file.\nThe contents of the file should be updated to reflect the next, targeted release\nso that the produced artifacts reflect the targeted version value and not a\nvalue based on the last, tagged commit.  Following the above examples where version  v0.3.1-rc1  was just staged, the VERSION  file should be updated to indicate that 0.3.1 Release Candidate 2\n( 0.3.1-rc2 ) is the next, targeted release:  $ cat VERSION\n0.3.1-rc2  Commit the change to the  VERSION  file with a commit message similar to the\nfollowing:  Bumped active dev version to v0.3.1-rc2\n\nThis patch bumps the active dev version to v0.3.1-rc2.  Once the  VERSION  file change is committed, push the change and open a pull \nrequest to merge into the project.", 
            "title": "Update Version File (Again)"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing to REX-Ray\n\n\nAn introduction to contributing to the REX-Ray project\n\n\n\n\nThe REX-Ray project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:\n\n\n\n\nCode patches via pull requests\n\n\nDocumentation improvements\n\n\nBug reports and patch reviews\n\n\nOS, Storage, and Volume Drivers\n\n\nA distributed server/client model with profile support\n\n\n\n\nReporting an Issue\n\n\nPlease include as much detail as you can. This includes:\n\n\n\n\nThe OS type and version\n\n\nThe REX-Ray version\n\n\nThe storage system in question\n\n\nA set of logs with debug-logging enabled that show the problem\n\n\n\n\nTesting the Development Version\n\n\nIf you want to just install and try out the latest development version of\nREX-Ray you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master. It is \nstrongly\n recommended\nthat you do this within a [virtualenv].\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s unstable\n\n\n\n\nInstalling for Development\n\n\nFirst you'll need to fork and clone the repository. Once you have a local\ncopy, run the following command.\n\n\nmake install\n\n\n\n\nThis will install REX-Ray into your \nGOPATH\n and you'll be able to make changes\nlocally, test them, and commit ideas and fixes back to your fork of the\nrepository.\n\n\nRunning the tests\n\n\nTo run the tests, run the following commands:\n\n\nmake install test\n\n\n\n\nThe \nmake install\n isn't strictly necessary, but it ensures that the tests are\nexecuted with the latest bits.\n\n\nSubmitting Pull Requests\n\n\nOnce you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing-to-rex-ray", 
            "text": "An introduction to contributing to the REX-Ray project   The REX-Ray project welcomes, and depends, on contributions from developers and\nusers in the open source community. Contributions can be made in a number of\nways, a few examples are:   Code patches via pull requests  Documentation improvements  Bug reports and patch reviews  OS, Storage, and Volume Drivers  A distributed server/client model with profile support", 
            "title": "Contributing to REX-Ray"
        }, 
        {
            "location": "/about/contributing/#reporting-an-issue", 
            "text": "Please include as much detail as you can. This includes:   The OS type and version  The REX-Ray version  The storage system in question  A set of logs with debug-logging enabled that show the problem", 
            "title": "Reporting an Issue"
        }, 
        {
            "location": "/about/contributing/#testing-the-development-version", 
            "text": "If you want to just install and try out the latest development version of\nREX-Ray you can do so with the following command. This can be useful if you\nwant to provide feedback for a new feature or want to confirm if a bug you\nhave encountered is fixed in the git master. It is  strongly  recommended\nthat you do this within a [virtualenv].  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -s unstable", 
            "title": "Testing the Development Version"
        }, 
        {
            "location": "/about/contributing/#installing-for-development", 
            "text": "First you'll need to fork and clone the repository. Once you have a local\ncopy, run the following command.  make install  This will install REX-Ray into your  GOPATH  and you'll be able to make changes\nlocally, test them, and commit ideas and fixes back to your fork of the\nrepository.", 
            "title": "Installing for Development"
        }, 
        {
            "location": "/about/contributing/#running-the-tests", 
            "text": "To run the tests, run the following commands:  make install test  The  make install  isn't strictly necessary, but it ensures that the tests are\nexecuted with the latest bits.", 
            "title": "Running the tests"
        }, 
        {
            "location": "/about/contributing/#submitting-pull-requests", 
            "text": "Once you are happy with your changes or you are ready for some feedback, push\nit to your fork and send a pull request. For a change to be accepted it will\nmost likely need to have tests and documentation if it is a new feature.", 
            "title": "Submitting Pull Requests"
        }, 
        {
            "location": "/about/license/", 
            "text": "Licensing\n\n\nThe legal stuff\n\n\n\n\nREX-Ray License\n\n\nLicensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at \nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/about/license/#licensing", 
            "text": "The legal stuff", 
            "title": "Licensing"
        }, 
        {
            "location": "/about/license/#rex-ray-license", 
            "text": "Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.", 
            "title": "REX-Ray License"
        }, 
        {
            "location": "/about/release-notes/", 
            "text": "Release Notes\n\n\n\n\nUpgrading\n\n\nTo upgrade REX-Ray to the latest version, use \ncurl install\n:\n\n\ncurl -sSL https://dl.bintray.com/emccode/rexray/install | sh -\n\n\n\nYou can determine your currently installed version using \nrexray version\n:\n\n\n$ rexray version\nBinary: /usr/local/bin/rexray\nSemVer: 0.2.1\nOsArch: Linux-x86_64\nBranch: master\nCommit: 36ccc425faeab49d792eda4851e3d72a85744874\nFormed: Tue, 27 Oct 2015 12:54:19 CDT\n\n\n\nVersion 0.3.2 (2016-03-04)\n\n\nNew Features\n\n\n\n\nSupport for Docker 1.10 and Volume Plugin Interface 1.2 (\n#273\n)\n\n\nStale PID File Prevents Service Start (\n#258\n)\n\n\nModule/Personality Support (\n#275\n)\n\n\nIsilon Preemption (\n#231\n)\n\n\nIsilon Snapshots (\n#260\n)\n\n\nboot2Docker Support (\n#263\n)\n\n\nScaleIO Dynamic Storage Pool Support (\n#267\n)\n\n\n\n\nEnhancements\n\n\n\n\nImproved installation documentation (\n#331\n)\n\n\nScaleIO volume name limitation (\n#304\n)\n\n\nDocker cache volumes for path operations (\n#306\n)\n\n\nConfig file validation (\n#312\n)\n\n\nBetter logging (\n#296\n)\n\n\nDocumentation Updates (\n#285\n)\n\n\n\n\nBug Fixes\n\n\n\n\nFixes issue with daemon process getting cleaned as part of SystemD Cgroup (\n#327\n)\n\n\nFixes regression in 0.3.2 RC3/RC4 resulting in no log file (\n#319\n)\n\n\nFixes no volumes returned on empty list (\n#322\n)\n\n\nFixes \"Unsupported FS\" when mounting/unmounting with EC2 (\n#321\n)\n\n\nScaleIO re-authentication issue (\n#303\n)\n\n\nDocker XtremIO create volume issue (\n#307\n)\n\n\nService status is reported correctly (\n#310\n)\n\n\n\n\nUpdates\n\n\n\n\nGo 1.6 (\n#308\n)\n\n\n\n\nThank You\n\n\n\n\nDan Forrest\n\n\nKapil Jain\n\n\nAlex Kamalov\n\n\n\n\nVersion 0.3.1 (2015-12-30)\n\n\nNew Features\n\n\n\n\nSupport for VirtualBox (\n#209\n)\n\n\nAdded Developer's Guide (\n#226\n)\n\n\n\n\nEnhancements\n\n\n\n\nMount/Unmount Accounting (\n#212\n)\n\n\nSupport for Sub-Path Volume Mounts / Permissions (\n#215\n)\n\n\n\n\nMilestone Issues\n\n\nThis release also includes many other small enhancements and bug fixes. For a\ncomplete list click \nhere\n.\n\n\nDownloads\n\n\nClick \nhere\n for the 0.3.1\nbinaries.\n\n\nVersion 0.3.0 (2015-12-08)\n\n\nNew Features\n\n\n\n\nPre-Emption support (\n#190\n)\n\n\nSupport for VMAX (\n#197\n)\n\n\nSupport for Isilon (\n#198\n)\n\n\nSupport for Google Compute Engine (GCE) (\n#194\n)\n\n\n\n\nEnhancements\n\n\n\n\nAdded driver example configurations (\n#201\n)\n\n\nNew configuration file format (\n#188\n)\n\n\n\n\nTweaks\n\n\n\n\nChopped flags \n--rexrayLogLevel\n becomes \nlogLevel\n (\n#196\n)\n\n\n\n\nPre-Emption Support\n\n\nPre-Emption is an important feature when using persistent volumes and container\nschedulers.  Without pre-emption, the default behavior of the storage drivers is\nto deny the attaching operation if the volume is already mounted elsewhere.\n\nIf it is desired that a host should be able to pre-empt from other hosts, then\nthis feature can be used to enable any host to pre-empt from another.\n\n\nMilestone Issues\n\n\nThis release also includes many other small enhancements and bug fixes. For a\ncomplete list click \nhere\n.\n\n\nDownloads\n\n\nClick \nhere\n for the 0.3.0\nbinaries.\n\n\nVersion 0.2.1 (2015-10-27)\n\n\nREX-Ray release 0.2.1 includes OpenStack support, vastly improved documentation,\nand continued foundation changes for future features.\n\n\nNew Features\n\n\n\n\nSupport for OpenStack (\n#111\n)\n\n\nCreate volume from volume using existing settings (\n#129\n)\n\n\n\n\nEnhancements\n\n\n\n\nA+ \nGoReport Card\n\n\nA+ \nCode Coverage\n\n\nGoDoc Support\n\n\nAbility to load REX-Ray as an independent storage platform (\n#127\n)\n\n\nNew documentation at http://rexray.readthedocs.org (\n#145\n)\n\n\nMore foundation updates\n\n\n\n\nTweaks\n\n\n\n\nCommand aliases for \nget\n and \ndelete\n - \nls\n and \nrm\n (\n#107\n)\n\n\n\n\nVersion 0.2.0 (2015-09-30)\n\n\nInstallation, SysV, SystemD Support\n\n\nREX-Ray now includes built-in support for installing itself as a service on\nLinux distributions that support either SystemV or SystemD initialization\nsystems. This feature has been tested successfully on both CentOS 7 Minimal\n(SystemD) and Ubuntu 14.04 Server (SystemV) distributions.\n\n\nTo install REX-Ray on a supported Linux distribution, all that is required\nnow is to download the binary and execute:\n\n\nsudo ./rexray service install\n\n\n\nWhat does that do? In short the above command will determine if the Linux\ndistribution uses systemctl, update-rc.d, or chkconfig to manage system\nservices. After that the following steps occur:\n\n\n\n\nThe path /opt/rexray is created and chowned to root:root with permissions\n set to 0755.\n\n\nThe binary is copied to /opt/rexray/rexray and chowned to root:root with\n permissions set to 4755. This is important, because this means that any\n non-privileged user can execute the rexray binary as root without requiring\n sudo privileges. For more information on this feature, please read about the\n \nLinux kernel's super-user ID (SUID) bit\n.\n\n\n\n\nBecause the REX-Ray binary can now be executed with root privileges by\n non-root users, the binary can be used by non-root users to easily attach\n and mount external storage.\n\n\n\n\nThe directory /etc/rexray is created and chowned to root:root.\n\n\n\n\nThe next steps depends on the type of Linux distribution. However, it's\nimportant to know that the new version of the REX-Ray binary now supports\nmanaging its own PID (at \n/var/run/rexray.pid\n) when run as a service as well\nas supports the standard SysV control commands such as \nstart\n, \nstop\n,\n\nstatus\n, and \nrestart\n.\n\n\nFor SysV Linux distributions that use \nchkconfig\n or \nupdate-rc.d\n, a symlink\nof the REX-Ray binary is created in \n/etc/init.d\n and then either\n\nchkconfig rexray on\n or \nupdate-rc.d rexray defaults\n is executed.\n\n\nModern Linux distributions have moved to SystemD for controlling services.\nIf the \nsystemctl\n command is detected when installing REX-Ray then a unit\nfile is written to \n/etc/systemd/system/rexray.servic\ne with the following\ncontents:\n\n\n[Unit]\nDescription=rexray\nBefore=docker.service\n\n[Service]\nEnvironmentFile=/etc/rexray/rexray.env\nExecStart=/usr/local/bin/rexray start -f\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\n\n[Install]\nWantedBy=docker.service\n\n\n\nThe REX-Ray service is not started immediately upon installation. The install\ncommand completes by informing the users that they should visit the\n\nREX-Ray website\n for information on how to\nconfigure REX-Ray's storage drivers. The text to the users also explains how\nto start the REX-Ray service once it's configured using the service command\nparticular to the Linux distribution.\n\n\nSingle Service\n\n\nThis release also removes the need for REX-Ray to be configured as multiple\nservice instances in order to provide multiple end-points to such consumers\nsuch as \nDocker\n. REX-Ray's backend now supports an internal, modular design\nwhich enables it to host multiple module instances of any module, such as the\nDockerVolumeDriverModule. In fact, one of the default, included modules is...\n\n\nAdmin Module \n HTTP JSON API\n\n\nThe AdminModule enables an HTTP JSON API for managing REX-Ray's module system\nas well as provides a UI to view the currently running modules. Simply start\nthe REX-Ray server and then visit the URL http://localhost:7979 in your favorite\nbrowser to see what's loaded. Or you can access either of the currently\nsupported REST URLs:\n\n\nhttp://localhost:7979/r/module/types\n\n\n\nand\n\n\nhttp://localhost:7979/r/module/instances\n\n\n\nActually, those aren't the \nonly\n two URLs, but the others are for internal\nusers as of this point. However, the source \nis\n open, so... :)\n\n\nIf you want to know what modules are available by using the CLI, after starting\nthe REX-Ray service simply type:\n\n\n[0]akutz@poppy:rexray$ rexray service module types\n[\n  {\n    \"id\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"addresses\": [\n      \"unix:///run/docker/plugins/rexray.sock\",\n      \"tcp://:7980\"\n    ]\n  },\n  {\n    \"id\": 1,\n    \"name\": \"AdminModule\",\n    \"addresses\": [\n      \"tcp://:7979\"\n    ]\n  }\n]\n[0]akutz@poppy:rexray$\n\n\n\nTo get a list of the \nrunning\n modules you would type:\n\n\n[0]akutz@poppy:rexray$ rexray service module instance get\n[\n  {\n    \"id\": 1,\n    \"typeId\": 1,\n    \"name\": \"AdminModule\",\n    \"address\": \"tcp://:7979\",\n    \"description\": \"The REX-Ray admin module\",\n    \"started\": true\n  },\n  {\n    \"id\": 2,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"unix:///run/docker/plugins/rexray.sock\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  },\n  {\n    \"id\": 3,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"tcp://:7980\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  }\n]\n[0]akutz@poppy:rexray$\n\n\n\nHmmm, you know, the REX-Ray CLI looks a little different in the above examples,\ndoesn't it? About that...\n\n\nCommand Line Interface\n\n\nThe CLI has also been enhanced to present a more simplified view up front to\nusers. The commands are now categorized into logical groups:\n\n\n[0]akutz@pax:~$ rexray\nREX-Ray:\n  A guest-based storage introspection tool that enables local\n  visibility and management from cloud and storage platforms.\n\nUsage:\n  rexray [flags]\n  rexray [command]\n\nAvailable Commands:\n  volume      The volume manager\n  snapshot    The snapshot manager\n  device      The device manager\n  adapter     The adapter manager\n  service     The service controller\n  version     Print the version\n  help        Help about any command\n\nGlobal Flags:\n  -c, --config=\"/Users/akutz/.rexray/config.yaml\": The REX-Ray configuration file\n  -?, --help[=false]: Help for rexray\n  -h, --host=\"tcp://:7979\": The REX-Ray service address\n  -l, --logLevel=\"info\": The log level (panic, fatal, error, warn, info, debug)\n  -v, --verbose[=false]: Print verbose help information\n\nUse \"rexray [command] --help\" for more information about a command.\n\n\n\nTravis-CI Support\n\n\nREX-Ray now supports Travis-CI builds either from the primary REX-Ray repository\nor via a fork. All builds should be executed through the Makefile, which is a\nTravis-CI default. For the Travis-CI settings please be sure to set the\nenvironment variable \nGO15VENDOREXPERIMENT\n to \n1\n.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#release-notes", 
            "text": "", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/release-notes/#upgrading", 
            "text": "To upgrade REX-Ray to the latest version, use  curl install :  curl -sSL https://dl.bintray.com/emccode/rexray/install | sh -  You can determine your currently installed version using  rexray version :  $ rexray version\nBinary: /usr/local/bin/rexray\nSemVer: 0.2.1\nOsArch: Linux-x86_64\nBranch: master\nCommit: 36ccc425faeab49d792eda4851e3d72a85744874\nFormed: Tue, 27 Oct 2015 12:54:19 CDT", 
            "title": "Upgrading"
        }, 
        {
            "location": "/about/release-notes/#version-032-2016-03-04", 
            "text": "", 
            "title": "Version 0.3.2 (2016-03-04)"
        }, 
        {
            "location": "/about/release-notes/#new-features", 
            "text": "Support for Docker 1.10 and Volume Plugin Interface 1.2 ( #273 )  Stale PID File Prevents Service Start ( #258 )  Module/Personality Support ( #275 )  Isilon Preemption ( #231 )  Isilon Snapshots ( #260 )  boot2Docker Support ( #263 )  ScaleIO Dynamic Storage Pool Support ( #267 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements", 
            "text": "Improved installation documentation ( #331 )  ScaleIO volume name limitation ( #304 )  Docker cache volumes for path operations ( #306 )  Config file validation ( #312 )  Better logging ( #296 )  Documentation Updates ( #285 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#bug-fixes", 
            "text": "Fixes issue with daemon process getting cleaned as part of SystemD Cgroup ( #327 )  Fixes regression in 0.3.2 RC3/RC4 resulting in no log file ( #319 )  Fixes no volumes returned on empty list ( #322 )  Fixes \"Unsupported FS\" when mounting/unmounting with EC2 ( #321 )  ScaleIO re-authentication issue ( #303 )  Docker XtremIO create volume issue ( #307 )  Service status is reported correctly ( #310 )", 
            "title": "Bug Fixes"
        }, 
        {
            "location": "/about/release-notes/#updates", 
            "text": "Go 1.6 ( #308 )", 
            "title": "Updates"
        }, 
        {
            "location": "/about/release-notes/#thank-you", 
            "text": "Dan Forrest  Kapil Jain  Alex Kamalov", 
            "title": "Thank You"
        }, 
        {
            "location": "/about/release-notes/#version-031-2015-12-30", 
            "text": "", 
            "title": "Version 0.3.1 (2015-12-30)"
        }, 
        {
            "location": "/about/release-notes/#new-features_1", 
            "text": "Support for VirtualBox ( #209 )  Added Developer's Guide ( #226 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_1", 
            "text": "Mount/Unmount Accounting ( #212 )  Support for Sub-Path Volume Mounts / Permissions ( #215 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#milestone-issues", 
            "text": "This release also includes many other small enhancements and bug fixes. For a\ncomplete list click  here .", 
            "title": "Milestone Issues"
        }, 
        {
            "location": "/about/release-notes/#downloads", 
            "text": "Click  here  for the 0.3.1\nbinaries.", 
            "title": "Downloads"
        }, 
        {
            "location": "/about/release-notes/#version-030-2015-12-08", 
            "text": "", 
            "title": "Version 0.3.0 (2015-12-08)"
        }, 
        {
            "location": "/about/release-notes/#new-features_2", 
            "text": "Pre-Emption support ( #190 )  Support for VMAX ( #197 )  Support for Isilon ( #198 )  Support for Google Compute Engine (GCE) ( #194 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_2", 
            "text": "Added driver example configurations ( #201 )  New configuration file format ( #188 )", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#tweaks", 
            "text": "Chopped flags  --rexrayLogLevel  becomes  logLevel  ( #196 )", 
            "title": "Tweaks"
        }, 
        {
            "location": "/about/release-notes/#pre-emption-support", 
            "text": "Pre-Emption is an important feature when using persistent volumes and container\nschedulers.  Without pre-emption, the default behavior of the storage drivers is\nto deny the attaching operation if the volume is already mounted elsewhere. \nIf it is desired that a host should be able to pre-empt from other hosts, then\nthis feature can be used to enable any host to pre-empt from another.", 
            "title": "Pre-Emption Support"
        }, 
        {
            "location": "/about/release-notes/#milestone-issues_1", 
            "text": "This release also includes many other small enhancements and bug fixes. For a\ncomplete list click  here .", 
            "title": "Milestone Issues"
        }, 
        {
            "location": "/about/release-notes/#downloads_1", 
            "text": "Click  here  for the 0.3.0\nbinaries.", 
            "title": "Downloads"
        }, 
        {
            "location": "/about/release-notes/#version-021-2015-10-27", 
            "text": "REX-Ray release 0.2.1 includes OpenStack support, vastly improved documentation,\nand continued foundation changes for future features.", 
            "title": "Version 0.2.1 (2015-10-27)"
        }, 
        {
            "location": "/about/release-notes/#new-features_3", 
            "text": "Support for OpenStack ( #111 )  Create volume from volume using existing settings ( #129 )", 
            "title": "New Features"
        }, 
        {
            "location": "/about/release-notes/#enhancements_3", 
            "text": "A+  GoReport Card  A+  Code Coverage  GoDoc Support  Ability to load REX-Ray as an independent storage platform ( #127 )  New documentation at http://rexray.readthedocs.org ( #145 )  More foundation updates", 
            "title": "Enhancements"
        }, 
        {
            "location": "/about/release-notes/#tweaks_1", 
            "text": "Command aliases for  get  and  delete  -  ls  and  rm  ( #107 )", 
            "title": "Tweaks"
        }, 
        {
            "location": "/about/release-notes/#version-020-2015-09-30", 
            "text": "", 
            "title": "Version 0.2.0 (2015-09-30)"
        }, 
        {
            "location": "/about/release-notes/#installation-sysv-systemd-support", 
            "text": "REX-Ray now includes built-in support for installing itself as a service on\nLinux distributions that support either SystemV or SystemD initialization\nsystems. This feature has been tested successfully on both CentOS 7 Minimal\n(SystemD) and Ubuntu 14.04 Server (SystemV) distributions.  To install REX-Ray on a supported Linux distribution, all that is required\nnow is to download the binary and execute:  sudo ./rexray service install  What does that do? In short the above command will determine if the Linux\ndistribution uses systemctl, update-rc.d, or chkconfig to manage system\nservices. After that the following steps occur:   The path /opt/rexray is created and chowned to root:root with permissions\n set to 0755.  The binary is copied to /opt/rexray/rexray and chowned to root:root with\n permissions set to 4755. This is important, because this means that any\n non-privileged user can execute the rexray binary as root without requiring\n sudo privileges. For more information on this feature, please read about the\n  Linux kernel's super-user ID (SUID) bit .   Because the REX-Ray binary can now be executed with root privileges by\n non-root users, the binary can be used by non-root users to easily attach\n and mount external storage.   The directory /etc/rexray is created and chowned to root:root.   The next steps depends on the type of Linux distribution. However, it's\nimportant to know that the new version of the REX-Ray binary now supports\nmanaging its own PID (at  /var/run/rexray.pid ) when run as a service as well\nas supports the standard SysV control commands such as  start ,  stop , status , and  restart .  For SysV Linux distributions that use  chkconfig  or  update-rc.d , a symlink\nof the REX-Ray binary is created in  /etc/init.d  and then either chkconfig rexray on  or  update-rc.d rexray defaults  is executed.  Modern Linux distributions have moved to SystemD for controlling services.\nIf the  systemctl  command is detected when installing REX-Ray then a unit\nfile is written to  /etc/systemd/system/rexray.servic e with the following\ncontents:  [Unit]\nDescription=rexray\nBefore=docker.service\n\n[Service]\nEnvironmentFile=/etc/rexray/rexray.env\nExecStart=/usr/local/bin/rexray start -f\nExecReload=/bin/kill -HUP $MAINPID\nKillMode=process\n\n[Install]\nWantedBy=docker.service  The REX-Ray service is not started immediately upon installation. The install\ncommand completes by informing the users that they should visit the REX-Ray website  for information on how to\nconfigure REX-Ray's storage drivers. The text to the users also explains how\nto start the REX-Ray service once it's configured using the service command\nparticular to the Linux distribution.", 
            "title": "Installation, SysV, SystemD Support"
        }, 
        {
            "location": "/about/release-notes/#single-service", 
            "text": "This release also removes the need for REX-Ray to be configured as multiple\nservice instances in order to provide multiple end-points to such consumers\nsuch as  Docker . REX-Ray's backend now supports an internal, modular design\nwhich enables it to host multiple module instances of any module, such as the\nDockerVolumeDriverModule. In fact, one of the default, included modules is...", 
            "title": "Single Service"
        }, 
        {
            "location": "/about/release-notes/#admin-module-http-json-api", 
            "text": "The AdminModule enables an HTTP JSON API for managing REX-Ray's module system\nas well as provides a UI to view the currently running modules. Simply start\nthe REX-Ray server and then visit the URL http://localhost:7979 in your favorite\nbrowser to see what's loaded. Or you can access either of the currently\nsupported REST URLs:  http://localhost:7979/r/module/types  and  http://localhost:7979/r/module/instances  Actually, those aren't the  only  two URLs, but the others are for internal\nusers as of this point. However, the source  is  open, so... :)  If you want to know what modules are available by using the CLI, after starting\nthe REX-Ray service simply type:  [0]akutz@poppy:rexray$ rexray service module types\n[\n  {\n    \"id\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"addresses\": [\n      \"unix:///run/docker/plugins/rexray.sock\",\n      \"tcp://:7980\"\n    ]\n  },\n  {\n    \"id\": 1,\n    \"name\": \"AdminModule\",\n    \"addresses\": [\n      \"tcp://:7979\"\n    ]\n  }\n]\n[0]akutz@poppy:rexray$  To get a list of the  running  modules you would type:  [0]akutz@poppy:rexray$ rexray service module instance get\n[\n  {\n    \"id\": 1,\n    \"typeId\": 1,\n    \"name\": \"AdminModule\",\n    \"address\": \"tcp://:7979\",\n    \"description\": \"The REX-Ray admin module\",\n    \"started\": true\n  },\n  {\n    \"id\": 2,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"unix:///run/docker/plugins/rexray.sock\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  },\n  {\n    \"id\": 3,\n    \"typeId\": 2,\n    \"name\": \"DockerVolumeDriverModule\",\n    \"address\": \"tcp://:7980\",\n    \"description\": \"The REX-Ray Docker VolumeDriver module\",\n    \"started\": true\n  }\n]\n[0]akutz@poppy:rexray$  Hmmm, you know, the REX-Ray CLI looks a little different in the above examples,\ndoesn't it? About that...", 
            "title": "Admin Module &amp; HTTP JSON API"
        }, 
        {
            "location": "/about/release-notes/#command-line-interface", 
            "text": "The CLI has also been enhanced to present a more simplified view up front to\nusers. The commands are now categorized into logical groups:  [0]akutz@pax:~$ rexray\nREX-Ray:\n  A guest-based storage introspection tool that enables local\n  visibility and management from cloud and storage platforms.\n\nUsage:\n  rexray [flags]\n  rexray [command]\n\nAvailable Commands:\n  volume      The volume manager\n  snapshot    The snapshot manager\n  device      The device manager\n  adapter     The adapter manager\n  service     The service controller\n  version     Print the version\n  help        Help about any command\n\nGlobal Flags:\n  -c, --config=\"/Users/akutz/.rexray/config.yaml\": The REX-Ray configuration file\n  -?, --help[=false]: Help for rexray\n  -h, --host=\"tcp://:7979\": The REX-Ray service address\n  -l, --logLevel=\"info\": The log level (panic, fatal, error, warn, info, debug)\n  -v, --verbose[=false]: Print verbose help information\n\nUse \"rexray [command] --help\" for more information about a command.", 
            "title": "Command Line Interface"
        }, 
        {
            "location": "/about/release-notes/#travis-ci-support", 
            "text": "REX-Ray now supports Travis-CI builds either from the primary REX-Ray repository\nor via a fork. All builds should be executed through the Makefile, which is a\nTravis-CI default. For the Travis-CI settings please be sure to set the\nenvironment variable  GO15VENDOREXPERIMENT  to  1 .", 
            "title": "Travis-CI Support"
        }
    ]
}